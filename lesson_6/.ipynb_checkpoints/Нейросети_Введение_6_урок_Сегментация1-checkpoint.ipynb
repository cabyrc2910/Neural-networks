{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NAoxoiHvCASI"
   },
   "outputs": [],
   "source": [
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 397
    },
    "id": "8i_4UQC_LxeA",
    "outputId": "41d1eb7e-de4a-4cfe-e242-361a0c6d5eb5"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "errorDetails": {
      "actions": [
       {
        "action": "open_url",
        "actionText": "Open Examples",
        "url": "/notebooks/snippets/importing_libraries.ipynb"
       }
      ]
     },
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-7b738a6bbf8f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"scene_parse_150\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'datasets'",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"scene_parse_150\", split=\"train\")\n",
    "index = 10\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g87--n2AtyO_"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_datasets as С\n",
    "tfds.disable_progress_bar()\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.config.experimental.set_visible_devices([], 'GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oWe0_rQM4JbC"
   },
   "source": [
    " Загрузка датасета\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "40ITeStwDwZb"
   },
   "outputs": [],
   "source": [
    "dataset, info = tfds.load('lost_and_found', with_info=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L7hS3tkUSGSD"
   },
   "outputs": [],
   "source": [
    "dataset['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rJcVdj_U4vzf"
   },
   "source": [
    "Следующий код выполнит простую аугументацию данных посредством переворота изображений. В дополнение изображение будет нормализовано к 0 и 1. Пиксели сегментационной маски будут помечены {1, 2, 3}, но для удобства из данного цифрового ряда будет вычтено по 1 и в итоге получиться {0, 1, 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FD60EbcAQqov"
   },
   "outputs": [],
   "source": [
    "def normalize(input_image, input_mask):\n",
    "  input_image = tf.cast(input_image, tf.float32) / 255.0\n",
    "  input_mask -= 1\n",
    "  return input_image, input_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2NPlCnBXQwb1"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def load_image_train(datapoint):\n",
    "  for img in datapoint:\n",
    "    input_image = tf.image.resize(datapoint['image_left'], (240, 320,), method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    # input_mask = tf.image.resize(datapoint['label'], (240, 320), method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "\n",
    "    if tf.random.uniform(()) > 0.5:\n",
    "      input_image = tf.image.flip_left_right(input_image)\n",
    "      # input_mask = tf.image.flip_left_right(input_mask)\n",
    "\n",
    "    # input_image = normalize(input_image)\n",
    "\n",
    "  return input_image #, input_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zf0S67hJRp3D"
   },
   "outputs": [],
   "source": [
    "def load_image_test(datapoint):\n",
    "  input_image = tf.image.resize(datapoint['image_left'], (128, 128, 3))\n",
    "  input_mask = tf.image.resize(datapoint['segmentation_label'], (128, 128))\n",
    "  input_image, input_mask = normalize(input_image, input_mask)\n",
    "  return input_image, input_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "65-qHTjX5VZh"
   },
   "source": [
    "Датасет уже содержит необходимые тестовый и тренеровочный сплиты, поэтому давайте использовать их."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kO024rTXUX8C"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "row_lens = []\n",
    "col_lens = []\n",
    "for img in dataset['train']:\n",
    "  row_lens.append(int(img['image_left'].shape[0]))\n",
    "  col_lens.append(int(img['image_left'].shape[1]))\n",
    "t = pd.DataFrame(row_lens, columns=['rows'])\n",
    "t['cols'] = col_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "COoEDlgLYSaV"
   },
   "outputs": [],
   "source": [
    "images = []\n",
    "for img in dataset['train']:\n",
    "  single_img = img['image_left']\n",
    "  images.append(single_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lxWwU72vZ1RV"
   },
   "outputs": [],
   "source": [
    "images[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qBkQ3kNUxuep"
   },
   "outputs": [],
   "source": [
    "plt.imshow(images[3])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BfjCAIIKWgNN"
   },
   "outputs": [],
   "source": [
    "t = tf.image.resize(images[2], (240, 320, ), method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "plt.imshow(t[:,:,:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p7LYcMs8qpWN"
   },
   "outputs": [],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yHwj2-8SaQli"
   },
   "outputs": [],
   "source": [
    "TRAIN_LENGTH = info.splits['train'].num_examples\n",
    "BATCH_SIZE = 32\n",
    "BUFFER_SIZE = 100\n",
    "STEPS_PER_EPOCH = TRAIN_LENGTH // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5-kJ9RbDHW7_"
   },
   "outputs": [],
   "source": [
    "load_image_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "39fYScNz9lmo"
   },
   "outputs": [],
   "source": [
    "train = dataset['train'].map(load_image_train, num_parallel_calls=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DeFwFDN6EVoI"
   },
   "outputs": [],
   "source": [
    "train_dataset = train.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "train_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "#test_dataset = test.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xa3gMAE_9qNa"
   },
   "source": [
    "Давайте посмотрим на пример  изображения из датасета и соотвествующую ему маску из датасета."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "03lgSwtdsbWB"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H82mreKYZgtm"
   },
   "outputs": [],
   "source": [
    "train.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3N2RPAAW9q4W"
   },
   "outputs": [],
   "source": [
    "def display_images(display_list):\n",
    "  plt.figure(figsize=(15, 15))\n",
    "  title = ['Input Image', 'True Mask', 'Predicted Mask']\n",
    "\n",
    "  for i in range(len(display_list)):\n",
    "    plt.subplot(1, len(display_list), i+1)\n",
    "    plt.title(title[i])\n",
    "    plt.imshow(display_list[i][:,:,:])\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jgldsz4taDAt"
   },
   "outputs": [],
   "source": [
    "display_list = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7XQQBT-yZxtn"
   },
   "outputs": [],
   "source": [
    "for image in train.take(1):\n",
    "\tsample_image= image\n",
    "\tdisplay_list = sample_image\n",
    "\n",
    "display_images(display_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l-LithGUvt78"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import seaborn as sns\n",
    "from tensorflow import keras\n",
    "import cv2\n",
    "import PIL\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5NrKFRr1wkvL"
   },
   "outputs": [],
   "source": [
    "width, height = 224, 224"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FAOe93FRMk3w"
   },
   "source": [
    "Определение модели\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LuT2SflUa1Ns"
   },
   "outputs": [],
   "source": [
    "base_model = keras.applications.vgg16.VGG16(\n",
    "    include_top=False, input_shape=(width, height, 3))\n",
    "\n",
    "layer_names = [\n",
    "    'block1_pool',\n",
    "    'block2_pool',\n",
    "    'block3_pool',\n",
    "    'block4_pool',\n",
    "    'block5_pool',\n",
    "]\n",
    "base_model_outputs = [base_model.get_layer(\n",
    "    name).output for name in layer_names]\n",
    "base_model.trainable = False\n",
    "\n",
    "VGG_16 = tf.keras.models.Model(base_model.input,\n",
    "                               base_model_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yac-LBVKa_2c"
   },
   "outputs": [],
   "source": [
    "def fcn8_decoder(convs, n_classes):\n",
    "\tf1, f2, f3, f4, p5 = convs\n",
    "\n",
    "\tn = 4096\n",
    "\tc6 = tf.keras.layers.Conv2D(\n",
    "\t\tn, (7, 7), activation='relu', padding='same',\n",
    "\tname=\"conv6\")(p5)\n",
    "\tc7 = tf.keras.layers.Conv2D(\n",
    "\t\tn, (1, 1), activation='relu', padding='same',\n",
    "\tname=\"conv7\")(c6)\n",
    "\n",
    "\tf5 = c7\n",
    "\n",
    "\t# upsample the output of the encoder\n",
    "\t# then crop extra pixels that were introduced\n",
    "\to = tf.keras.layers.Conv2DTranspose(n_classes, kernel_size=(\n",
    "\t\t4, 4), strides=(2, 2), use_bias=False)(f5)\n",
    "\to = tf.keras.layers.Cropping2D(cropping=(1, 1))(o)\n",
    "\n",
    "\t# load the pool 4 prediction and do a 1x1\n",
    "\t# convolution to reshape it to the same shape of `o` above\n",
    "\to2 = f4\n",
    "\to2 = (tf.keras.layers.Conv2D(n_classes, (1, 1),\n",
    "\t\t\t\t\t\t\t\tactivation='relu',\n",
    "\t\t\t\t\t\t\t\tpadding='same'))(o2)\n",
    "\n",
    "\t# add the results of the upsampling and pool 4 prediction\n",
    "\to = tf.keras.layers.Add()([o, o2])\n",
    "\n",
    "\t# upsample the resulting tensor of the operation you just did\n",
    "\to = (tf.keras.layers.Conv2DTranspose(\n",
    "\t\tn_classes, kernel_size=(4, 4), strides=(2, 2),\n",
    "\tuse_bias=False))(o)\n",
    "\to = tf.keras.layers.Cropping2D(cropping=(1, 1))(o)\n",
    "\n",
    "\t# load the pool 3 prediction and do a 1x1\n",
    "\t# convolution to reshape it to the same shape of `o` above\n",
    "\to2 = f3\n",
    "\to2 = (tf.keras.layers.Conv2D(n_classes, (1, 1),\n",
    "\t\t\t\t\t\t\t\tactivation='relu',\n",
    "\t\t\t\t\t\t\t\tpadding='same'))(o2)\n",
    "\n",
    "\t# add the results of the upsampling and pool 3 prediction\n",
    "\to = tf.keras.layers.Add()([o, o2])\n",
    "\n",
    "\t# upsample up to the size of the original image\n",
    "\to = tf.keras.layers.Conv2DTranspose(\n",
    "\t\tn_classes, kernel_size=(8, 8), strides=(8, 8),\n",
    "\tuse_bias=False)(o)\n",
    "\n",
    "\t# append a softmax to get the class probabilities\n",
    "\to = tf.keras.layers.Activation('softmax')(o)\n",
    "\treturn o\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h7OAXlEobFlE"
   },
   "outputs": [],
   "source": [
    "def segmentation_model():\n",
    "\n",
    "\tinputs = keras.layers.Input(shape=(width, height, 3))\n",
    "\tconvs = VGG_16(inputs)\n",
    "\toutputs = fcn8_decoder(convs, 3)\n",
    "\tmodel = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "\treturn model\n",
    "\n",
    "\n",
    "opt = keras.optimizers.Adam()\n",
    "\n",
    "model = segmentation_model()\n",
    "model.compile(optimizer=opt,\n",
    "\t\t\tloss=tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "\t\t\t\tfrom_logits=True),\n",
    "\t\t\tmetrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iTN0HsHTbI0k"
   },
   "outputs": [],
   "source": [
    "def create_mask(pred_mask):\n",
    "\tpred_mask = tf.argmax(pred_mask, axis=-1)\n",
    "\tpred_mask = pred_mask[..., tf.newaxis]\n",
    "\treturn pred_mask[0]\n",
    "\n",
    "\n",
    "def show_predictions(dataset=None, num=1):\n",
    "\tif dataset:\n",
    "\t\tfor image, mask in dataset.take(num):\n",
    "\t\t\tpred_mask = model.predict(image)\n",
    "\t\t\tdisplay_images([image[0], mask[0], create_mask(pred_mask)])\n",
    "\telse:\n",
    "\t\tdisplay_images([sample_image, sample_mask,\n",
    "\t\t\t\t\t\tcreate_mask(model.predict(sample_image[tf.newaxis, ...]))])\n",
    "\n",
    "\n",
    "show_predictions()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jTqqUeMIlYzV"
   },
   "source": [
    "# Обучение  \n",
    "Теперь, когда все необходимые функции вместе с моделью созданы, мы будем обучать модель. Мы будем обучать модель в течение 20 эпох и выполним разделение проверки на 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cA07UrVAvRkP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xJjqy9vKbLMz"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "VAL_SUBSPLITS = 5\n",
    "VALIDATION_STEPS = info.splits['test'].num_examples//BATCH_SIZE//VAL_SUBSPLITS\n",
    "\n",
    "model_history = model.fit(train_ds, epochs=EPOCHS,\n",
    "\t\t\t\t\t\tsteps_per_epoch=STEPS_PER_EPOCH,\n",
    "\t\t\t\t\t\tvalidation_steps=VALIDATION_STEPS,\n",
    "\t\t\t\t\t\tvalidation_data=test_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TeshMgvTvRg3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DpTq3BpnvRd1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "crYTxyoFbODb"
   },
   "outputs": [],
   "source": [
    "def compute_metrics(y_true, y_pred):\n",
    "\t'''\n",
    "\tComputes IOU and Dice Score.\n",
    "\n",
    "\tArgs:\n",
    "\ty_true (tensor) - ground truth label map\n",
    "\ty_pred (tensor) - predicted label map\n",
    "\t'''\n",
    "\n",
    "\tclass_wise_iou = []++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\tclass_wise_dice_score = []++++++++++++++++++++\n",
    "\n",
    "\tsmoothening_factor = 0.00001\n",
    "\n",
    "\tfor i in range(3):\n",
    "\t\tintersection = np.sum((y_pred == i) * (y_true == i))\n",
    "\t\ty_true_area = np.sum((y_true == i))++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++=+++++++++++++++++=+++++=+++++++++++++++++++++++++++++++++++++++\n",
    "\t\ty_pred_area = np.sum((y_pred == i))+++++++++++++++++++++\n",
    "\t\tiou = (intersection + smoothening_factor) / \\++++++++++++++++++++++++++++++++++++\n",
    "\t\t\t(combined_area - intersection + smoothening_factor)\n",
    "\t\tclass_wise_iou.append(iou)\n",
    "\n",
    "\t\tdice_score = 2 * ((intersection + smoothening_factor) /\n",
    "\t\t\t\t\t\t(combined_area + smoothening_factor))\n",
    "\t\tclass_wise_dice_score.append(dice_score)\n",
    "\n",
    "\t9+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++return class_wise_iou, class_wise_dice_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CQuGBajmvRa_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PlFW7IthcMBU"
   },
   "outputs": [],
   "source": [
    "def get_test_image_and_annotation_arrays():\n",
    "\t'''\n",
    "\tUnpacks the test dataset and returns\n",
    "\tthe input images and segmentation masks\n",
    "\t'''\n",
    "\n",
    "\tds = test_ds.unbatch()\n",
    "\tds = ds.batch(info.splits['test'].num_examples)\n",
    "\n",
    "\timages = []\n",
    "\ty_true_segments = []\n",
    "\n",
    "\tfor image, annotation in ds.take(1):\n",
    "\t\ty_true_segments = annotation.numpy()\n",
    "\t\timages = image.numpy()\n",
    "\n",
    "\ty_true_segments = y_true_segments[:(\n",
    "\t\tinfo.splits['test'].num_examples - (info.splits['test']\n",
    "\t\t\t\t\t\t\t\t\t\t\t.num_examples % BATCH_SIZE))]\n",
    "\timages = images[:(info.splits['test'].num_examples -\n",
    "\t\t\t\t\t(info.splits['test'].num_examples % BATCH_SIZE))]\n",
    "\n",
    "\treturn images, y_true_segments\n",
    "\n",
    "\n",
    "y_true_images, y_true_segments = get_test_image_and_annotation_arrays()\n",
    "\n",
    "integer_slider = 2574\n",
    "img = np.reshape(y_true_images[integer_slider], (1, width, height, 3))\n",
    "y_pred_mask = model.predict(img)\n",
    "y_pred_mask = create_mask(y_pred_mask)\n",
    "y_pred_mask.shape\n",
    "\n",
    "\n",
    "def display_prediction(display_list, display_string):\n",
    "\tplt.figure(figsize=(15, 15))\n",
    "\ttitle = ['Input Image', 'True Mask', 'Predicted Mask']\n",
    "\n",
    "\tfor i in range(len(display_list)):\n",
    "\t\tplt.subplot(1, len(display_list), i+1)\n",
    "\t\tplt.title(title[i])\n",
    "\t\tplt.xticks([])\n",
    "\t\tplt.yticks([])\n",
    "\t\tif i == 1:\n",
    "\t\t\tplt.xlabel(display_string, fontsize=12)\n",
    "\t\tplt.imshow(keras.preprocessing.image.array_to_img(display_list[i]))\n",
    "\tplt.show()\n",
    "\n",
    "\n",
    "iou, dice_score = compute_metrics(\n",
    "\ty_true_segments[integer_slider], y_pred_mask.numpy())\n",
    "display_list = [y_true_images[integer_slider],\n",
    "\t\t\t\ty_true_segments[integer_slider], y_pred_mask]\n",
    "\n",
    "display_string_list = [\"{}: IOU: {} Dice Score: {}\".format(display_list[idx],\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\ti, dc) for idx, (i, dc) in\n",
    "\t\t\t\t\tenumerate(zip(np.round(iou, 4), np.round(dice_score, 4)))]\n",
    "display_string = \"\\n\\n\".join(display_string_list)\n",
    "\n",
    "\n",
    "# showing predictions with metrics\n",
    "display_prediction(display_list, display_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jht_fxXavRXF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1-xPgGskvRT9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lTkxOFHmvRP9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9_um3LXLvRMN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_A3MvmH7vQ7X"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jm-2BorYvQ4N"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JHYCx-kpvQ0m"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c6iB4iMvMkX9"
   },
   "outputs": [],
   "source": [
    "OUTPUT_CHANNELS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W4mQle3lthit"
   },
   "source": [
    "Как упоминалось ранее энкодером будет предтренированный MobileNetV2, который подготовлен и готов к использованию - [tf.keras.applications](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/applications). Энкодер состоит из определенных аутпутов из средних слоев модели. Обратите внимание энкодр не будет участвовать в процессе тренировкие модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "liCeLH0ctjq7"
   },
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.MobileNetV2(input_shape=[256, 256, 3], include_top=False)\n",
    "\n",
    "# Use the activations of these layers\n",
    "layer_names = [\n",
    "    'block_1_expand_relu',   # 64x64\n",
    "    'block_3_expand_relu',   # 32x32\n",
    "    'block_6_expand_relu',   # 16x16\n",
    "    'block_13_expand_relu',  # 8x8\n",
    "    'block_16_project',      # 4x4\n",
    "]\n",
    "layers = [base_model.get_layer(name).output for name in layer_names]\n",
    "\n",
    "# Create the feature extraction model\n",
    "down_stack = tf.keras.Model(inputs=base_model.input, outputs=layers)\n",
    "\n",
    "down_stack.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KPw8Lzra5_T9"
   },
   "source": [
    "\n",
    "\n",
    "Декодер/апсемплер это просто серия апсемпл блоков имплементированнхы в TensorFlow examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p0ZbfywEbZpJ"
   },
   "outputs": [],
   "source": [
    "up_stack = [\n",
    "    pix2pix.upsample(512, 3),  # 4x4 -> 8x8\n",
    "    pix2pix.upsample(256, 3),  # 8x8 -> 16x16\n",
    "    pix2pix.upsample(128, 3),  # 16x16 -> 32x32\n",
    "    pix2pix.upsample(64, 3),   # 32x32 -> 64x64\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "45HByxpVtrPF"
   },
   "outputs": [],
   "source": [
    "def unet_model(output_channels):\n",
    "  inputs = tf.keras.layers.Input(shape=[256, 256, 3])\n",
    "  x = inputs\n",
    "\n",
    "  # Downsampling through the model\n",
    "  skips = down_stack(x)\n",
    "  x = skips[-1]\n",
    "  skips = reversed(skips[:-1])\n",
    "\n",
    "  # Upsampling and establishing the skip connections\n",
    "  for up, skip in zip(up_stack, skips):\n",
    "    x = up(x)\n",
    "    concat = tf.keras.layers.Concatenate()\n",
    "    x = concat([x, skip])\n",
    "\n",
    "  # This is the last layer of the model\n",
    "  last = tf.keras.layers.Conv2DTranspose(\n",
    "      output_channels, 3, strides=2,\n",
    "      padding='same')  #64x64 -> 128x128\n",
    "\n",
    "  x = last(x)\n",
    "\n",
    "  return tf.keras.Model(inputs=inputs, outputs=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j0DGH_4T0VYn"
   },
   "source": [
    "## Тренировка модели\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6he36HK5uKAc"
   },
   "outputs": [],
   "source": [
    "model = unet_model(OUTPUT_CHANNELS)\n",
    "model.compile(optimizer='SGD',\n",
    "              loss=tf.keras.losses.KLDivergence(),\n",
    "              metrics=['top_k_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xVMzbIZLcyEF"
   },
   "source": [
    "Посмотрим на получившуюся архитектуру модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ODwcrKl4YXo9"
   },
   "outputs": [],
   "source": [
    "pip install pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AJSrUIlhZEa-"
   },
   "outputs": [],
   "source": [
    "%pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sw82qF1Gcovr"
   },
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tc3MiEO2twLS"
   },
   "source": [
    "\n",
    "Давайте попробуем сделать предсказание с помощью нашей модели до того как началось обучение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UwvIKLZPtxV_"
   },
   "outputs": [],
   "source": [
    "def create_mask(pred_mask):\n",
    "  pred_mask = tf.argmax(pred_mask, axis=-1)\n",
    "  pred_mask = pred_mask[..., tf.newaxis]\n",
    "  return pred_mask[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YLNsrynNtx4d"
   },
   "outputs": [],
   "source": [
    "def show_predictions(dataset=None, num=1):\n",
    "  if dataset:\n",
    "    for image, mask in dataset.take(num):\n",
    "      pred_mask = model.predict(image)\n",
    "      display([image[0], mask[0], create_mask(pred_mask)])\n",
    "  else:\n",
    "    display([sample_image, sample_mask,\n",
    "             create_mask(model.predict(sample_image[tf.newaxis, ...]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X_1CC0T4dho3"
   },
   "outputs": [],
   "source": [
    "show_predictions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "22AyVYWQdkgk"
   },
   "source": [
    "\n",
    "\n",
    "Давайте осуществлять мониторинг того как улучшается работа модели в процессе обучения. Для завершения этой задачи callback функция определена ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wHrHsqijdmL6"
   },
   "outputs": [],
   "source": [
    "class DisplayCallback(tf.keras.callbacks.Callback):\n",
    "  def on_epoch_end(self, epoch, logs=None):\n",
    "    clear_output(wait=True)\n",
    "    show_predictions()\n",
    "    print ('\\nSample Prediction after epoch {}\\n'.format(epoch+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "StKDH_B9t4SD"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 5 # увеличьте при необходимости\n",
    "VAL_SUBSPLITS = 5\n",
    "VALIDATION_STEPS = info.splits['test'].num_examples//BATCH_SIZE//VAL_SUBSPLITS\n",
    "\n",
    "model_history = model.fit(train_dataset, epochs=EPOCHS,\n",
    "                          steps_per_epoch=STEPS_PER_EPOCH,\n",
    "                          validation_steps=VALIDATION_STEPS,\n",
    "                          validation_data=test_dataset,\n",
    "                          callbacks=[DisplayCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P_mu0SAbt40Q"
   },
   "outputs": [],
   "source": [
    "loss = model_history.history['loss']\n",
    "val_loss = model_history.history['val_loss']\n",
    "\n",
    "epochs = range(EPOCHS)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'bo', label='Validation loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss Value')\n",
    "plt.ylim([0, 1])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "emb5Y7131dNN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "unP3cnxo_N72"
   },
   "source": [
    "## Make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7BVXldSo-0mW"
   },
   "source": [
    "Давайте сделаем несколько предсказаний. Для экономии времени использовалось небольшое количество эпох, но вы можете его увеличить для того чтобы модель давала более точные результаты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ikrzoG24qwf5"
   },
   "outputs": [],
   "source": [
    "show_predictions(test_dataset, 3)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
