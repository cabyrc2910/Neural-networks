<!DOCTYPE html>
<html lang="en">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="description" content="Keras documentation">
  <meta name="author" content="Keras Team">
  <link rel="shortcut icon" href="https://keras.io/img/favicon.ico">
  
  <link rel="canonical" href="https://keras.io/guides/keras_cv/generate_images_with_stable_diffusion/" />
  

  <!-- Social -->
  <meta property="og:title" content="Keras documentation: High-performance image generation using Stable Diffusion in KerasCV">
  <meta property="og:image" content="https://keras.io/img/logo-k-keras-wb.png">
  <meta name="twitter:title" content="Keras documentation: High-performance image generation using Stable Diffusion in KerasCV">
  <meta name="twitter:image" content="https://keras.io/img/k-keras-social.png">
  <meta name="twitter:card" content="summary">

  <title>High-performance image generation using Stable Diffusion in KerasCV</title>

  <!-- Bootstrap core CSS -->
  <link href="/css/bootstrap.min.css" rel="stylesheet">

  <!-- Custom fonts for this template -->
  <link href="https://fonts.googleapis.com/css2?family=Open+Sans:wght@400;600;700;800&display=swap" rel="stylesheet">

  <!-- Custom styles for this template -->
  <link href="/css/docs.css" rel="stylesheet">
  <link href="/css/monokai.css" rel="stylesheet">

  <!-- Google Tag Manager -->
  <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
  new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
  j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
  'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
  })(window,document,'script','dataLayer','GTM-5DNGF4N');
  </script>
  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-175165319-128', 'auto');
  ga('send', 'pageview');
  </script>
  <!-- End Google Tag Manager -->

  <script async defer src="https://buttons.github.io/buttons.js"></script>

</head>

<body>
  <!-- Google Tag Manager (noscript) -->
  <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-5DNGF4N"
  height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
  <!-- End Google Tag Manager (noscript) -->

  <div class='k-page'>
  
    <div class="k-nav" id="nav-menu">
      <a href='/'><img src='/img/logo-small.png' class='logo-small' /></a>

      <div class="nav flex-column nav-pills" role="tablist" aria-orientation="vertical">

        
          <a class="nav-link" href="/about/" role="tab" aria-selected="">About Keras</a>
          
        
          <a class="nav-link" href="/getting_started/" role="tab" aria-selected="">Getting started</a>
          
        
          <a class="nav-link active" href="/guides/" role="tab" aria-selected="">Developer guides</a>
          
            
              <a class="nav-sublink" href="/guides/functional_api/">The Functional API</a>
                
            
              <a class="nav-sublink" href="/guides/sequential_model/">The Sequential model</a>
                
            
              <a class="nav-sublink" href="/guides/making_new_layers_and_models_via_subclassing/">Making new layers & models via subclassing</a>
                
            
              <a class="nav-sublink" href="/guides/training_with_built_in_methods/">Training & evaluation with the built-in methods</a>
                
            
              <a class="nav-sublink" href="/guides/custom_train_step_in_jax/">Customizing `fit()` with JAX</a>
                
            
              <a class="nav-sublink" href="/guides/custom_train_step_in_tensorflow/">Customizing `fit()` with TensorFlow</a>
                
            
              <a class="nav-sublink" href="/guides/custom_train_step_in_torch/">Customizing `fit()` with PyTorch</a>
                
            
              <a class="nav-sublink" href="/guides/writing_a_custom_training_loop_in_jax/">Writing a custom training loop in JAX</a>
                
            
              <a class="nav-sublink" href="/guides/writing_a_custom_training_loop_in_tensorflow/">Writing a custom training loop in TensorFlow</a>
                
            
              <a class="nav-sublink" href="/guides/writing_a_custom_training_loop_in_torch/">Writing a custom training loop in PyTorch</a>
                
            
              <a class="nav-sublink" href="/guides/serialization_and_saving/">Serialization & saving</a>
                
            
              <a class="nav-sublink" href="/guides/customizing_saving_and_serialization/">Customizing saving & serialization</a>
                
            
              <a class="nav-sublink" href="/guides/writing_your_own_callbacks/">Writing your own callbacks</a>
                
            
              <a class="nav-sublink" href="/guides/transfer_learning/">Transfer learning & fine-tuning</a>
                
            
              <a class="nav-sublink" href="/guides/distributed_training_with_jax/">Distributed training with JAX</a>
                
            
              <a class="nav-sublink" href="/guides/distributed_training_with_tensorflow/">Distributed training with TensorFlow</a>
                
            
              <a class="nav-sublink" href="/guides/distributed_training_with_torch/">Distributed training with PyTorch</a>
                
            
              <a class="nav-sublink" href="/guides/distribution/">Distributed training with Keras 3</a>
                
            
              <a class="nav-sublink" href="/guides/migrating_to_keras_3/">Migrating Keras 2 code to Keras 3</a>
                
            
              <a class="nav-sublink" href="/guides/keras_tuner/">Hyperparameter Tuning</a>
                
            
              <a class="nav-sublink active" href="/guides/keras_cv/">KerasCV</a>
                
                  
                    <a class="nav-sublink2" href="/guides/keras_cv/object_detection_keras_cv/">Use KerasCV to assemble object detection pipelines</a>
                  
                    <a class="nav-sublink2" href="/guides/keras_cv/classification_with_keras_cv/">Use KerasCV to train powerful image classifiers.</a>
                  
                    <a class="nav-sublink2" href="/guides/keras_cv/cut_mix_mix_up_and_rand_augment/">CutMix, MixUp, and RandAugment image augmentation with KerasCV</a>
                  
                    <a class="nav-sublink2 active" href="/guides/keras_cv/generate_images_with_stable_diffusion/">High-performance image generation using Stable Diffusion in KerasCV</a>
                  
                    <a class="nav-sublink2" href="/guides/keras_cv/custom_image_augmentations/">Custom Image Augmentations with BaseImageAugmentationLayer</a>
                  
                    <a class="nav-sublink2" href="/guides/keras_cv/semantic_segmentation_deeplab_v3_plus/">Semantic Segmentation with KerasCV</a>
                  
                    <a class="nav-sublink2" href="/guides/keras_cv/segment_anything_in_keras_cv/">Segment Anything in KerasCV</a>
                  
                
            
              <a class="nav-sublink" href="/guides/keras_nlp/">KerasNLP</a>
                
            
          
        
          <a class="nav-link" href="/api/" role="tab" aria-selected="">Keras 3 API documentation</a>
          
        
          <a class="nav-link" href="/2.15/api/" role="tab" aria-selected="">Keras 2 API documentation</a>
          
        
          <a class="nav-link" href="/examples/" role="tab" aria-selected="">Code examples</a>
          
        
          <a class="nav-link" href="/keras_tuner/" role="tab" aria-selected="">KerasTuner: Hyperparameter Tuning</a>
          
        
          <a class="nav-link" href="/keras_cv/" role="tab" aria-selected="">KerasCV: Computer Vision Workflows</a>
          
        
          <a class="nav-link" href="/keras_nlp/" role="tab" aria-selected="">KerasNLP: Natural Language Workflows</a>
          
        

      </div>

    </div>

    <div class='k-main'>
      
      <div class='k-main-top'>
        <script>
          function displayDropdownMenu() {
            e = document.getElementById("nav-menu");
            if (e.style.display == "block") {
              e.style.display = "none";
            }
            else {
              e.style.display = "block";
              document.getElementById("dropdown-nav").style.display = "block";
            }
          }

          function resetMobileUI() {
            if (window.innerWidth <= 840) {
              document.getElementById("nav-menu").style.display = "none";
              document.getElementById("dropdown-nav").style.display = "block";
            }
            else {
              document.getElementById("nav-menu").style.display = "block";
              document.getElementById("dropdown-nav").style.display = "none";
            }

            var navmenu = document.getElementById("nav-menu");
            var menuheight = navmenu.clientHeight;
            var kmain = document.getElementById("k-main-id");
            kmain.style.minHeight = (menuheight + 100) + 'px';
          }

          window.onresize = resetMobileUI;

          window.addEventListener("load", (event) => {
            resetMobileUI()
          });
        </script>
        <div id='dropdown-nav' onclick="displayDropdownMenu();">
          <svg viewBox="-20 -20 120 120" width="60" height="60">
            <rect width="100" height="20"></rect>
            <rect y="30" width="100" height="20"></rect>
            <rect y="60" width="100" height="20"></rect>
          </svg>
        </div>

        <form class="bd-search d-flex align-items-center k-search-form" id="search-form">
          <input type="search" class="k-search-input" id="search-input" placeholder="Search Keras documentation..." aria-label="Search Keras documentation..." autocomplete="off">
            <button class="k-search-btn">
              <svg width="13" height="13" viewBox="0 0 13 13"><title>search</title><path d="m4.8495 7.8226c0.82666 0 1.5262-0.29146 2.0985-0.87438 0.57232-0.58292 0.86378-1.2877 0.87438-2.1144 0.010599-0.82666-0.28086-1.5262-0.87438-2.0985-0.59352-0.57232-1.293-0.86378-2.0985-0.87438-0.8055-0.010599-1.5103 0.28086-2.1144 0.87438-0.60414 0.59352-0.8956 1.293-0.87438 2.0985 0.021197 0.8055 0.31266 1.5103 0.87438 2.1144 0.56172 0.60414 1.2665 0.8956 2.1144 0.87438zm4.4695 0.2115 3.681 3.6819-1.259 1.284-3.6817-3.7 0.0019784-0.69479-0.090043-0.098846c-0.87973 0.76087-1.92 1.1413-3.1207 1.1413-1.3553 0-2.5025-0.46363-3.4417-1.3909s-1.4088-2.0686-1.4088-3.4239c0-1.3553 0.4696-2.4966 1.4088-3.4239 0.9392-0.92727 2.0864-1.3969 3.4417-1.4088 1.3553-0.011889 2.4906 0.45771 3.406 1.4088 0.9154 0.95107 1.379 2.0924 1.3909 3.4239 0 1.2126-0.38043 2.2588-1.1413 3.1385l0.098834 0.090049z"></path></svg>
            </button>
        </form>
        <script>
          var form = document.getElementById('search-form');
          form.onsubmit = function(e) {
            e.preventDefault();
            var query = document.getElementById('search-input').value;
            window.location.href = '/search.html?query=' + query;
            return False
          }
        </script>

      </div>
      <div class='k-main-inner' id='k-main-id'>
        <div class='k-location-slug'>
            <span class="k-location-slug-pointer">►</span> 
                <a href='/guides/'>Developer guides</a> /
              
                <a href='/guides/keras_cv/'>KerasCV</a> /
               High-performance image generation using Stable Diffusion in KerasCV
        </div>
        <div class='k-content'>
          <h1 id="highperformance-image-generation-using-stable-diffusion-in-kerascv">High-performance image generation using Stable Diffusion in KerasCV</h1>
<p><strong>Authors:</strong> <a href="https://twitter.com/fchollet">fchollet</a>, <a href="https://twitter.com/luke_wood_ml">lukewood</a>, <a href="https://github.com/divamgupta">divamgupta</a><br>
<strong>Date created:</strong> 2022/09/25<br>
<strong>Last modified:</strong> 2022/09/25<br>
<strong>Description:</strong> Generate new images using KerasCV's Stable Diffusion model.</p>
<p><img class="k-inline-icon" src="https://colab.research.google.com/img/colab_favicon.ico"/> <a href="https://colab.research.google.com/github/keras-team/keras-io/blob/master/guides/ipynb/keras_cv/generate_images_with_stable_diffusion.ipynb"><strong>View in Colab</strong></a>  <span class="k-dot">•</span><img class="k-inline-icon" src="https://github.com/favicon.ico"/> <a href="https://github.com/keras-team/keras-io/blob/master/guides/keras_cv/generate_images_with_stable_diffusion.py"><strong>GitHub source</strong></a></p>
<hr />
<h2 id="overview">Overview</h2>
<p>In this guide, we will show how to generate novel images based on a text prompt using
the KerasCV implementation of <a href="https://stability.ai/">stability.ai</a>'s text-to-image model,
<a href="https://github.com/CompVis/stable-diffusion">Stable Diffusion</a>.</p>
<p>Stable Diffusion is a powerful, open-source text-to-image generation model.  While there
exist multiple open-source implementations that allow you to easily create images from
textual prompts, KerasCV's offers a few distinct advantages.
These include <a href="https://www.tensorflow.org/xla">XLA compilation</a> and
<a href="https://www.tensorflow.org/guide/mixed_precision">mixed precision</a> support,
which together achieve state-of-the-art generation speed.</p>
<p>In this guide, we will explore KerasCV's Stable Diffusion implementation, show how to use
these powerful performance boosts, and explore the performance benefits
that they offer.</p>
<p><strong>Note:</strong> To run this guide on the <code>torch</code> backend, please set <code>jit_compile=False</code>
everywhere. XLA compilation for Stable Diffusion does not currently work with
torch.</p>
<p>To get started, let's install a few dependencies and sort out some imports:</p>
<div class="codehilite"><pre><span></span><code><span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="o">-</span><span class="n">q</span> <span class="o">--</span><span class="n">upgrade</span> <span class="n">keras</span><span class="o">-</span><span class="n">cv</span>
<span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="o">-</span><span class="n">q</span> <span class="o">--</span><span class="n">upgrade</span> <span class="n">keras</span>  <span class="c1"># Upgrade to Keras 3.</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">keras_cv</span>
<span class="kn">import</span> <span class="nn">keras</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
</code></pre></div>

<hr />
<h2 id="introduction">Introduction</h2>
<p>Unlike most tutorials, where we first explain a topic then show how to implement it,
with text-to-image generation it is easier to show instead of tell.</p>
<p>Check out the power of <code>keras_cv.models.StableDiffusion()</code>.</p>
<p>First, we construct a model:</p>
<div class="codehilite"><pre><span></span><code><span class="n">model</span> <span class="o">=</span> <span class="n">keras_cv</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">StableDiffusion</span><span class="p">(</span>
    <span class="n">img_width</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">img_height</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">jit_compile</span><span class="o">=</span><span class="kc">False</span>
<span class="p">)</span>
</code></pre></div>

<div class="k-default-codeblock">

<div class="codehilite"><pre><span></span><code>By using this model checkpoint, you acknowledge that its usage is subject to the terms of the CreativeML Open RAIL-M license at https://raw.githubusercontent.com/CompVis/stable-diffusion/main/LICENSE
</code></pre></div>


</div>
<p>Next, we give it a prompt:</p>
<div class="codehilite"><pre><span></span><code><span class="n">images</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">text_to_image</span><span class="p">(</span><span class="s2">&quot;photograph of an astronaut riding a horse&quot;</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">plot_images</span><span class="p">(</span><span class="n">images</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">images</span><span class="p">)):</span>
        <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">images</span><span class="p">),</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">images</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s2">&quot;off&quot;</span><span class="p">)</span>


<span class="n">plot_images</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
</code></pre></div>

<div class="k-default-codeblock">

<div class="codehilite"><pre><span></span><code> 50/50 ━━━━━━━━━━━━━━━━━━━━ 63s 211ms/step
</code></pre></div>


</div>

<p><img alt="png" src="/img/guides/generate_images_with_stable_diffusion/generate_images_with_stable_diffusion_7_1.png" /></p>
<p>Pretty incredible!</p>
<p>But that's not all this model can do.  Let's try a more complex prompt:</p>
<div class="codehilite"><pre><span></span><code><span class="n">images</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">text_to_image</span><span class="p">(</span>
    <span class="s2">&quot;cute magical flying dog, fantasy art, &quot;</span>
    <span class="s2">&quot;golden color, high quality, highly detailed, elegant, sharp focus, &quot;</span>
    <span class="s2">&quot;concept art, character concepts, digital painting, mystery, adventure&quot;</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">plot_images</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
</code></pre></div>

<div class="k-default-codeblock">

<div class="codehilite"><pre><span></span><code> 50/50 ━━━━━━━━━━━━━━━━━━━━ 10s 209ms/step
</code></pre></div>


</div>

<p><img alt="png" src="/img/guides/generate_images_with_stable_diffusion/generate_images_with_stable_diffusion_9_1.png" /></p>
<p>The possibilities are literally endless (or at least extend to the boundaries of
Stable Diffusion's latent manifold).</p>
<hr />
<h2 id="wait-how-does-this-even-work">Wait, how does this even work?</h2>
<p>Unlike what you might expect at this point, Stable Diffusion doesn't actually run on magic.
It's a kind of "latent diffusion model". Let's dig into what that means.</p>
<p>You may be familiar with the idea of <em>super-resolution</em>:
it's possible to train a deep learning model to <em>denoise</em> an input image &ndash; and thereby turn it into a higher-resolution
version. The deep learning model doesn't do this by magically recovering the information that's missing from the noisy, low-resolution
input &ndash; rather, the model uses its training data distribution to hallucinate the visual details that would be most likely
given the input. To learn more about super-resolution, you can check out the following Keras.io tutorials:</p>
<ul>
<li><a href="https://keras.io/examples/vision/super_resolution_sub_pixel/">Image Super-Resolution using an Efficient Sub-Pixel CNN</a></li>
<li><a href="https://keras.io/examples/vision/edsr/">Enhanced Deep Residual Networks for single-image super-resolution</a></li>
</ul>
<p><img alt="Super-resolution" src="https://i.imgur.com/M0XdqOo.png" /></p>
<p>When you push this idea to the limit, you may start asking &ndash; what if we just run such a model on pure noise?
The model would then "denoise the noise" and start hallucinating a brand new image. By repeating the process multiple
times, you can get turn a small patch of noise into an increasingly clear and high-resolution artificial picture.</p>
<p>This is the key idea of latent diffusion, proposed in
<a href="https://arxiv.org/abs/2112.10752">High-Resolution Image Synthesis with Latent Diffusion Models</a> in 2020.
To understand diffusion in depth, you can check the Keras.io tutorial
<a href="https://keras.io/examples/generative/ddim/">Denoising Diffusion Implicit Models</a>.</p>
<p><img alt="Denoising diffusion" src="https://i.imgur.com/FSCKtZq.gif" /></p>
<p>Now, to go from latent diffusion to a text-to-image system,
you still need to add one key feature: the ability to control the generated visual contents via prompt keywords.
This is done via "conditioning", a classic deep learning technique which consists of concatenating to the
noise patch a vector that represents a bit of text, then training the model on a dataset of {image: caption} pairs.</p>
<p>This gives rise to the Stable Diffusion architecture. Stable Diffusion consists of three parts:</p>
<ul>
<li>A text encoder, which turns your prompt into a latent vector.</li>
<li>A diffusion model, which repeatedly "denoises" a 64x64 latent image patch.</li>
<li>A decoder, which turns the final 64x64 latent patch into a higher-resolution 512x512 image.</li>
</ul>
<p>First, your text prompt gets projected into a latent vector space by the text encoder,
which is simply a pretrained, frozen language model. Then that prompt vector is concatenated
to a randomly generated noise patch, which is repeatedly "denoised" by the diffusion model over a series
of "steps" (the more steps you run the clearer and nicer your image will be &ndash; the default value is 50 steps).</p>
<p>Finally, the 64x64 latent image is sent through the decoder to properly render it in high resolution.</p>
<p><img alt="The Stable Diffusion architecture" src="https://i.imgur.com/2uC8rYJ.png" /></p>
<p>All-in-all, it's a pretty simple system &ndash; the Keras implementation
fits in four files that represent less than 500 lines of code in total:</p>
<ul>
<li><a href="https://github.com/keras-team/keras-cv/blob/master/keras_cv/models/stable_diffusion/text_encoder.py">text_encoder.py</a>: 87 LOC</li>
<li><a href="https://github.com/keras-team/keras-cv/blob/master/keras_cv/models/stable_diffusion/diffusion_model.py">diffusion_model.py</a>: 181 LOC</li>
<li><a href="https://github.com/keras-team/keras-cv/blob/master/keras_cv/models/stable_diffusion/decoder.py">decoder.py</a>: 86 LOC</li>
<li><a href="https://github.com/keras-team/keras-cv/blob/master/keras_cv/models/stable_diffusion/stable_diffusion.py">stable_diffusion.py</a>: 106 LOC</li>
</ul>
<p>But this relatively simple system starts looking like magic once you train on billions of pictures and their captions.
As Feynman said about the universe: <em>"It's not complicated, it's just a lot of it!"</em></p>
<hr />
<h2 id="perks-of-kerascv">Perks of KerasCV</h2>
<p>With several implementations of Stable Diffusion publicly available why should you use
<a href="/api/keras_cv/models/tasks/stable_diffusion#stablediffusion-class"><code>keras_cv.models.StableDiffusion</code></a>?</p>
<p>Aside from the easy-to-use API, KerasCV's Stable Diffusion model comes
with some powerful advantages, including:</p>
<ul>
<li>Graph mode execution</li>
<li>XLA compilation through <code>jit_compile=True</code></li>
<li>Support for mixed precision computation</li>
</ul>
<p>When these are combined, the KerasCV Stable Diffusion model runs orders of magnitude
faster than naive implementations.  This section shows how to enable all of these
features, and the resulting performance gain yielded from using them.</p>
<p>For the purposes of comparison, we ran benchmarks comparing the runtime of the
<a href="https://github.com/huggingface/diffusers">HuggingFace diffusers</a> implementation of
Stable Diffusion against the KerasCV implementation.
Both implementations were tasked to generate 3 images with a step count of 50 for each
image.  In this benchmark, we used a Tesla T4 GPU.</p>
<p><a href="https://github.com/LukeWood/stable-diffusion-performance-benchmarks">All of our benchmarks are open source on GitHub, and may be re-run on Colab to
reproduce the results.</a>
The results from the benchmark are displayed in the table below:</p>
<table>
<thead>
<tr>
<th>GPU</th>
<th>Model</th>
<th>Runtime</th>
</tr>
</thead>
<tbody>
<tr>
<td>Tesla T4</td>
<td>KerasCV (Warm Start)</td>
<td><strong>28.97s</strong></td>
</tr>
<tr>
<td>Tesla T4</td>
<td>diffusers (Warm Start)</td>
<td>41.33s</td>
</tr>
<tr>
<td>Tesla V100</td>
<td>KerasCV (Warm Start)</td>
<td><strong>12.45</strong></td>
</tr>
<tr>
<td>Tesla V100</td>
<td>diffusers (Warm Start)</td>
<td>12.72</td>
</tr>
</tbody>
</table>
<p>30% improvement in execution time on the Tesla T4!.  While the improvement is much lower
on the V100, we generally expect the results of the benchmark to consistently favor the KerasCV
across all NVIDIA GPUs.</p>
<p>For the sake of completeness, both cold-start and warm-start generation times are
reported. Cold-start execution time includes the one-time cost of model creation and compilation,
and is therefore negligible in a production environment (where you would reuse the same model instance
many times). Regardless, here are the cold-start numbers:</p>
<table>
<thead>
<tr>
<th>GPU</th>
<th>Model</th>
<th>Runtime</th>
</tr>
</thead>
<tbody>
<tr>
<td>Tesla T4</td>
<td>KerasCV (Cold Start)</td>
<td>83.47s</td>
</tr>
<tr>
<td>Tesla T4</td>
<td>diffusers (Cold Start)</td>
<td>46.27s</td>
</tr>
<tr>
<td>Tesla V100</td>
<td>KerasCV (Cold Start)</td>
<td>76.43</td>
</tr>
<tr>
<td>Tesla V100</td>
<td>diffusers (Cold Start)</td>
<td>13.90</td>
</tr>
</tbody>
</table>
<p>While the runtime results from running this guide may vary, in our testing the KerasCV
implementation of Stable Diffusion is significantly faster than its PyTorch counterpart.
This may be largely attributed to XLA compilation.</p>
<p><strong>Note: The performance benefits of each optimization can vary
significantly between hardware setups.</strong></p>
<p>To get started, let's first benchmark our unoptimized model:</p>
<div class="codehilite"><pre><span></span><code><span class="n">benchmark_result</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">images</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">text_to_image</span><span class="p">(</span>
    <span class="s2">&quot;A cute otter in a rainbow whirlpool holding shells, watercolor&quot;</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">end</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">benchmark_result</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="s2">&quot;Standard&quot;</span><span class="p">,</span> <span class="n">end</span> <span class="o">-</span> <span class="n">start</span><span class="p">])</span>
<span class="n">plot_images</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Standard model: </span><span class="si">{</span><span class="p">(</span><span class="n">end</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">start</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> seconds&quot;</span><span class="p">)</span>
<span class="n">keras</span><span class="o">.</span><span class="n">backend</span><span class="o">.</span><span class="n">clear_session</span><span class="p">()</span>  <span class="c1"># Clear session to preserve memory.</span>
</code></pre></div>

<div class="k-default-codeblock">

<div class="codehilite"><pre><span></span><code> 50/50 ━━━━━━━━━━━━━━━━━━━━ 10s 209ms/step
Standard model: 10.57 seconds
</code></pre></div>


</div>

<p><img alt="png" src="/img/guides/generate_images_with_stable_diffusion/generate_images_with_stable_diffusion_13_1.png" /></p>
<h3 id="mixed-precision">Mixed precision</h3>
<p>"Mixed precision" consists of performing computation using <code>float16</code>
precision, while storing weights in the <code>float32</code> format.
This is done to take advantage of the fact that <code>float16</code> operations are backed by
significantly faster kernels than their <code>float32</code> counterparts on modern NVIDIA GPUs.</p>
<p>Enabling mixed precision computation in Keras
(and therefore for <a href="/api/keras_cv/models/tasks/stable_diffusion#stablediffusion-class"><code>keras_cv.models.StableDiffusion</code></a>) is as simple as calling:</p>
<div class="codehilite"><pre><span></span><code><span class="n">keras</span><span class="o">.</span><span class="n">mixed_precision</span><span class="o">.</span><span class="n">set_global_policy</span><span class="p">(</span><span class="s2">&quot;mixed_float16&quot;</span><span class="p">)</span>
</code></pre></div>

<p>That's all.  Out of the box - it just works.</p>
<div class="codehilite"><pre><span></span><code><span class="n">model</span> <span class="o">=</span> <span class="n">keras_cv</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">StableDiffusion</span><span class="p">(</span><span class="n">jit_compile</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Compute dtype:&quot;</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">diffusion_model</span><span class="o">.</span><span class="n">compute_dtype</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span>
    <span class="s2">&quot;Variable dtype:&quot;</span><span class="p">,</span>
    <span class="n">model</span><span class="o">.</span><span class="n">diffusion_model</span><span class="o">.</span><span class="n">variable_dtype</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div>

<div class="k-default-codeblock">

<div class="codehilite"><pre><span></span><code>By using this model checkpoint, you acknowledge that its usage is subject to the terms of the CreativeML Open RAIL-M license at https://raw.githubusercontent.com/CompVis/stable-diffusion/main/LICENSE
Compute dtype: float16
Variable dtype: float32
</code></pre></div>


</div>
<p>As you can see, the model constructed above now uses mixed precision computation;
leveraging the speed of <code>float16</code> operations for computation, while storing variables
in <code>float32</code> precision.</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># Warm up model to run graph tracing before benchmarking.</span>
<span class="n">model</span><span class="o">.</span><span class="n">text_to_image</span><span class="p">(</span><span class="s2">&quot;warming up the model&quot;</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

<span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">images</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">text_to_image</span><span class="p">(</span>
    <span class="s2">&quot;a cute magical flying dog, fantasy art, &quot;</span>
    <span class="s2">&quot;golden color, high quality, highly detailed, elegant, sharp focus, &quot;</span>
    <span class="s2">&quot;concept art, character concepts, digital painting, mystery, adventure&quot;</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">end</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">benchmark_result</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="s2">&quot;Mixed Precision&quot;</span><span class="p">,</span> <span class="n">end</span> <span class="o">-</span> <span class="n">start</span><span class="p">])</span>
<span class="n">plot_images</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Mixed precision model: </span><span class="si">{</span><span class="p">(</span><span class="n">end</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">start</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> seconds&quot;</span><span class="p">)</span>
<span class="n">keras</span><span class="o">.</span><span class="n">backend</span><span class="o">.</span><span class="n">clear_session</span><span class="p">()</span>
</code></pre></div>

<div class="k-default-codeblock">

<div class="codehilite"><pre><span></span><code> 50/50 ━━━━━━━━━━━━━━━━━━━━ 42s 132ms/step
 50/50 ━━━━━━━━━━━━━━━━━━━━ 6s 129ms/step
Mixed precision model: 6.65 seconds
</code></pre></div>


</div>

<p><img alt="png" src="/img/guides/generate_images_with_stable_diffusion/generate_images_with_stable_diffusion_19_1.png" /></p>
<h3 id="xla-compilation">XLA Compilation</h3>
<p>TensorFlow and JAX come with the
<a href="https://www.tensorflow.org/xla">XLA: Accelerated Linear Algebra</a> compiler built-in.
<a href="/api/keras_cv/models/tasks/stable_diffusion#stablediffusion-class"><code>keras_cv.models.StableDiffusion</code></a> supports a <code>jit_compile</code> argument out of the box.
Setting this argument to <code>True</code> enables XLA compilation, resulting in a significant
speed-up.</p>
<p>Let's use this below:</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># Set back to the default for benchmarking purposes.</span>
<span class="n">keras</span><span class="o">.</span><span class="n">mixed_precision</span><span class="o">.</span><span class="n">set_global_policy</span><span class="p">(</span><span class="s2">&quot;float32&quot;</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">keras_cv</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">StableDiffusion</span><span class="p">(</span><span class="n">jit_compile</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="c1"># Before we benchmark the model, we run inference once to make sure the TensorFlow</span>
<span class="c1"># graph has already been traced.</span>
<span class="n">images</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">text_to_image</span><span class="p">(</span><span class="s2">&quot;An avocado armchair&quot;</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plot_images</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
</code></pre></div>

<div class="k-default-codeblock">

<div class="codehilite"><pre><span></span><code>By using this model checkpoint, you acknowledge that its usage is subject to the terms of the CreativeML Open RAIL-M license at https://raw.githubusercontent.com/CompVis/stable-diffusion/main/LICENSE
 50/50 ━━━━━━━━━━━━━━━━━━━━ 48s 209ms/step
</code></pre></div>


</div>

<p><img alt="png" src="/img/guides/generate_images_with_stable_diffusion/generate_images_with_stable_diffusion_21_1.png" /></p>
<p>Let's benchmark our XLA model:</p>
<div class="codehilite"><pre><span></span><code><span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">images</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">text_to_image</span><span class="p">(</span>
    <span class="s2">&quot;A cute otter in a rainbow whirlpool holding shells, watercolor&quot;</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">end</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">benchmark_result</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="s2">&quot;XLA&quot;</span><span class="p">,</span> <span class="n">end</span> <span class="o">-</span> <span class="n">start</span><span class="p">])</span>
<span class="n">plot_images</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;With XLA: </span><span class="si">{</span><span class="p">(</span><span class="n">end</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">start</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> seconds&quot;</span><span class="p">)</span>
<span class="n">keras</span><span class="o">.</span><span class="n">backend</span><span class="o">.</span><span class="n">clear_session</span><span class="p">()</span>
</code></pre></div>

<div class="k-default-codeblock">

<div class="codehilite"><pre><span></span><code> 50/50 ━━━━━━━━━━━━━━━━━━━━ 11s 210ms/step
With XLA: 10.63 seconds
</code></pre></div>


</div>

<p><img alt="png" src="/img/guides/generate_images_with_stable_diffusion/generate_images_with_stable_diffusion_23_1.png" /></p>
<p>On an A100 GPU, we get about a 2x speedup.  Fantastic!</p>
<hr />
<h2 id="putting-it-all-together">Putting it all together</h2>
<p>So, how do you assemble the world's most performant stable diffusion inference
pipeline (as of September 2022).</p>
<p>With these two lines of code:</p>
<div class="codehilite"><pre><span></span><code><span class="n">keras</span><span class="o">.</span><span class="n">mixed_precision</span><span class="o">.</span><span class="n">set_global_policy</span><span class="p">(</span><span class="s2">&quot;mixed_float16&quot;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">keras_cv</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">StableDiffusion</span><span class="p">(</span><span class="n">jit_compile</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div>

<div class="k-default-codeblock">

<div class="codehilite"><pre><span></span><code>By using this model checkpoint, you acknowledge that its usage is subject to the terms of the CreativeML Open RAIL-M license at https://raw.githubusercontent.com/CompVis/stable-diffusion/main/LICENSE
</code></pre></div>


</div>
<p>And to use it...</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># Let&#39;s make sure to warm up the model</span>
<span class="n">images</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">text_to_image</span><span class="p">(</span>
    <span class="s2">&quot;Teddy bears conducting machine learning research&quot;</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">plot_images</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
</code></pre></div>

<div class="k-default-codeblock">

<div class="codehilite"><pre><span></span><code> 50/50 ━━━━━━━━━━━━━━━━━━━━ 48s 131ms/step
</code></pre></div>


</div>

<p><img alt="png" src="/img/guides/generate_images_with_stable_diffusion/generate_images_with_stable_diffusion_28_1.png" /></p>
<p>Exactly how fast is it?
Let's find out!</p>
<div class="codehilite"><pre><span></span><code><span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">images</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">text_to_image</span><span class="p">(</span>
    <span class="s2">&quot;A mysterious dark stranger visits the great pyramids of egypt, &quot;</span>
    <span class="s2">&quot;high quality, highly detailed, elegant, sharp focus, &quot;</span>
    <span class="s2">&quot;concept art, character concepts, digital painting&quot;</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">end</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">benchmark_result</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="s2">&quot;XLA + Mixed Precision&quot;</span><span class="p">,</span> <span class="n">end</span> <span class="o">-</span> <span class="n">start</span><span class="p">])</span>
<span class="n">plot_images</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;XLA + mixed precision: </span><span class="si">{</span><span class="p">(</span><span class="n">end</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">start</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> seconds&quot;</span><span class="p">)</span>
</code></pre></div>

<div class="k-default-codeblock">

<div class="codehilite"><pre><span></span><code> 50/50 ━━━━━━━━━━━━━━━━━━━━ 6s 130ms/step
XLA + mixed precision: 6.66 seconds
</code></pre></div>


</div>

<p><img alt="png" src="/img/guides/generate_images_with_stable_diffusion/generate_images_with_stable_diffusion_30_1.png" /></p>
<p>Let's check out the results:</p>
<div class="codehilite"><pre><span></span><code><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{:&lt;22}</span><span class="s2"> </span><span class="si">{:&lt;22}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s2">&quot;Model&quot;</span><span class="p">,</span> <span class="s2">&quot;Runtime&quot;</span><span class="p">))</span>
<span class="k">for</span> <span class="n">result</span> <span class="ow">in</span> <span class="n">benchmark_result</span><span class="p">:</span>
    <span class="n">name</span><span class="p">,</span> <span class="n">runtime</span> <span class="o">=</span> <span class="n">result</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{:&lt;22}</span><span class="s2"> </span><span class="si">{:&lt;22}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">runtime</span><span class="p">))</span>
</code></pre></div>

<div class="k-default-codeblock">

<div class="codehilite"><pre><span></span><code>Model                  Runtime               
Standard               10.572920799255371    
Mixed Precision        6.651048421859741     
XLA                    10.632121562957764    
XLA + Mixed Precision  6.659237861633301     
</code></pre></div>


</div>
<p>It only took our fully-optimized model four seconds to generate three novel images from
a text prompt on an A100 GPU.</p>
<hr />
<h2 id="conclusions">Conclusions</h2>
<p>KerasCV offers a state-of-the-art implementation of Stable Diffusion &ndash; and
through the use of XLA and mixed precision, it delivers the fastest Stable Diffusion pipeline available as of September 2022.</p>
<p>Normally, at the end of a keras.io tutorial we leave you with some future directions to continue in to learn.
This time, we leave you with one idea:</p>
<p><strong>Go run your own prompts through the model! It is an absolute blast!</strong></p>
<p>If you have your own NVIDIA GPU, or a M1 MacBookPro, you can also run the model locally on your machine.
(Note that when running on a M1 MacBookPro, you should not enable mixed precision, as it is not yet well supported
by Apple's Metal runtime.)</p>
        </div>
        
        <div class='k-outline'>
          
            <div class='k-outline-depth-1'>
              
              <a href='#highperformance-image-generation-using-stable-diffusion-in-kerascv'>High-performance image generation using Stable Diffusion in KerasCV</a>
            </div>
          
            <div class='k-outline-depth-2'>
               ◆  
              <a href='#overview'>Overview</a>
            </div>
          
            <div class='k-outline-depth-2'>
               ◆  
              <a href='#introduction'>Introduction</a>
            </div>
          
            <div class='k-outline-depth-2'>
               ◆  
              <a href='#wait-how-does-this-even-work'>Wait, how does this even work?</a>
            </div>
          
            <div class='k-outline-depth-2'>
               ◆  
              <a href='#perks-of-kerascv'>Perks of KerasCV</a>
            </div>
          
            <div class='k-outline-depth-3'>
              
              <a href='#mixed-precision'>Mixed precision</a>
            </div>
          
            <div class='k-outline-depth-3'>
              
              <a href='#xla-compilation'>XLA Compilation</a>
            </div>
          
            <div class='k-outline-depth-2'>
               ◆  
              <a href='#putting-it-all-together'>Putting it all together</a>
            </div>
          
            <div class='k-outline-depth-2'>
               ◆  
              <a href='#conclusions'>Conclusions</a>
            </div>
          
        </div>
        
      </div>
    </div>

  </div>

</body>

<footer style="float: left; width: 100%; padding: 1em; border-top: solid 1px #bbb;">
  <a href="https://policies.google.com/terms">Terms</a> | <a href="https://policies.google.com/privacy">Privacy</a>
</footer>

</html>