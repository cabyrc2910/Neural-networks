<!DOCTYPE html>
<html lang="ru" data-vue-meta="%7B%22lang%22:%7B%22ssr%22:%22ru%22%7D%7D">
<head >
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width,initial-scale=1.0,viewport-fit=cover,maximum-scale=1,user-scalable=0">
  <meta name="referrer" content="unsafe-url">
  <title>Введение в библиотеку Transformers и платформу Hugging Face / Хабр</title>
  <style>
    /* cyrillic-ext */
    @font-face {
      font-family: 'Fira Sans';
      font-style: normal;
      font-weight: 500;
      font-display: swap;
      src: url(https://fonts.gstatic.com/s/firasans/v11/va9B4kDNxMZdWfMOD5VnZKveSxf6TF0.woff2) format('woff2');
      unicode-range: U+0460-052F, U+1C80-1C88, U+20B4, U+2DE0-2DFF, U+A640-A69F, U+FE2E-FE2F;
    }

    /* cyrillic */
    @font-face {
      font-family: 'Fira Sans';
      font-style: normal;
      font-weight: 500;
      font-display: swap;
      src: url(https://fonts.gstatic.com/s/firasans/v11/va9B4kDNxMZdWfMOD5VnZKveQhf6TF0.woff2) format('woff2');
      unicode-range: U+0400-045F, U+0490-0491, U+04B0-04B1, U+2116;
    }

    /* latin-ext */
    @font-face {
      font-family: 'Fira Sans';
      font-style: normal;
      font-weight: 500;
      font-display: swap;
      src: url(https://fonts.gstatic.com/s/firasans/v11/va9B4kDNxMZdWfMOD5VnZKveSBf6TF0.woff2) format('woff2');
      unicode-range: U+0100-024F, U+0259, U+1E00-1EFF, U+2020, U+20A0-20AB, U+20AD-20CF, U+2113, U+2C60-2C7F, U+A720-A7FF;
    }

    /* latin */
    @font-face {
      font-family: 'Fira Sans';
      font-style: normal;
      font-weight: 500;
      font-display: swap;
      src: url(https://fonts.gstatic.com/s/firasans/v11/va9B4kDNxMZdWfMOD5VnZKveRhf6.woff2) format('woff2');
      unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
    }

    /* non-breaking hyphen */
    @font-face {
      font-family: 'Fira Sans';
      font-style: normal;
      font-weight: 500;
      font-display: swap;
      src: url(https://fonts.gstatic.com/l/font?kit=KFOlCnqEu92Fr1MmEU9vBh0_IsHAlmrO6g&skey=ee881451c540fdec&v=v29) format('woff2');
      unicode-range: U+02011;
    }
  </style>
  <link rel="preload" href="https://assets.habr.com/habr-web/css/theme/light.css" as="style"/><link id="light-colors" rel="stylesheet" href="https://assets.habr.com/habr-web/css/theme/light.css" media="all" />
  <link rel="preload" href="https://assets.habr.com/habr-web/css/chunk-vendors.5e25635d.css" as="style"><link rel="preload" href="https://assets.habr.com/habr-web/js/chunk-vendors.02fc1736.js" as="script"><link rel="preload" href="https://assets.habr.com/habr-web/css/app.a69a184a.css" as="style"><link rel="preload" href="https://assets.habr.com/habr-web/js/app.6cc13d0e.js" as="script"><link rel="preload" href="https://assets.habr.com/habr-web/js/7298.c8f1d73c.js" as="script">
  <link rel="stylesheet" href="https://assets.habr.com/habr-web/css/chunk-vendors.5e25635d.css"><link rel="stylesheet" href="https://assets.habr.com/habr-web/css/app.a69a184a.css">
  <script>window.i18nFetch = new Promise((res, rej) => {
          const xhr = new XMLHttpRequest();
          xhr.open('GET', '/js/i18n/ru-compiled.81983413baaf34ca3062d1338d3534a8.json');
          xhr.responseType = 'json';
          xhr.onload = function(e) {
            if (this.status === 200) {
              res({ru: xhr.response});
            } else {
              rej(e);
            }
          };
          xhr.send();
        });</script>
  
  <script data-vue-meta="ssr" type="application/ld+json" data-vmid="ldjson-schema">{"@context":"http:\/\/schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/habr.com\/ru\/articles\/704592\/"},"headline":"Введение в библиотеку Transformers и платформу Hugging Face","datePublished":"2022-12-09T19:23:20+03:00","dateModified":"2022-12-10T19:07:37+03:00","author":{"@type":"Person","name":"Вячеслав"},"publisher":{"@type":"Organization","name":"Habr","logo":{"@type":"ImageObject","url":"https:\/\/habrastorage.org\/webt\/a_\/lk\/9m\/a_lk9mjkccjox-zccjrpfolmkmq.png"}},"description":"Исходники: https:\/\/github.com\/huggingface\/transformersДокументация: https:\/\/huggingface.co\/docs\/transformers\/main\/en\/indexПлатформа Hugging Face это коллекция го...","url":"https:\/\/habr.com\/ru\/articles\/704592\/#post-content-body","about":["h_python","h_data_mining","h_bigdata","h_machine_learning","h_artificial_intelligence","f_develop","f_popsci"],"image":["https:\/\/habr.com\/share\/publication\/704592\/5ab0d3a9f222402a1a974810e134c148\/","https:\/\/habrastorage.org\/getpro\/habr\/upload_files\/1fa\/262\/9d5\/1fa2629d5c8a96de953971af60a11162.png","https:\/\/habrastorage.org\/getpro\/habr\/upload_files\/1e7\/159\/83a\/1e715983a83f6bed950edcd450cc5e5e.png","https:\/\/habrastorage.org\/getpro\/habr\/upload_files\/090\/161\/072\/090161072e1b347bed5e63e490f0834f.png","https:\/\/habrastorage.org\/getpro\/habr\/upload_files\/98a\/b09\/e32\/98ab09e32d56a658ff21ddab03c86b57.png","https:\/\/habrastorage.org\/getpro\/habr\/upload_files\/298\/70a\/e7f\/29870ae7f525c03a7622c082bb407ac6.png"]}</script>
  <style>.grecaptcha-badge{visibility: hidden;}</style>
  <meta name="habr-version" content="2.169.0">
  <meta name="csrf-token" content="HzRlcsWF-ZUxrA8acvo85UnyCuCfO4h4OtGI">
  <meta data-vue-meta="ssr" property="fb:app_id" content="444736788986613"><meta data-vue-meta="ssr" property="fb:pages" content="472597926099084"><meta data-vue-meta="ssr" name="twitter:card" content="summary_large_image"><meta data-vue-meta="ssr" name="twitter:site" content="@habr_com"><meta data-vue-meta="ssr" property="og:site_name" content="Хабр" data-vmid="og:site_name"><meta data-vue-meta="ssr" property="og:title" content="Введение в библиотеку Transformers и платформу Hugging Face" data-vmid="og:title"><meta data-vue-meta="ssr" name="twitter:title" content="Введение в библиотеку Transformers и платформу Hugging Face" data-vmid="twitter:title"><meta data-vue-meta="ssr" name="aiturec:title" content="Введение в библиотеку Transformers и платформу Hugging Face" data-vmid="aiturec:title"><meta data-vue-meta="ssr" name="description" content="Исходники: https://github.com/huggingface/transformers Документация: https://huggingface.co/docs/transformers/main/en/index Платформа Hugging Face это коллекция готовых современных предварительно..." data-vmid="description"><meta data-vue-meta="ssr" itemprop="description" content="Исходники: https://github.com/huggingface/transformers Документация: https://huggingface.co/docs/transformers/main/en/index Платформа Hugging Face это коллекция готовых современных предварительно..." data-vmid="description:itemprop"><meta data-vue-meta="ssr" property="og:description" content="Исходники: https://github.com/huggingface/transformers Документация: https://huggingface.co/docs/transformers/main/en/index Платформа Hugging Face это коллекция готовых современных предварительно..." data-vmid="og:description"><meta data-vue-meta="ssr" name="twitter:description" content="Исходники: https://github.com/huggingface/transformers Документация: https://huggingface.co/docs/transformers/main/en/index Платформа Hugging Face это коллекция готовых современных предварительно..." data-vmid="twitter:description"><meta data-vue-meta="ssr" property="aiturec:description" content="Исходники: https://github.com/huggingface/transformers Документация: https://huggingface.co/docs/transformers/main/en/index Платформа Hugging Face это коллекция готовых современных предварительно..." data-vmid="aiturec:description"><meta data-vue-meta="ssr" itemprop="image" content="https://habrastorage.org/getpro/habr/upload_files/60f/1a5/aaf/60f1a5aafe57e4d36ffbc1c3ac0e5113.png" data-vmid="image:itemprop"><meta data-vue-meta="ssr" property="og:image" content="https://habrastorage.org/getpro/habr/upload_files/60f/1a5/aaf/60f1a5aafe57e4d36ffbc1c3ac0e5113.png" data-vmid="og:image"><meta data-vue-meta="ssr" property="og:image:width" content="1200" data-vmid="og:image:width"><meta data-vue-meta="ssr" property="og:image:height" content="630" data-vmid="og:image:height"><meta data-vue-meta="ssr" property="aiturec:image" content="https://habrastorage.org/getpro/habr/upload_files/60f/1a5/aaf/60f1a5aafe57e4d36ffbc1c3ac0e5113.png" data-vmid="aiturec:image"><meta data-vue-meta="ssr" name="twitter:image" content="https://habrastorage.org/getpro/habr/upload_files/60f/1a5/aaf/60f1a5aafe57e4d36ffbc1c3ac0e5113.png" data-vmid="twitter:image"><meta data-vue-meta="ssr" property="vk:image" content="https://habrastorage.org/getpro/habr/upload_files/60f/1a5/aaf/60f1a5aafe57e4d36ffbc1c3ac0e5113.png?format=vk" data-vmid="vk:image"><meta data-vue-meta="ssr" property="aiturec:item_id" content="704592" data-vmid="aiturec:item_id"><meta data-vue-meta="ssr" property="aiturec:datetime" content="2022-12-09T16:23:20.000Z" data-vmid="aiturec:datetime"><meta data-vue-meta="ssr" content="https://habr.com/ru/articles/704592/" property="og:url" data-vmid="og:url"><meta data-vue-meta="ssr" property="og:type" content="article" data-vmid="og:type"><meta data-vue-meta="ssr" property="og:locale" content="ru_RU" data-vmid="og:locale"><meta data-vue-meta="ssr" name="keywords" content="transformers, bert, gpt, machine leraning, data science, deep learning">
  <link data-vue-meta="ssr" href="https://habr.com/ru/rss/publications/704592/?fl=ru" type="application/rss+xml" title="" rel="alternate" name="rss"><link data-vue-meta="ssr" href="https://habr.com/ru/articles/704592/" rel="canonical" data-vmid="canonical"><link data-vue-meta="ssr" rel="image_src" href="https://habrastorage.org/getpro/habr/upload_files/60f/1a5/aaf/60f1a5aafe57e4d36ffbc1c3ac0e5113.png" data-vmid="image:href"><link data-vue-meta="ssr" rel="amphtml" href="https://habr.com/ru/amp/publications/704592/">
  <meta name="apple-mobile-web-app-status-bar-style" content="#303b44">
  <meta name="msapplication-TileColor" content="#629FBC">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="mobile-web-app-capable" content="yes">
  <link
    rel="shortcut icon"
    type="image/png"
    sizes="16x16"
    href="https://assets.habr.com/habr-web/img/favicons/favicon-16.png"
  >
  <link
    rel="shortcut icon"
    type="image/png"
    sizes="32x32"
    href="https://assets.habr.com/habr-web/img/favicons/favicon-32.png"
  >
  <link
    rel="apple-touch-icon"
    type="image/png"
    sizes="76x76"
    href="https://assets.habr.com/habr-web/img/favicons/apple-touch-icon-76.png"
  >
  <link
    rel="apple-touch-icon"
    type="image/png"
    sizes="120x120"
    href="https://assets.habr.com/habr-web/img/favicons/apple-touch-icon-120.png"
  >
  <link
    rel="apple-touch-icon"
    type="image/png"
    sizes="152x152"
    href="https://assets.habr.com/habr-web/img/favicons/apple-touch-icon-152.png"
  >
  <link
    rel="apple-touch-icon"
    type="image/png"
    sizes="180x180"
    href="https://assets.habr.com/habr-web/img/favicons/apple-touch-icon-180.png"
  >
  <link
    rel="apple-touch-icon"
    type="image/png"
    sizes="256x256"
    href="https://assets.habr.com/habr-web/img/favicons/apple-touch-icon-256.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 320px) and (device-height: 568px) and (-webkit-device-pixel-ratio: 2) and (orientation: landscape)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_1136x640.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 375px) and (device-height: 812px) and (-webkit-device-pixel-ratio: 3) and (orientation: landscape)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_2436x1125.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 414px) and (device-height: 896px) and (-webkit-device-pixel-ratio: 2) and (orientation: landscape)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_1792x828.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 414px) and (device-height: 896px) and (-webkit-device-pixel-ratio: 2) and (orientation: portrait)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_828x1792.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 375px) and (device-height: 667px) and (-webkit-device-pixel-ratio: 2) and (orientation: landscape)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_1334x750.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 414px) and (device-height: 896px) and (-webkit-device-pixel-ratio: 3) and (orientation: portrait)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_1242x2668.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 414px) and (device-height: 736px) and (-webkit-device-pixel-ratio: 3) and (orientation: landscape)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_2208x1242.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 375px) and (device-height: 812px) and (-webkit-device-pixel-ratio: 3) and (orientation: portrait)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_1125x2436.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 414px) and (device-height: 736px) and (-webkit-device-pixel-ratio: 3) and (orientation: portrait)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_1242x2208.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 1024px) and (device-height: 1366px) and (-webkit-device-pixel-ratio: 2) and (orientation: landscape)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_2732x2048.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 414px) and (device-height: 896px) and (-webkit-device-pixel-ratio: 3) and (orientation: landscape)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_2688x1242.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 834px) and (device-height: 1112px) and (-webkit-device-pixel-ratio: 2) and (orientation: landscape)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_2224x1668.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 375px) and (device-height: 667px) and (-webkit-device-pixel-ratio: 2) and (orientation: portrait)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_750x1334.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 1024px) and (device-height: 1366px) and (-webkit-device-pixel-ratio: 2) and (orientation: portrait)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_2048x2732.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 834px) and (device-height: 1194px) and (-webkit-device-pixel-ratio: 2) and (orientation: landscape)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_2388x1668.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 834px) and (device-height: 1112px) and (-webkit-device-pixel-ratio: 2) and (orientation: portrait)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_1668x2224.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 320px) and (device-height: 568px) and (-webkit-device-pixel-ratio: 2) and (orientation: portrait)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_640x1136.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 834px) and (device-height: 1194px) and (-webkit-device-pixel-ratio: 2) and (orientation: portrait)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_1668x2388.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 768px) and (device-height: 1024px) and (-webkit-device-pixel-ratio: 2) and (orientation: landscape)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_2048x1536.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 768px) and (device-height: 1024px) and (-webkit-device-pixel-ratio: 2) and (orientation: portrait)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_1536x2048.png"
  >
  <link
    rel="mask-icon"
    color="#77a2b6"
    href="https://assets.habr.com/habr-web/img/favicons/apple-touch-icon-120.svg"
  >
  <link
    crossorigin="use-credentials"
    href="/manifest.webmanifest"
    rel="manifest"
  >
  <script async src="https://unpkg.com/pwacompat" crossorigin="anonymous"></script>
  <script>window.yaContextCb = window.yaContextCb || []</script>
  <script src="https://yandex.ru/ads/system/context.js" async></script>
</head>
<body>


<div id="app" data-server-rendered="true" data-async-called="true"><div class="tm-layout__wrapper"><!----> <div></div> <!----> <header class="tm-header"><div class="tm-page-width"><div class="tm-header__container"><!----> <span class="tm-header__logo-wrap"><a href="/ru/" class="tm-header__logo tm-header__logo tm-header__logo_hl-ru"><svg height="16" width="16" class="tm-svg-img tm-header__icon"><title>Хабр</title> <use xlink:href="/img/habr-logo-ru.svg#logo"></use></svg></a> <span class="tm-header__beta-sign" style="display:none;">β</span></span> <div class="tm-dropdown tm-header__projects"><div class="tm-dropdown__head"><button class="tm-header__dropdown-toggle"><svg height="16" width="16" class="tm-svg-img tm-header__icon tm-header__icon_dropdown"><title>Открыть список</title> <use xlink:href="/img/megazord-v28.faec9762..svg#arrow-down"></use></svg></button></div> <!----></div> <a href="/ru/sandbox/start/" class="tm-header__become-author-btn">
            Как стать автором
          </a> <div class="tm-feature tm-header__feature tm-feature tm-feature_variant-inline"><!----></div> <!----> <!----></div></div></header> <div class="tm-layout"><div class="tm-page-progress-bar"></div> <div data-menu-sticky="true" class="tm-base-layout__header tm-base-layout__header_is-sticky"><div class="tm-page-width"><div class="tm-base-layout__header-wrapper"><div class="tm-main-menu"><div class="tm-main-menu__section"><nav class="tm-main-menu__section-content"><a href="/ru/feed/" class="tm-main-menu__item">
        Моя лента
      </a> <a href="/ru/articles/" class="tm-main-menu__item tm-main-menu__item_active">
        Все потоки
      </a> <a href="/ru/flows/develop/" class="tm-main-menu__item">
          Разработка
        </a><a href="/ru/flows/admin/" class="tm-main-menu__item">
          Администрирование
        </a><a href="/ru/flows/design/" class="tm-main-menu__item">
          Дизайн
        </a><a href="/ru/flows/management/" class="tm-main-menu__item">
          Менеджмент
        </a><a href="/ru/flows/marketing/" class="tm-main-menu__item">
          Маркетинг
        </a><a href="/ru/flows/popsci/" class="tm-main-menu__item">
          Научпоп
        </a></nav></div></div> <div class="tm-header-user-menu tm-base-layout__user-menu"><a href="/ru/search/" class="tm-header-user-menu__item tm-header-user-menu__search"><svg height="24" width="24" class="tm-svg-img tm-header-user-menu__icon tm-header-user-menu__icon_search tm-header-user-menu__icon_dark"><title>Поиск</title> <use xlink:href="/img/megazord-v28.faec9762..svg#search"></use></svg></a> <!----> <!----> <!----> <div class="tm-header-user-menu__item"><button class="tm-header-user-menu__toggle"><svg height="24" width="24" class="tm-svg-img tm-header-user-menu__icon tm-header-user-menu__icon_dark"><title>Настройки</title> <use xlink:href="/img/megazord-v28.faec9762..svg#page-settings"></use></svg></button></div> <a href="https://habr.com/kek/v1/auth/habrahabr/?back=/ru/articles/704592/&amp;hl=ru" rel="nofollow" class="tm-header-user-menu__item"><button type="button" class="tm-header-user-menu__login btn btn_solid btn_small">
        Войти
      </button></a> <!----> <DIV class="v-portal" style="display:none;"></DIV></div></div></div></div> <!----> <div class="tm-page-width"></div> <main class="tm-layout__container"><div hl="ru" data-async-called="true" class="tm-page"><div class="tm-page-width"><!----> <div class="tm-page__wrapper"><div class="tm-page__main tm-page__main_has-sidebar"><div class="pull-down"><!----> <div class="pull-down__header" style="height:0px;"><div class="pull-down__content" style="bottom:10px;"><svg height="24" width="24" class="tm-svg-img pull-down__icon pull-down__arrow"><title>Обновить</title> <use xlink:href="/img/megazord-v28.faec9762..svg#pull-arrow"></use></svg></div></div> <div class="tm-adfox-banner__container"><!----> <div id="adfox_169815559787254866" class="tm-adfox-banner tm-adfox-banner tm-adfox-banner_variant-narrow"></div></div> <div class="tm-article-presenter"> <div class="tm-article-presenter__body"><div class="tm-misprint-area"><div class="tm-misprint-area__wrapper"><article class="tm-article-presenter__content tm-article-presenter__content_narrow"><div class="tm-article-presenter__header"> <div class="tm-article-snippet tm-article-presenter__snippet tm-article-snippet"> <div class="tm-article-snippet__meta-container"><div class="tm-article-snippet__meta"><span class="tm-user-info tm-article-snippet__author"><a href="/ru/users/slivka_83/" title="slivka_83" class="tm-user-info__userpic"><div class="tm-entity-image"><img alt height="24" src="//habrastorage.org/r/w48/getpro/habr/avatars/9c2/473/9b3/9c24739b37b7521529b59008baf7bf29.png" width="24" class="tm-entity-image__pic"></div></a> <span class="tm-user-info__user tm-user-info__user_appearance-default"><a href="/ru/users/slivka_83/" class="tm-user-info__username">
      slivka_83
      <!----></a> <span class="tm-article-datetime-published"><time datetime="2022-12-09T16:23:20.000Z" title="2022-12-09, 19:23">9  дек  2022 в 19:23</time></span></span></span></div> <!----></div> <h1 lang="ru" class="tm-title tm-title_h1"><span>Введение в библиотеку Transformers и платформу Hugging Face</span></h1> <div class="tm-article-snippet__stats"><!----> <div class="tm-article-reading-time"><span class="tm-svg-icon__wrapper tm-article-reading-time__icon"><svg height="24" width="24" class="tm-svg-img tm-svg-icon"><title>Время на прочтение</title> <use xlink:href="/img/megazord-v28.faec9762..svg#clock"></use></svg></span> <span class="tm-article-reading-time__label">
    17 мин
  </span></div> <span class="tm-icon-counter tm-data-icons__item"><svg height="24" width="24" class="tm-svg-img tm-icon-counter__icon"><title>Количество просмотров</title> <use xlink:href="/img/megazord-v28.faec9762..svg#counter-views"></use></svg> <span class="tm-icon-counter__value">43K</span></span></div> <div class="tm-publication-hubs__container"><div class="tm-publication-hubs"><span class="tm-publication-hub__link-container"><a href="/ru/hubs/python/" class="tm-publication-hub__link"><span>Python</span> <span title="Профильный хаб" class="tm-article-snippet__profiled-hub">*</span></a></span><span class="tm-publication-hub__link-container"><a href="/ru/hubs/data_mining/" class="tm-publication-hub__link"><span>Data Mining</span> <span title="Профильный хаб" class="tm-article-snippet__profiled-hub">*</span></a></span><span class="tm-publication-hub__link-container"><a href="/ru/hubs/bigdata/" class="tm-publication-hub__link"><span>Big Data</span> <span title="Профильный хаб" class="tm-article-snippet__profiled-hub">*</span></a></span><span class="tm-publication-hub__link-container"><a href="/ru/hubs/machine_learning/" class="tm-publication-hub__link"><span>Машинное обучение</span> <span title="Профильный хаб" class="tm-article-snippet__profiled-hub">*</span></a></span><span class="tm-publication-hub__link-container"><a href="/ru/hubs/artificial_intelligence/" class="tm-publication-hub__link"><span>Искусственный интеллект</span> <!----></a></span></div></div> <div class="tm-article-labels"><div class="tm-article-labels__container"><div class="tm-publication-label tm-publication-label_variant-tutorial"><span>
    Туториал
  </span></div> <div class="tm-publication-label tm-publication-label_variant-technotext2022"><a href="/ru/technotext/2022/">
    Технотекст 2022
  </a></div></div></div> <!----> <!----></div></div> <!----> <div data-gallery-root="" lang="ru" class="tm-article-body"><div></div> <div id="post-content-body"><div><div class="article-formatted-body article-formatted-body article-formatted-body_version-2"><div xmlns="http://www.w3.org/1999/xhtml"><p>Исходники: <a href="https://github.com/huggingface/transformers" rel="noopener noreferrer nofollow"><u>https://github.com/huggingface/transformers</u></a><u><br/></u>Документация: <a href="https://huggingface.co/docs/transformers/main/en/index" rel="noopener noreferrer nofollow"><u>https://huggingface.co/docs/transformers/main/en/index</u></a></p><p>Платформа Hugging Face это коллекция готовых современных предварительно обученных Deep Learning моделей. А библиотека Transformers предоставляет инструменты и интерфейсы для их простой загрузки и использования. Это позволяет вам экономить время и ресурсы, необходимые для обучения моделей с нуля.</p><p>Модели решают весьма разнообразный спектр задач:</p><ul><li><p>NLP: classification, NER, question answering, language modeling, summarization, translation, multiple choice, text generation.</p></li><li><p>CV: classification, object detection,segmentation.</p></li><li><p>Audio: classification, automatic speech recognition.</p></li><li><p>Multimodal: table question answering, optical character recognition, information extraction from scanned documents, video classification, visual question answering.</p></li><li><p>Reinforcement Learning</p></li><li><p>Time Series</p></li></ul><p>Одна и та же задача может решаться различными архитектурами и их список впечатляет - более 150 на текущий момент. Из наиболее известных: <a href="https://huggingface.co/docs/transformers/main/en/model_doc/vit" rel="noopener noreferrer nofollow"><u>Vision Transformer (ViT)</u></a>, <a href="https://huggingface.co/docs/transformers/main/en/model_doc/t5" rel="noopener noreferrer nofollow"><u>T5</u></a>, <a href="https://huggingface.co/docs/transformers/main/en/model_doc/resnet" rel="noopener noreferrer nofollow"><u>ResNet</u></a>, <a href="https://huggingface.co/docs/transformers/main/en/model_doc/bert" rel="noopener noreferrer nofollow"><u>BERT</u></a>, <a href="https://huggingface.co/docs/transformers/main/en/model_doc/gpt2" rel="noopener noreferrer nofollow"><u>GPT2</u></a>. На этих архитектурах обучены более 60 000 моделей.</p><p>Модели Transformers поддерживают три фреймворках: PyTorch, TensorFlow и JAX. Для На PyTorch’а доступны почти все архитектуры. А вот для остальных надо смотреть совместимость: <a href="https://huggingface.co/docs/transformers/main/en/index#supported-frameworks" rel="noopener noreferrer nofollow"><u>https://huggingface.co/docs/transformers/main/en/index#supported-frameworks</u></a><u><br/></u>Также модели можно экспортировать в форматы ONNX и TorchScript.</p><p>Transformers не является набором модулей, из которых составляется нейронная сеть, как например PyTorch. Вместо это Transformers предоставляет несколько высокоуровневых абстракций, которые позволяют работать с моделями в несколько строк кода.</p><p>Начнем с установки…</p><h2>Установка</h2><p>Нам понадобится сами трансформеры:</p><pre><code class="bash">pip install transformers</code></pre><p>И т.к. мы будем писать на торче, то:</p><pre><code class="bash">pip install torch</code></pre><p>Еще нам понадобится библиотека evaluate, которая предоставляет различные ML метрики:</p><pre><code class="bash">pip install evaluate</code></pre><h2>Поиск моделей</h2><p>Прежде чем приступать к коду нам нужно формализовать нашу задачу до одного из общепринятых классов и найти подходящую для нее модель на хабе Hugging Face: <a href="https://huggingface.co/models" rel="noopener noreferrer nofollow"><u>https://huggingface.co/models</u></a></p><figure class="full-width "><img src="https://habrastorage.org/r/w1560/getpro/habr/upload_files/1fa/262/9d5/1fa2629d5c8a96de953971af60a11162.png" width="1536" height="816" data-src="https://habrastorage.org/getpro/habr/upload_files/1fa/262/9d5/1fa2629d5c8a96de953971af60a11162.png"/><figcaption></figcaption></figure><p>Слева вы можете увидеть ряд фильтров:</p><ul><li><p>Класс задачи</p></li><li><p>Поддерживаемый фреймворк глубокого обучения</p></li><li><p>На каком датасете происходило обучение</p></li><li><p>Язык, на котором училась модель (если это NLP задача).</p></li></ul><p>Также вы сможете поискать модель по названию — часто название модели содержит ее предназначение или архитектуру. Например, NLP-модели для классификации токсичности текста могут содержать “toxic” в названии. А берто-подобные архитектуры содержат слово “bert”.</p><p>На ваш поиск может вывалится множество моделей, поскольку для одной и той же задачи имеется множество предобученных моделей на разных архитектурах. И вам нужно выбрать подходящую. Что значит “подходящую”? Тут на вкус и цвет фломастеры разные: кому-то важнее точность, кому-то универсальность, а кому-то размер модели — выбирайте :)</p><p>Провалившись в конкретную модель вы сможете найти:</p><ul><li><p>Более подробное описание модели</p></li><li><p>Кол-во классов, которые предсказывает модель</p></li><li><p>Примеры кода</p></li><li><p>Бенчмарки</p></li><li><p>И возможность поэкспериментировать</p></li></ul><blockquote><p>Более подробно про различные классы задач и как они решаются можете почитать здесь: <a href="https://huggingface.co/tasks" rel="noopener noreferrer nofollow"><u>https://huggingface.co/tasks</u></a></p></blockquote><figure class="full-width "><img src="https://habrastorage.org/r/w1560/getpro/habr/upload_files/1e7/159/83a/1e715983a83f6bed950edcd450cc5e5e.png" width="1536" height="815" data-src="https://habrastorage.org/getpro/habr/upload_files/1e7/159/83a/1e715983a83f6bed950edcd450cc5e5e.png"/><figcaption></figcaption></figure><p>После того как вы нашли нужную модель скопируйте ее полное название, далее оно нам понадобится…</p><h2>Использование моделей</h2><p>Для доступа к моделям есть два способа:</p><ul><li><p>Прямое использование моделей на исходном фреймворке — больше кода, но и больше гибкости.</p></li><li><p>Класс Pipeline — самый простой способ воспользоваться моделями из transformers. С него и начнем.</p></li></ul><h3>Pipeline</h3><p>Использование любой модели включает в себя минимум два шага: соответствующим образом подготовить данные и непосредственное использование модели. Класс Pipeline объединяет в себе эти два шага.</p><p>Посмотрим как его задействовать на примере задачи классификации (токсичности) текста:</p><pre><code class="python">from transformers import pipeline

clf = pipeline(
    task = 'sentiment-analysis', 
    model = 'SkolkovoInstitute/russian_toxicity_classifier')

text = ['У нас в есть убунты и текникал превью.',
    	'Как минимум два малолетних дегенерата в треде, мда.']

clf(text)

#вывод
[{'label': 'neutral', 'score': 0.9872767329216003},
 {'label': 'toxic', 'score': 0.985331654548645}]</code></pre><p>Здесь мы:</p><ul><li><p>В конструкторе pipeline указали задачу, которую хотим решить, а также название конкретной модели из хаба Hugging Face (<a href="https://huggingface.co/models" rel="noopener noreferrer nofollow"><u>https://huggingface.co/models</u></a>).</p></li><li><p>Задали набор документов, в которых нужно найти токсичный текст.</p></li><li><p>На выходе модель для каждого примера вывела наиболее вероятный класс и его скор.</p></li></ul><blockquote><p>С полным списком наименований задач, которые поддерживаются Pipeline вы можете ознакомится здесь: <a href="https://huggingface.co/docs/transformers/main/en/main_classes/pipelines" rel="noopener noreferrer nofollow"><u>https://huggingface.co/docs/transformers/main/en/main_classes/pipelines</u></a></p></blockquote><blockquote><p>При первом обращении к какой-либо модели произойдет ее загрузка. При повторном обращении к этой модели загрузка будет производится из кэша.</p></blockquote><p>Что тут можно улучшить:</p><ul><li><p>Помимо конкретной модели в pipeline можно передать tokenizer. Токенайзер используется в NLP задачах и отвечает за предварительную обработку текста и конвертирует их в массив чисел, которые затем поступают на вход модели (об этом подробнее ниже). <br/><br/>Обычно для модели используется точно такой же tokenizer, который использовался при обучении (только так можно гарантировать корректность ее работы). Но если по каким-либо причинам вам потребовался другой, то его можно задать примерно так:</p></li></ul><pre><code class="python">pipeline(
    task = 'question-answering', 
    model = 'distilbert-base-cased-distilled-squad', 
    tokenizer = 'bert-base-cased')</code></pre><ul><li><p>По умолчанию классификатор возвращает наиболее вероятный класс, но вы можете вернуть и все значения:</p></li></ul><pre><code class="python">clf(text, top_k=None)

#вывод
[[{'label': 'neutral', 'score': 0.9872767329216003},
  {'label': 'toxic', 'score': 0.012723307125270367}],
 [{'label': 'neutral', 'score': 0.01466838177293539},
  {'label': 'toxic', 'score': 0.985331654548645}]]</code></pre><ul><li><p>Если все данные, которые нужно обработать, не влазят в память, то можно задействовать генератор, который будет поштучно загружать данные в память и подавать их в модель:</p></li></ul><pre><code class="python">from transformers import pipeline

clf = pipeline(
    task = 'sentiment-analysis', 
    model = 'SkolkovoInstitute/russian_toxicity_classifier')

text = ['У нас в есть убунты и текникал превью.',
        'Как минимум два малолетних дегенерата в треде, мда.']

def data(text):
    for row in text:
        yield row

for out in clf(data(text)):
    print(out)

#вывод
{'label': 'neutral', 'score': 0.9872767329216003}
{'label': 'toxic', 'score': 0.985331654548645}</code></pre><h3>PyTorch</h3><p>А теперь посмотрим как использовать модель на нативном Торче. Будем классифицировать котиков :)</p><pre><code class="python">import torch
import requests
from PIL import Image
from io import BytesIO
from transformers import AutoImageProcessor, AutoModelForImageClassification

response = requests.get(
    'https://github.com/laxmimerit/dog-cat-full-dataset/blob/master/data/train/cats/cat.10055.jpg?raw=true')
img = Image.open(BytesIO(response.content))

img_proc = AutoImageProcessor.from_pretrained(
    'google/vit-base-patch16-224')
model = AutoModelForImageClassification.from_pretrained(
    'google/vit-base-patch16-224')

inputs = img_proc(img, return_tensors='pt')

with torch.no_grad():
    logits = model(**inputs).logits

predicted_id = logits.argmax(-1).item()
predicted_label = model.config.id2label[predicted_id]
print(predicted_id, '-', predicted_label)

#вывод
281 - tabby, tabby cat</code></pre><p>Тут мы:</p><ul><li><p>Импортируем два AutoClass’а: AutoImageProcessor и AutoModelForImageClassification. <br/>AutoClass (начинается с Auto) это специальный класс, который автоматически извлекает архитектуру предварительно обученной модели по ее имени или пути.</p></li><li><p>Загружаем картинку по URL.</p></li><li><p>Загружаем ImageProcessor. Это аналог токенайзера, но только для картинок — выравнивает размеры картинок, нормализует и т.д. (об ImageProcessor чуть подробнее ниже). В предыдущем варианте предобработкой занимался сам Pipeline где-то в своих недрах. Сейчас же нам придется заниматься этим самостоятельно.</p></li><li><p>Загружаем модель. Сама модель представляет собой PyTorch nn.Module, который вы можете использовать как обычно при работе с торчом.</p></li><li><p>Обрабатываем картинку посредством ImageProcessor. ImageProcessor возвращает словарь, который подаем на вход модели с оператором распаковки (**).</p></li><li><p>Все модели Transformers возвращают логиты, которые идут перед последней функцией активации (например, softmax). Соответственно, нам самим необходимо их обработать, чтобы получить на выходе вероятность или класс.</p></li></ul><h4>Автоматическое определение архитектуры</h4><p>Для каждой архитектуры и каждой задачи под нее есть свой специальный именной класс. Например: BertForSequenceClassification, GPT2ForSequenceClassification, RobertaForSequenceClassification и т.д. Также и для их предобработчиков: BertTokenizer, .GPT2Tokenizer и т.д.</p><p>Чтобы каждый раз не заморачиваться с определением точного названия класса в Transformers завезли так называемый AutoClass. AutoClass позволяет автоматически считывать всю метаинформацию (архитектуру и пр.) из предварительно обученной модели при ее загрузке:</p><pre><code class="python">img_proc = AutoImageProcessor.from_pretrained(
    'google/vit-base-patch16-224')
model = AutoModelForImageClassification.from_pretrained(
    'google/vit-base-patch16-224')

tokenizer = AutoTokenizer.from_pretrained(
    'SkolkovoInstitute/russian_toxicity_classifier')
model = AutoModelForSequenceClassification.from_pretrained(
    'SkolkovoInstitute/russian_toxicity_classifier')</code></pre><p>Каждый автокласс привязан к определенной задаче. С полным списком автоклассов можете ознакомится здесь: <a href="https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoModel" rel="noopener noreferrer nofollow"><u>https://huggingface.co/docs/transformers/main/en/model_doc/auto</u></a></p><blockquote><p>Если вы обучаете модель с нуля, то вам нужно импортировать точный конечный класс.</p></blockquote><h2>Дообучение</h2><p>Не часто вам потребуются модели как есть. В соревнованиях, а тем более в работе вам скорее всего придется дообучить модель на своем датасете. И тут вас есть несколько вариантов…</p><h3>Trainer</h3><p>Самый простой способ - воспользоваться классом Trainer. Это аналог Pipline’а. Только он предназначен для организации упрощенного процесса обучения.</p><pre><code class="python">import datasets
import evaluate
import pandas as pd
import numpy as np
from datasets import Dataset
from sklearn.model_selection import train_test_split
from transformers import (AutoTokenizer, AutoModelForSequenceClassification, 
                          TrainingArguments, Trainer)

# Загружаем данные
df = pd.read_csv('toxic.csv')
df.columns = ['text','label']
df['label'] = df['label'].astype(int)

# Конвертируем датафрейм в Dataset
train, test = train_test_split(df, test_size=0.3)
train = Dataset.from_pandas(train)
test = Dataset.from_pandas(test)

# Выполняем предобработку текста
tokenizer = AutoTokenizer.from_pretrained(
    'SkolkovoInstitute/russian_toxicity_classifier')

def tokenize_function(examples):
	return tokenizer(examples['text'], padding='max_length', truncation=True)

tokenized_train = train.map(tokenize_function)
tokenized_test = test.map(tokenize_function)

# Загружаем предобученную модель
model = AutoModelForSequenceClassification.from_pretrained(
	'SkolkovoInstitute/russian_toxicity_classifier',
	num_labels=2)

# Задаем параметры обучения
training_args = TrainingArguments(
	output_dir = 'test_trainer_log',
	evaluation_strategy = 'epoch',
	per_device_train_batch_size = 6,
	per_device_eval_batch_size = 6,
	num_train_epochs = 5,
	report_to='none')

# Определяем как считать метрику
metric = evaluate.load('f1')
def compute_metrics(eval_pred):
	logits, labels = eval_pred
	predictions = np.argmax(logits, axis=-1)
	return metric.compute(predictions=predictions, references=labels)

# Выполняем обучение
trainer = Trainer(
	model = model,
	args = training_args,
	train_dataset = tokenized_train,
	eval_dataset = tokenized_test,
	compute_metrics = compute_metrics)

trainer.train()

# Сохраняем модель
save_directory = './pt_save_pretrained'
#tokenizer.save_pretrained(save_directory)
model.save_pretrained(save_directory)
#alternatively save the trainer
#trainer.save_model('CustomModels/CustomHamSpam')</code></pre><figure class="full-width "><img src="https://habrastorage.org/r/w1560/getpro/habr/upload_files/090/161/072/090161072e1b347bed5e63e490f0834f.png" width="894" height="336" data-src="https://habrastorage.org/getpro/habr/upload_files/090/161/072/090161072e1b347bed5e63e490f0834f.png"/><figcaption></figcaption></figure><p>Что мы тут делаем:</p><ul><li><p>Загружаем данные в пандас.</p></li><li><p>Переводим датафрейм в класс Dataset, которые можно подавать на вход модели при обучении.</p></li><li><p>Применяем к каждой строке датасета (тексту) токенизатор, чтобы перевести его в массив чисел.</p></li><li><p>Подгружаем предварительно обученную модель.</p></li><li><p>Определяем экземпляр класса TrainingArguments. В нем задаются гиперпараметры, которые будут использоваться при обучении модели, а также специальные флаги, которые активируют различные варианты обучения.<br/>Т.к. класс универсальный и предназначен для обучения разных архитектур и задач, то параметров у него довольно много. Подробнее ознакомится с ними можете здесь: <a href="https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments" rel="noopener noreferrer nofollow"><u>https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments</u></a></p><p>Сейчас зададим только: куда сохранять промежуточные результаты во время обучения, стратегию обучения (в данном случае — по эпохам), кол-во эпох и размер батча.</p></li><li><p>Посредством модуля Evaluate определяем интересующую нас метрику и задаем функцию, которая будет выполнять расчет в процессе обучения.<br/><br/>З.Ы. Не забываем что все модели возвращают логиты, которые необходимо соответствующим образом преобразовать.<br/><br/>Использовать библиотеку Evaluate не обязательно. Можно хоть из скалерна брать метрики. Главно определить функцию, которая будет производить расчеты.<br/><br/>Список доступных метрик: <a href="https://huggingface.co/evaluate-metric" rel="noopener noreferrer nofollow"><u>https://huggingface.co/evaluate-metric</u></a><u><br/></u>тут же вы можете с ними поэкспериментировать.</p></li><li><p>Создаем объект Trainer и подгружаем в него все ранее определенные компоненты: модель, аргументы, датасеты, функцию оценки. И запускаем обучение.</p></li><li><p>После обучения сохраняем результат на диск.</p></li></ul><p>В последующем мы можем загрузить нашу модель так:</p><pre><code class="python">model = AutoModelForSequenceClassification.from_pretrained(
	'./pt_save_pretrained')</code></pre><blockquote><p>Trainer поддерживает поиск гиперпараметров посредством специализированных пакетов: optuna, sigopt, raytune и wandb. Более подрбно: <a href="https://huggingface.co/docs/transformers/hpo_train" rel="noopener noreferrer nofollow"><u>https://huggingface.co/docs/transformers/hpo_train</u></a></p></blockquote><h3>PyTorch</h3><p>Теперь задействуем классический алгоритм обучения на торче:</p><pre><code class="python">import torch
import evaluate
import pandas as pd
from tqdm.auto import tqdm
from datasets import Dataset
from torch.optim import AdamW
from torch.utils.data import DataLoader
from sklearn.model_selection import train_test_split
from transformers import (AutoTokenizer, 
                          AutoModelForSequenceClassification, get_scheduler)

# Загружаем данные
df = pd.read_csv('toxic.csv')
df.columns = ['text','label']
df['label'] = df['label'].astype(int)

# Конвертируем датафрейм в Dataset
train, test = train_test_split(df, test_size=0.2)
train = Dataset.from_pandas(train)
test = Dataset.from_pandas(test)

# Выполняем предобработку текста
tokenizer = AutoTokenizer.from_pretrained(
	'SkolkovoInstitute/russian_toxicity_classifier')

def tokenize_function(examples):
	return tokenizer(examples['text'], padding='max_length', truncation=True)

def ds_preproc(ds):
	ds = ds.map(tokenize_function)
	ds = ds.remove_columns(['text', 'index_level_0'])
	ds = ds.rename_column('label', 'labels')
	ds.set_format('torch')
	return ds

tokenized_train = ds_preproc(train)
tokenized_test = ds_preproc(test)

# Создаем даталоадер
train_dataloader = DataLoader(tokenized_train, shuffle=True, batch_size=8)
test_dataloader = DataLoader(tokenized_test, batch_size=8)

# Загружаем модель и указываем кол-во классов
model = AutoModelForSequenceClassification.from_pretrained(
	'SkolkovoInstitute/russian_toxicity_classifier',
	num_labels=2)

# Задаем оптимайзер и шедулер
optimizer = AdamW(model.parameters(), lr=5e-6)

num_epochs = 5
num_training_steps = num_epochs * len(train_dataloader)

lr_scheduler = get_scheduler(
	name = 'linear',
	optimizer = optimizer,
	num_warmup_steps = 0,
	num_training_steps = num_training_steps)

device = 'cuda'
model.to(device)

# Выполняем цикл...
for epoch in tqdm(range(num_epochs)):

	#... обучения
	model.train()
	for batch in tqdm(train_dataloader, leave=False):
    	batch = {k: v.to(device) for k, v in batch.items()}
    	outputs = model(**batch)
    	loss = outputs.loss
    	loss.backward()
    	optimizer.step()
    	lr_scheduler.step()
    	optimizer.zero_grad()

	#... оценки
	metric = evaluate.load('f1')

	model.eval()
	for batch in tqdm(test_dataloader, leave=False):
    	batch = {k: v.to(device) for k, v in batch.items()}
    	with torch.no_grad():
        	outputs = model(**batch)
    	logits = outputs.logits
    	predictions = torch.argmax(logits, dim=-1)

    	metric.add_batch(predictions=predictions, references=batch['labels'])

	print(f'epoch {epoch} -', metric.compute())

# Сохраняем модель
save_directory = './pt_save_pretrained'
model.save_pretrained(save_directory)</code></pre><p>Этот пайплайн не сильно отличается от других процессов обучения нейронных сетей:</p><ul><li><p>Загружаем датасет в пандас, разбиваем на трейн/тест.</p></li><li><p>Конвертируем датафрейм в класс Dataset, который принимает на вход модель.</p></li><li><p>Переводим текст в токены, токены в массив чисел. Также очищаем датасет от лишних полей (иначе модель будет ругаться).</p></li><li><p>Формируем DataLoader для тренировочных и тестовых данных, чтобы мы могли </p></li><li><p>Определяем оптимизатор и шедулер.</p></li><li><p>Загружаем модель и указываем кол-во классов, которые должна выучить модель.</p></li><li><p>Задаем оптимизатор и планировщик. Причем оптимизатор родной для торча, а планировщик из библиотеки трансформеров.</p></li><li><p>Итерируемся по эпохам. На каждой выполняем цикл обучения и цикл оценки.</p></li><li><p>Сохраняем модель на диске.</p></li></ul><h3>Эмбединги</h3><p>Третий способ дообучить модель — вытащить из нее эмбеддинги (актуально для NLP задач) и обучить какую-либо классическую модель, используя эти эмбединги как фичи.</p><p>Вытащить эмбединги можно двумя способами.</p><ol><li><p>Первый - ручной:</p></li></ol><pre><code class="python">import torch
import pandas as pd
from transformers import AutoTokenizer, AutoModel

#Mean Pooling - Take attention mask into account for correct averaging
def mean_pooling(model_output, attention_mask):
	token_embeddings = model_output[0] #First element of model_output contains all token embeddings
	input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
	sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)
	sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)
	return sum_embeddings / sum_mask

#Sentences we want sentence embeddings for
sentences = ['This framework generates embeddings for each input sentence',
             'Sentences are passed as a list of string.',
             'The quick brown fox jumps over the lazy dog.']

#Load AutoModel from huggingface model repository
tokenizer = AutoTokenizer.from_pretrained(
	'sentence-transformers/all-MiniLM-L6-v2')
model = AutoModel.from_pretrained(
	'sentence-transformers/all-MiniLM-L6-v2')

#Tokenize sentences
encoded_input = tokenizer(
	sentences, 
	padding=True, 
	truncation=True, 
	max_length=128, 
	return_tensors='pt')

#Compute token embeddings
with torch.no_grad():
	model_output = model(encoded_input)

#Perform pooling. In this case, mean pooling
sentence_embeddings = mean_pooling(
	model_output,
	encoded_input['attention_mask'])

df = pd.DataFrame(sentence_embeddings).astype('float')</code></pre><ol start="2"><li><p>Альтернативно можно задействовать отдельную библиотеку —  SentenceTransformers (ставится через пип):</p></li></ol><pre><code class="python">from sentence_transformers import SentenceTransformer

model = SentenceTransformer('SkolkovoInstitute/russian_toxicity_classifier')

text = ['У нас в есть убунты и текникал превью.',
    	'Как минимум два малолетних дегенерата в треде, мда.']

embeddings = model.encode(text)

df = pd.DataFrame(embeddings)</code></pre><figure class="full-width "><img src="https://habrastorage.org/r/w1560/getpro/habr/upload_files/98a/b09/e32/98ab09e32d56a658ff21ddab03c86b57.png" width="993" height="136" data-src="https://habrastorage.org/getpro/habr/upload_files/98a/b09/e32/98ab09e32d56a658ff21ddab03c86b57.png"/><figcaption></figcaption></figure><p>Как видите, SentenceTransformers может подгружать нужные модели с хаба Hugging Face.</p><h2>Предварительная обработка</h2><p>Чуть поподробнее разберем в чем заключается предварительная обработка текста и картинок…</p><h3>Токенайзер</h3><p>Токенайзер используется в моделях, которые так или иначе работают с текстом. Компьютер не умеет напрямую работать с текстом - только с числами. И тут в дело вступает токенайзер: он преобразует текст в массив чисел, которые затем поступают на вход модели.</p><p>По сути каждый токенайзер состоит из набора правил и глобально эти правила решают две задачи:</p><ul><li><p>Как поделить предложение на токены. В самом простом случае поделить можно на слова, а критерий разделения - пробел. Но в действительности способов гораздо больше и они довольно сложные.</p></li><li><p>Как привести разнородные предложения к одной длине. Очевидно что тексты бывают разной длины. Но все последовательности, подаваемые на вход модели должны иметь одинаковую длину.</p></li></ul><p>Сначала посмотрим как токенайзер разбивает предложение на токены:</p><pre><code class="python">from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(
	'SkolkovoInstitute/russian_toxicity_classifier')

tokenizer.tokenize('У нас в есть убунты и текникал превью.')

#вывод
['У','нас','в','есть','убу','##нты','и','тек','##ника','##л','превью','.']</code></pre><p>Обратите внимание на значки # в токенах. Так токенайзер выделяет подслова в словах. Это сделано, чтобы не запоминать кучу редких слов и уменьшить хранимый словарный запас. Также это позволяет модели обрабатывать слова, которые она никогда раньше не видела.</p><p>Это один из видов токенизации. Всего в трансформерах используются три основных вида токенизации: <a href="https://huggingface.co/docs/transformers/main/en/tokenizer_summary#byte-pair-encoding" rel="noopener noreferrer nofollow"><u>Byte-Pair Encoding (BPE)</u></a>,<a href="https://huggingface.co/docs/transformers/main/en/tokenizer_summary#wordpiece" rel="noopener noreferrer nofollow"> <u>WordPiece</u></a>, <a href="https://huggingface.co/docs/transformers/main/en/tokenizer_summary#sentencepiece" rel="noopener noreferrer nofollow"><u>SentencePiece</u></a>.</p><p>Теперь посмотрим, что подается на вход модели:</p><pre><code class="python">text = 'У нас в есть убунты и текникал превью.'
encoding = tokenizer(text)
print(encoding)

#вывод
'input_ids': [101, 486, 1159, 340, 999, 63692, 10285, 322, 3100, 1352, 343, 85379, 132, 102]
'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</code></pre><p>Токенайзер возвращает словарь, содержащий:</p><ul><li><p>input_ids — массив чисел, каждое из которых соответствует одному токену.</p></li><li><p>attention_mask — указывает модели, какие токены следует учитывать, а какие игнорировать.</p></li><li><p>token_type_ids — используется в специальных моделях, в которые подаются пары последовательностей. Например, вопрос-ответ. Тогда в token_type_ids эти две последовательности будут обозначены разными метками.</p></li></ul><p>Данный словарь подается на вход модели с оператором распаковки (**).</p><p>Теперь посмотрим как токенайзер выравнивает длину предложений. За это отвечают три параметра:</p><ul><li><p>padding — тензоры подаваемые в модель должны иметь одинаковую длину. Если этот параметр = True, то коротки последовательности дополняются служебными токенами до длины самой длинной последовательности.</p></li><li><p>truncation — очень длинные последовательности тоже плохо. Если параметр = True, то все последовательности усекаются до максимальной длины.</p></li><li><p>max_length — указываем до скольки токенов усекать последовательность.</p></li></ul><pre><code class="python">text = ['У нас в есть убунты',
    	'Как минимум два малолетних дегенерата в треде, мда.']

encoding = tokenizer(
	text,
	padding=True,
	truncation=True,
	max_length=512)

print(encoding)

#вывод
{'input_ids': [
[101, 486, 1159, 340, 999, 63692, 10285, 102, 0, 0, 0, 0, 0], 
[101, 1235, 3932, 1617, 53502, 97527, 303, 340, 39685, 128, 48557, 132, 102]], 'token_type_ids': [
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 
'attention_mask': [
[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0], 
[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}</code></pre><p>Обратите внимание, что каждая последовательность представлена одинаковым количеством чисел (выровнены по самому длинному предложению). При этом короткие предложения дополнены нулями. А чтобы модель не обращала на них внимания, соответствующие токены в attention_mask обозначены нулями.</p><p>По каким правилам происходит усечение и дополнение токенов читайте здесь: <a href="https://huggingface.co/docs/transformers/main/en/pad_truncation" rel="noopener noreferrer nofollow"><u>https://huggingface.co/docs/transformers/main/en/pad_truncation</u></a></p><h3>ImageProcessor</h3><p>ImageProcessor отвечает за подготовку данных CV задач. Его работа несколько проще. По сути он переводит все пиксели в числа и при необходимости выравнивает изображения до одинаковой длины/ширины.</p><pre><code class="python">from transformers import AutoImageProcessor
from PIL import Image
from io import BytesIO
import requests

response = requests.get(
	'https://github.com/laxmimerit/dog-cat-full-dataset/blob/master/data/train/cats/cat.10055.jpg?raw=true')
img = Image.open(BytesIO(response.content))

image_processor = AutoImageProcessor.from_pretrained(
	'google/vit-base-patch16-224')

inputs = image_processor(img, return_tensors='pt')

#вывод
{'pixel_values': tensor([[[[ 0.4275,  0.4275,  0.4196,  ...,  0.0902,  0.1216,  0.0667],
          [ 0.4431,  0.4353,  0.4118,  ...,  0.0902,  0.0588,  0.0118],
          [ 0.4431,  0.4353,  0.4039,  ...,  0.1686,  0.1059,  0.0431],
          ...,
          [-0.1373, -0.0745, -0.0431,  ...,  0.2941,  0.2863,  0.2627],
          [-0.1529, -0.1137, -0.0588,  ...,  0.2784,  0.2627,  0.2627],
          [-0.1529, -0.1294, -0.0745,  ...,  0.2706,  0.2392,  0.2392]],
         [[ 0.4275,  0.4431,  0.4588,  ...,  0.0275,  0.0667,  0.0588],
          [ 0.4431,  0.4510,  0.4510,  ...,  0.0275,  0.0039,  0.0039],
          [ 0.4431,  0.4431,  0.4431,  ...,  0.1059,  0.0510,  0.0275],
          ...,

          [-0.2392, -0.1765, -0.1451,  ...,  0.1922,  0.1922,  0.1765],
          [-0.2549, -0.2157, -0.1608,  ...,  0.1765,  0.1765,  0.1922],
          [-0.2549, -0.2314, -0.1765,  ...,  0.1686,  0.1529,  0.1765]],
         [[ 0.4431,  0.4510,  0.4275,  ..., -0.0902, -0.0824, -0.0980],
          [ 0.4588,  0.4588,  0.4275,  ..., -0.0980, -0.1451, -0.1529],
          [ 0.4588,  0.4510,  0.4118,  ..., -0.0196, -0.0980, -0.1294],
          ...,
          [-0.3647, -0.3020, -0.2706,  ...,  0.0667,  0.0667,  0.0510],
          [-0.3804, -0.3490, -0.2941,  ...,  0.0431,  0.0353,  0.0431],
          [-0.3882, -0.3647, -0.3098,  ...,  0.0353,  0.0118,  0.0275]]]])}</code></pre><blockquote><p>Для некоторых моделей ImageProcessor выполняет еще и постобработку. Например, преобразует логиты в маски сегментации.</p></blockquote><h2>Прочее</h2><h3>Шаринг</h3><p>Если вы обучили/дообучили хорошую модель, то можете поделиться ею с сообществом. Подробная инструкция как это сделать: <a href="https://huggingface.co/docs/transformers/model_sharing" rel="noopener noreferrer nofollow"><u>https://huggingface.co/docs/transformers/model_sharing</u></a></p><p>Также вы можете создать и загрузить в хаб Hugging Face свой кастомный Pipeline: <a href="https://huggingface.co/docs/transformers/main/en/add_new_pipeline#how-to-create-a-custom-pipeline" rel="noopener noreferrer nofollow"><u>https://huggingface.co/docs/transformers/main/en/add_new_pipeline#how-to-create-a-custom-pipeline</u></a></p><h3>Датасеты</h3><p>Платформа Hugging Face предоставляет много готовых датасетов для аудио, CV и NLP задач, которые вы можете использовать для своих целей. Найти нужный датасет вы сможете в специальном хабе. По аналогии с моделями воспользуйтесь фильтрами, чтобы отыскать нужный вам датасет:</p><p><a href="https://huggingface.co/datasets" rel="noopener noreferrer nofollow"><u>https://huggingface.co/datasets</u></a></p><p>Загружать примерно так:</p><pre><code class="python">from datasets import load_dataset

dataset = load_dataset('rotten_tomatoes')</code></pre><p>Более подробно изучить функционал датасетов сможете здесь: <a href="https://huggingface.co/docs/datasets/index" rel="noopener noreferrer nofollow"><u>https://huggingface.co/docs/datasets/index</u></a></p><h2>Что дальше?</h2><p>А дальше изучаем и воспроизводим кучу готовых примеров:</p><ul><li><p><a href="https://huggingface.co/docs/transformers/notebooks" rel="noopener noreferrer nofollow"><u>https://huggingface.co/docs/transformers/notebooks</u></a></p></li><li><p><a href="https://huggingface.co/docs/transformers/community#community-notebooks" rel="noopener noreferrer nofollow"><u>https://huggingface.co/docs/transformers/community#community-notebooks</u></a></p></li><li><p><a href="https://github.com/huggingface/transformers/tree/main/examples" rel="noopener noreferrer nofollow"><u>https://github.com/huggingface/transformers/tree/main/examples</u></a></p></li><li><p><a href="https://github.com/NielsRogge/Transformers-Tutorials" rel="noopener noreferrer nofollow"><u>https://github.com/NielsRogge/Transformers-Tutorials</u></a></p></li></ul><p>Отдельно стоит отметить целый курс, посвященный трансформерам: <a href="https://huggingface.co/course/chapter1/1" rel="noopener noreferrer nofollow"><u>https://huggingface.co/course/chapter1/1</u></a></p><figure class=""><img src="https://habrastorage.org/r/w1560/getpro/habr/upload_files/298/70a/e7f/29870ae7f525c03a7622c082bb407ac6.png" width="175" height="175" data-src="https://habrastorage.org/getpro/habr/upload_files/298/70a/e7f/29870ae7f525c03a7622c082bb407ac6.png"/><figcaption></figcaption></figure><p>Код из статьи:<a href="https://github.com/slivka83/article/tree/main/hydra" rel="noopener noreferrer nofollow"> </a><a href="https://github.com/slivka83/article/blob/main/transformers/Transformers.ipynb" rel="noopener noreferrer nofollow"><u>https://github.com/slivka83/article/blob/main/transformers/Transformers.ipynb</u></a></p><p><a href="https://t.me/ds_private_sharing" rel="noopener noreferrer nofollow"><u>Мой телеграм-канал</u></a></p><p></p></div></div></div> <!----> <!----></div> <!----> <!----></div> <!----> <div class="tm-article-presenter__meta"><div class="tm-article-presenter__meta-list tm-separated-list"><span class="tm-separated-list__title">Теги:</span> <ul class="tm-separated-list__list"><li class="tm-separated-list__item"><a href="/ru/search/?target_type=posts&amp;order=relevance&amp;q=%5Btransformers%5D" class="tm-tags-list__link">transformers</a></li><li class="tm-separated-list__item"><a href="/ru/search/?target_type=posts&amp;order=relevance&amp;q=%5Bbert%5D" class="tm-tags-list__link">bert</a></li><li class="tm-separated-list__item"><a href="/ru/search/?target_type=posts&amp;order=relevance&amp;q=%5Bgpt%5D" class="tm-tags-list__link">gpt</a></li><li class="tm-separated-list__item"><a href="/ru/search/?target_type=posts&amp;order=relevance&amp;q=%5Bmachine%20leraning%5D" class="tm-tags-list__link">machine leraning</a></li><li class="tm-separated-list__item"><a href="/ru/search/?target_type=posts&amp;order=relevance&amp;q=%5Bdata%20science%5D" class="tm-tags-list__link">data science</a></li><li class="tm-separated-list__item"><a href="/ru/search/?target_type=posts&amp;order=relevance&amp;q=%5Bdeep%20learning%5D" class="tm-tags-list__link">deep learning</a></li> <!----></ul></div> <div class="tm-article-presenter__meta-list tm-separated-list"><span class="tm-separated-list__title">Хабы:</span> <ul class="tm-separated-list__list"><li class="tm-separated-list__item"><a href="/ru/hubs/python/" class="tm-hubs-list__link">Python</a></li><li class="tm-separated-list__item"><a href="/ru/hubs/data_mining/" class="tm-hubs-list__link">Data Mining</a></li><li class="tm-separated-list__item"><a href="/ru/hubs/bigdata/" class="tm-hubs-list__link">Big Data</a></li><li class="tm-separated-list__item"><a href="/ru/hubs/machine_learning/" class="tm-hubs-list__link">Машинное обучение</a></li><li class="tm-separated-list__item"><a href="/ru/hubs/artificial_intelligence/" class="tm-hubs-list__link">Искусственный интеллект</a></li> <!----></ul></div></div> <!----></article></div> <!----></div> <div class="tm-article-sticky-panel"><div class="tm-data-icons tm-article-sticky-panel__icons"><div class="tm-article-rating tm-data-icons__item"><div title="Всего голосов 23: ↑23 и ↓0" class="tm-votes-lever tm-article-rating__votes-switcher tm-votes-lever tm-votes-lever_appearance-article"><button title="Нравится" type="button" class="tm-votes-lever__button"><svg height="24" width="24" class="tm-svg-img tm-votes-lever__icon"><title>Голосование</title> <use xlink:href="/img/megazord-v28.faec9762..svg#counter-vote"></use></svg></button> <div class="tm-votes-lever__score tm-votes-lever__score tm-votes-lever__score_appearance-article"><span><span class="tm-votes-lever__score-counter tm-votes-lever__score-counter tm-votes-lever__score-counter_positive">
          +23
        </span></span></div> <button title="Не нравится" type="button" class="tm-votes-lever__button"><svg height="24" width="24" class="tm-svg-img tm-votes-lever__icon tm-votes-lever__icon_arrow-down"><title>Голосование</title> <use xlink:href="/img/megazord-v28.faec9762..svg#counter-vote"></use></svg></button></div> <DIV class="v-portal" style="display:none;"></DIV> <!----></div> <!----> <!----> <button title="Добавить в закладки" type="button" class="bookmarks-button tm-data-icons__item"><span class="tm-svg-icon__wrapper bookmarks-button__icon"><svg height="24" width="24" class="tm-svg-img tm-svg-icon"><title>Добавить в закладки</title> <use xlink:href="/img/megazord-v28.faec9762..svg#counter-favorite"></use></svg></span> <span title="Количество пользователей, добавивших публикацию в закладки" class="bookmarks-button__counter">
    156
  </span></button> <div title="Поделиться" class="tm-sharing tm-data-icons__item"><button type="button" class="tm-sharing__button"><svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg" class="tm-sharing__icon"><path fill="currentColor" d="M13.8 13.8V18l7.2-6.6L13.8 5v3.9C5 8.9 3 18.6 3 18.6c2.5-4.4 6-4.8 10.8-4.8z"></path></svg></button> <DIV class="v-portal" style="display:none;"></DIV></div> <div title="Читать комментарии" class="tm-article-comments-counter-link tm-data-icons__item"><a href="/ru/articles/704592/comments/" class="tm-article-comments-counter-link__link"><svg height="24" width="24" class="tm-svg-img tm-article-comments-counter-link__icon"><title>Комментарии</title> <use xlink:href="/img/megazord-v28.faec9762..svg#counter-comments"></use></svg> <span class="tm-article-comments-counter-link__value">
      12
    </span></a> <!----></div> <!----> <DIV class="v-portal" style="display:none;"></DIV></div></div></div>  <div class="tm-article-presenter__footer"><div class="tm-article-blocks"><!----> <section class="tm-block tm-block tm-block_spacing-bottom"><!----> <div class="tm-block__body tm-block__body tm-block__body_variant-balanced"><div class="tm-article-author"> <div class="tm-user-card tm-article-author__user-card tm-user-card tm-user-card_variant-article"><div class="tm-user-card__info-container"><div class="tm-user-card__header"><div class="tm-user-card__header-data"><a href="/ru/users/slivka_83/" class="tm-user-card__userpic tm-user-card__userpic_size-40"><div class="tm-entity-image"><img alt src="//habrastorage.org/getpro/habr/avatars/9c2/473/9b3/9c24739b37b7521529b59008baf7bf29.png" class="tm-entity-image__pic"></div></a> <div class="tm-user-card__meta"><div title=" 25 голосов " class="tm-counter-container tm-karma tm-karma"><div class="tm-counter-container__header"><div class="tm-karma__votes tm-karma__votes_positive">
      25
    </div></div> <div class="tm-counter-container__footer"><div class="tm-karma__text">
      Карма
    </div> <DIV class="v-portal" style="display:none;"></DIV></div></div> <div title="Рейтинг пользователя" class="tm-counter-container"><div class="tm-counter-container__header"> <div class="tm-votes-lever tm-votes-lever tm-votes-lever_appearance-rating"><!----> <div class="tm-votes-lever__score tm-votes-lever__score tm-votes-lever__score_appearance-rating"><span><span class="tm-votes-lever__score-counter tm-votes-lever__score-counter tm-votes-lever__score-counter_rating">
          -1
        </span></span></div> <!----></div></div> <div class="tm-counter-container__footer"><span class="tm-rating__text tm-rating__text">
      Рейтинг
    </span></div></div></div></div></div> <div class="tm-user-card__info tm-user-card__info tm-user-card__info_variant-article"><div class="tm-user-card__title tm-user-card__title tm-user-card__title_variant-article"><span class="tm-user-card__name tm-user-card__name tm-user-card__name_variant-article">Вячеслав</span> <a href="/ru/users/slivka_83/" class="tm-user-card__nickname tm-user-card__nickname tm-user-card__nickname_variant-article">
          @slivka_83
        </a> <!----></div> <p class="tm-user-card__short-info tm-user-card__short-info tm-user-card__short-info_variant-article">MLE</p></div></div> <div class="tm-user-card__buttons tm-user-card__buttons tm-user-card__buttons_variant-article"><!----> <!----> <!----> <!----></div></div> <div class="tm-article-author__user-contacts"><a href="https://telegram.me/slivka_83" rel="noopener" target="_blank" class="tm-article-author__contact">
      Telegram
    </a></div></div></div> <!----></section> <div class="tm-adfox-banner__container tm-page-article__banner"><!----> <div id="adfox_164725660339535756" class="tm-adfox-banner tm-adfox-banner tm-adfox-banner_variant-leaderboard"></div></div> <div class="tm-article-blocks__comments"><div id="publication-comments" class="tm-article-page-comments"><div class="tm-article-comments-counter-link tm-article-comments-counter-button"><a href="/ru/articles/704592/comments/" class="tm-article-comments-counter-link__link tm-article-comments-counter-link__link_button-style"><svg height="24" width="24" class="tm-svg-img tm-article-comments-counter-link__icon tm-article-comments-counter-link__icon_contrasted"><title>Комментарии</title> <use xlink:href="/img/megazord-v28.faec9762..svg#counter-comments"></use></svg> <span class="tm-article-comments-counter-link__value tm-article-comments-counter-link__value_contrasted">
       Комментарии 12 
    </span></a> <!----></div></div></div>  <section class="tm-block tm-block tm-block_spacing-bottom"><header class="tm-block__header tm-block__header tm-block__header_variant-borderless"><div class="tm-block__header-container"><h2 class="tm-block__title tm-block__title tm-block__title_variant-large">Публикации</h2> </div> <!----></header> <div class="tm-block__body tm-block__body tm-block__body_variant-condensed-slim"><div class="tm-tabs tm-tabs"><div><span class="tm-tabs__tab-item"><button class="tm-tabs__tab-link tm-tabs__tab-link tm-tabs__tab-link_active tm-tabs__tab-link_slim">
        Лучшие за сутки
      </button></span><span class="tm-tabs__tab-item"><button class="tm-tabs__tab-link tm-tabs__tab-link tm-tabs__tab-link_slim">
        Похожие
      </button></span></div> <!----></div> <div class="similar-and-daily__tab-view"><div class="placeholder-wrapper"><!----> <!----> <!----> <!----> <!----> <!----> <!----> <!----> <!----> <!----> <!----> <!----> <!----> <!----> <!----> <!----> <!----> <!----> <!----> <!----> <!----> <!----> <!----> <!----> <div class="tm-placeholder-article-cards"><div class="tm-placeholder-article-card"><div class="tm-placeholder__user"><div class="tm-placeholder__user-pic loads"></div> <div class="tm-placeholder__user-date loads"></div></div> <div class="tm-placeholder-article-card__title"><div class="tm-placeholder__line tm-placeholder-article-card__title-line loads"></div> <div class="tm-placeholder__line tm-placeholder-article-card__title-line loads"></div></div> <div class="tm-placeholder-article-card__icons tm-placeholder__counters"><div class="tm-placeholder-data-icon"><div class="tm-placeholder__icon tm-placeholder__icon_large loads"></div> <div class="tm-placeholder__line tm-placeholder__line_icon-text"></div></div><div class="tm-placeholder-data-icon"><div class="tm-placeholder__icon tm-placeholder__icon_large loads"></div> <div class="tm-placeholder__line tm-placeholder__line_icon-text"></div></div><div class="tm-placeholder-data-icon"><div class="tm-placeholder__icon tm-placeholder__icon_large loads"></div> <div class="tm-placeholder__line tm-placeholder__line_icon-text"></div></div><div class="tm-placeholder-data-icon"><div class="tm-placeholder__icon tm-placeholder__icon_large loads"></div> <div class="tm-placeholder__line tm-placeholder__line_icon-text"></div></div></div></div><div class="tm-placeholder-article-card"><div class="tm-placeholder__user"><div class="tm-placeholder__user-pic loads"></div> <div class="tm-placeholder__user-date loads"></div></div> <div class="tm-placeholder-article-card__title"><div class="tm-placeholder__line tm-placeholder-article-card__title-line loads"></div> <div class="tm-placeholder__line tm-placeholder-article-card__title-line loads"></div></div> <div class="tm-placeholder-article-card__icons tm-placeholder__counters"><div class="tm-placeholder-data-icon"><div class="tm-placeholder__icon tm-placeholder__icon_large loads"></div> <div class="tm-placeholder__line tm-placeholder__line_icon-text"></div></div><div class="tm-placeholder-data-icon"><div class="tm-placeholder__icon tm-placeholder__icon_large loads"></div> <div class="tm-placeholder__line tm-placeholder__line_icon-text"></div></div><div class="tm-placeholder-data-icon"><div class="tm-placeholder__icon tm-placeholder__icon_large loads"></div> <div class="tm-placeholder__line tm-placeholder__line_icon-text"></div></div><div class="tm-placeholder-data-icon"><div class="tm-placeholder__icon tm-placeholder__icon_large loads"></div> <div class="tm-placeholder__line tm-placeholder__line_icon-text"></div></div></div></div><div class="tm-placeholder-article-card"><div class="tm-placeholder__user"><div class="tm-placeholder__user-pic loads"></div> <div class="tm-placeholder__user-date loads"></div></div> <div class="tm-placeholder-article-card__title"><div class="tm-placeholder__line tm-placeholder-article-card__title-line loads"></div> <div class="tm-placeholder__line tm-placeholder-article-card__title-line loads"></div></div> <div class="tm-placeholder-article-card__icons tm-placeholder__counters"><div class="tm-placeholder-data-icon"><div class="tm-placeholder__icon tm-placeholder__icon_large loads"></div> <div class="tm-placeholder__line tm-placeholder__line_icon-text"></div></div><div class="tm-placeholder-data-icon"><div class="tm-placeholder__icon tm-placeholder__icon_large loads"></div> <div class="tm-placeholder__line tm-placeholder__line_icon-text"></div></div><div class="tm-placeholder-data-icon"><div class="tm-placeholder__icon tm-placeholder__icon_large loads"></div> <div class="tm-placeholder__line tm-placeholder__line_icon-text"></div></div><div class="tm-placeholder-data-icon"><div class="tm-placeholder__icon tm-placeholder__icon_large loads"></div> <div class="tm-placeholder__line tm-placeholder__line_icon-text"></div></div></div></div><div class="tm-placeholder-article-card"><div class="tm-placeholder__user"><div class="tm-placeholder__user-pic loads"></div> <div class="tm-placeholder__user-date loads"></div></div> <div class="tm-placeholder-article-card__title"><div class="tm-placeholder__line tm-placeholder-article-card__title-line loads"></div> <div class="tm-placeholder__line tm-placeholder-article-card__title-line loads"></div></div> <div class="tm-placeholder-article-card__icons tm-placeholder__counters"><div class="tm-placeholder-data-icon"><div class="tm-placeholder__icon tm-placeholder__icon_large loads"></div> <div class="tm-placeholder__line tm-placeholder__line_icon-text"></div></div><div class="tm-placeholder-data-icon"><div class="tm-placeholder__icon tm-placeholder__icon_large loads"></div> <div class="tm-placeholder__line tm-placeholder__line_icon-text"></div></div><div class="tm-placeholder-data-icon"><div class="tm-placeholder__icon tm-placeholder__icon_large loads"></div> <div class="tm-placeholder__line tm-placeholder__line_icon-text"></div></div><div class="tm-placeholder-data-icon"><div class="tm-placeholder__icon tm-placeholder__icon_large loads"></div> <div class="tm-placeholder__line tm-placeholder__line_icon-text"></div></div></div></div><div class="tm-placeholder-article-card"><div class="tm-placeholder__user"><div class="tm-placeholder__user-pic loads"></div> <div class="tm-placeholder__user-date loads"></div></div> <div class="tm-placeholder-article-card__title"><div class="tm-placeholder__line tm-placeholder-article-card__title-line loads"></div> <div class="tm-placeholder__line tm-placeholder-article-card__title-line loads"></div></div> <div class="tm-placeholder-article-card__icons tm-placeholder__counters"><div class="tm-placeholder-data-icon"><div class="tm-placeholder__icon tm-placeholder__icon_large loads"></div> <div class="tm-placeholder__line tm-placeholder__line_icon-text"></div></div><div class="tm-placeholder-data-icon"><div class="tm-placeholder__icon tm-placeholder__icon_large loads"></div> <div class="tm-placeholder__line tm-placeholder__line_icon-text"></div></div><div class="tm-placeholder-data-icon"><div class="tm-placeholder__icon tm-placeholder__icon_large loads"></div> <div class="tm-placeholder__line tm-placeholder__line_icon-text"></div></div><div class="tm-placeholder-data-icon"><div class="tm-placeholder__icon tm-placeholder__icon_large loads"></div> <div class="tm-placeholder__line tm-placeholder__line_icon-text"></div></div></div></div></div> <!----> <!----> <!----> <!----> <!----></div> <!----></div></div> <!----></section> <div><div class="placeholder-wrapper"><!----> <!----> <!----> <!----> <!----> <!----> <!----> <!----> <!----> <!----> <!----> <!----> <!----> <!----> <!----> <!----> <!----> <!----> <!----> <!----> <!----> <!----> <!----> <div class="tm-placeholder-promo"><div class="tm-placeholder-promo__header"><div class="tm-placeholder__line tm-placeholder__line_promo-title"></div></div> <div class="tm-placeholder-promo__body"><div class="tm-placeholder-promo__posts"><div class="tm-placeholder-promo__post"><div class="tm-placeholder-promo__image"></div> <div class="tm-placeholder__line tm-placeholder__line_post-title"></div></div> <div class="tm-placeholder-promo__post"><div class="tm-placeholder-promo__image"></div> <div class="tm-placeholder__line tm-placeholder__line_post-title"></div></div> <div class="tm-placeholder-promo__post"><div class="tm-placeholder-promo__image"></div> <div class="tm-placeholder__line tm-placeholder__line_post-title"></div></div></div> <div class="tm-placeholder-promo__dots"><div class="tm-placeholder-promo__dot"></div> <div class="tm-placeholder-promo__dot"></div> <div class="tm-placeholder-promo__dot"></div></div></div></div> <!----> <!----> <!----> <!----> <!----> <!----></div></div> <div hubs="python,data_mining,bigdata,machine_learning,artificial_intelligence" class="placeholder-wrapper"><!----> <!----> <!----> <!----> <!----> <!----> <!----> <!----> <!----> <!----> <!----> <!----> <!----> <!----> <!----> <!----> <!----> <!----> <!----> <div class="tm-placeholder-inset tm-placeholder-questions"><div class="tm-placeholder-inset__header"><div class="tm-placeholder__line tm-placeholder__line_inset-header loads"></div></div> <div class="tm-placeholder-inset__body"><ul class="tm-placeholder-list"><li class="tm-placeholder-list__item tm-placeholder-list__item_inset"><div class="tm-placeholder__line tm-placeholder__line_item-title loads"></div> <div class="tm-project-block-items__properties"><span class="tm-project-block-items__property-item"><span class="tm-placeholder__line loads" style="width: 100px;"></span></span><span class="tm-project-block-items__property-item"><span class="tm-placeholder__line loads" style="width: 100px;"></span></span><span class="tm-project-block-items__property-item"><span class="tm-placeholder__line loads" style="width: 100px;"></span></span></div></li><li class="tm-placeholder-list__item tm-placeholder-list__item_inset"><div class="tm-placeholder__line tm-placeholder__line_item-title loads"></div> <div class="tm-project-block-items__properties"><span class="tm-project-block-items__property-item"><span class="tm-placeholder__line loads" style="width: 100px;"></span></span><span class="tm-project-block-items__property-item"><span class="tm-placeholder__line loads" style="width: 100px;"></span></span><span class="tm-project-block-items__property-item"><span class="tm-placeholder__line loads" style="width: 100px;"></span></span></div></li><li class="tm-placeholder-list__item tm-placeholder-list__item_inset"><div class="tm-placeholder__line tm-placeholder__line_item-title loads"></div> <div class="tm-project-block-items__properties"><span class="tm-project-block-items__property-item"><span class="tm-placeholder__line loads" style="width: 100px;"></span></span><span class="tm-project-block-items__property-item"><span class="tm-placeholder__line loads" style="width: 100px;"></span></span><span class="tm-project-block-items__property-item"><span class="tm-placeholder__line loads" style="width: 100px;"></span></span></div></li><li class="tm-placeholder-list__item tm-placeholder-list__item_inset"><div class="tm-placeholder__line tm-placeholder__line_item-title loads"></div> <div class="tm-project-block-items__properties"><span class="tm-project-block-items__property-item"><span class="tm-placeholder__line loads" style="width: 100px;"></span></span><span class="tm-project-block-items__property-item"><span class="tm-placeholder__line loads" style="width: 100px;"></span></span><span class="tm-project-block-items__property-item"><span class="tm-placeholder__line loads" style="width: 100px;"></span></span></div></li><li class="tm-placeholder-list__item tm-placeholder-list__item_inset"><div class="tm-placeholder__line tm-placeholder__line_item-title loads"></div> <div class="tm-project-block-items__properties"><span class="tm-project-block-items__property-item"><span class="tm-placeholder__line loads" style="width: 100px;"></span></span><span class="tm-project-block-items__property-item"><span class="tm-placeholder__line loads" style="width: 100px;"></span></span><span class="tm-project-block-items__property-item"><span class="tm-placeholder__line loads" style="width: 100px;"></span></span></div></li></ul></div> <div class="tm-placeholder-inset__footer"><div class="tm-placeholder__line tm-placeholder__line_inset-footer loads"></div></div></div> <!----> <!----> <!----> <!----> <!----> <!----> <!----> <!----> <!----> <!----></div> <!----> </div></div></div></div></div> <div class="tm-page__sidebar"><div class="tm-layout-sidebar"><div class="tm-layout-sidebar__ads tm-layout-sidebar__ads_initial"><div class="tm-adfox-banner__container tm-layout-sidebar__banner tm-layout-sidebar__banner_top"><!----> <div id="adfox_164725680533065327" class="tm-adfox-banner tm-adfox-banner tm-adfox-banner_variant-half-page"></div></div></div> <div class="tm-sexy-sidebar tm-sexy-sidebar_initial" style="margin-top:0px;"><!----> <section data-navigatable="" tabindex="0" data-async-called="true" class="tm-block tm-stories-block tm-block tm-block_spacing-around"><header class="tm-block__header tm-block__header"><div class="tm-block__header-container"><h2 class="tm-block__title tm-block__title">Истории</h2> </div> <!----></header> <div class="tm-block__body tm-block__body tm-block__body_variant-equal"><div class="tm-stories-empty"><div class="tm-stories-card-empty"><div class="tm-stories-card-empty__image"></div> <div class="tm-stories-card-empty__title"><div class="tm-stories-card-empty__title-block"></div> <div class="tm-stories-card-empty__title-block"></div> <div class="tm-stories-card-empty__title-block"></div></div></div><div class="tm-stories-card-empty"><div class="tm-stories-card-empty__image"></div> <div class="tm-stories-card-empty__title"><div class="tm-stories-card-empty__title-block"></div> <div class="tm-stories-card-empty__title-block"></div> <div class="tm-stories-card-empty__title-block"></div></div></div><div class="tm-stories-card-empty"><div class="tm-stories-card-empty__image"></div> <div class="tm-stories-card-empty__title"><div class="tm-stories-card-empty__title-block"></div> <div class="tm-stories-card-empty__title-block"></div> <div class="tm-stories-card-empty__title-block"></div></div></div><div class="tm-stories-card-empty"><div class="tm-stories-card-empty__image"></div> <div class="tm-stories-card-empty__title"><div class="tm-stories-card-empty__title-block"></div> <div class="tm-stories-card-empty__title-block"></div> <div class="tm-stories-card-empty__title-block"></div></div></div><div class="tm-stories-card-empty"><div class="tm-stories-card-empty__image"></div> <div class="tm-stories-card-empty__title"><div class="tm-stories-card-empty__title-block"></div> <div class="tm-stories-card-empty__title-block"></div> <div class="tm-stories-card-empty__title-block"></div></div></div><div class="tm-stories-card-empty"><div class="tm-stories-card-empty__image"></div> <div class="tm-stories-card-empty__title"><div class="tm-stories-card-empty__title-block"></div> <div class="tm-stories-card-empty__title-block"></div> <div class="tm-stories-card-empty__title-block"></div></div></div></div> <!----></div> <!----></section> <section data-async-called="true" class="tm-block tm-block tm-block_spacing-around"><header class="tm-block__header tm-block__header"><div class="tm-block__header-container"><h2 class="tm-block__title tm-block__title">Работа</h2> </div> <!----></header> <div class="tm-block__body tm-block__body"><div class="tm-vacancies-block__item"><a href="https://career.habr.com/vacancies/django_razrabotchik" target="_blank" class="tm-vacancies-block__vacancy-title">
        Django разработчик
      </a> <div class="tm-vacancies-block__vacancies-count">
        42
    вакансии
      </div></div><div class="tm-vacancies-block__item"><a href="https://career.habr.com/vacancies/data_scientist" target="_blank" class="tm-vacancies-block__vacancy-title">
        Data Scientist
      </a> <div class="tm-vacancies-block__vacancies-count">
        73
    вакансии
      </div></div><div class="tm-vacancies-block__item"><a href="https://career.habr.com/vacancies/programmist_python" target="_blank" class="tm-vacancies-block__vacancy-title">
        Python разработчик
      </a> <div class="tm-vacancies-block__vacancies-count">
        129
    вакансий
      </div></div></div> <footer class="tm-block__footer"><a href="https://career.habr.com/catalog" class="tm-block-extralink">
      Все вакансии
    </a></footer></section> <section data-async-called="true" class="tm-block block tm-block tm-block_spacing-around" data-v-cb9bff32><header class="tm-block__header tm-block__header"><div class="tm-block__header-container"><h2 class="tm-block__title tm-block__title">Ближайшие события</h2> </div> <!----></header> <div class="tm-block__body tm-block__body"><div class="swiper-container slider" style="--swiper-space-between:22px;" data-v-cb9bff32><div class="swiper-wrapper"><div class="swiper-slide" data-v-cb9bff32><section id="events_176" tabindex="-1" class="tm-block tm-block tm-block_spacing-none" data-v-cb9bff32><!----> <div class="tm-event-card tm-event-card_is-widget"><img alt="" data-src="https://habrastorage.org/getpro/habr/upload_files/ac9/383/51c/ac938351c65f564f8b0bf373c436de53.png" loading="eager" src="https://habrastorage.org/r/w390/getpro/habr/upload_files/ac9/383/51c/ac938351c65f564f8b0bf373c436de53.png" srcset="https://habrastorage.org/r/w390/getpro/habr/upload_files/ac9/383/51c/ac938351c65f564f8b0bf373c436de53.png, https://habrastorage.org/getpro/habr/upload_files/ac9/383/51c/ac938351c65f564f8b0bf373c436de53.png 2x" class="tm-event-card__image"> <!----> <div class="tm-event-card__info"><a href="/ru/events/#events_176" class="tm-event-card__title-link">Серия занятий «Тренировки по алгоритмам 5.0» от Яндекса</a> <div class="tm-event-card__date"><div class="tm-event-card__day"><svg height="24" width="24" class="tm-svg-img tm-event-card__icon"><title>Дата</title> <use xlink:href="/img/megazord-v28.faec9762..svg#calendar"></use></svg> <span>1  марта   – 19  апреля  </span></div> <div class="tm-event-card__time"><svg height="24" width="24" class="tm-svg-img tm-event-card__icon"><title>Время</title> <use xlink:href="/img/megazord-v28.faec9762..svg#clock"></use></svg> <span>19:00</span></div></div> <div class="tm-event-card__places"><svg height="24" width="24" class="tm-svg-img tm-event-card__icon tm-event-card__icon_place"><title>Место</title> <use xlink:href="/img/megazord-v28.faec9762..svg#geo"></use></svg> <div class="tm-event-card__places-list"><span class="tm-event-card__places-item"><span>Онлайн</span> <!----></span></div></div> <div class="tm-event-card__footer"><a href="/ru/events/#events_176" class="tm-event-card__link">
            Подробнее в календаре
          </a></div></div> <!----></div> <!----></section></div><div class="swiper-slide" data-v-cb9bff32><section id="events_191" tabindex="-1" class="tm-block tm-block tm-block_spacing-none" data-v-cb9bff32><!----> <div class="tm-event-card tm-event-card_is-widget"><img alt="" data-src="https://habrastorage.org/getpro/habr/upload_files/6df/72d/56c/6df72d56ce1040e73b3646064d61f267.png" loading="eager" src="https://habrastorage.org/r/w390/getpro/habr/upload_files/6df/72d/56c/6df72d56ce1040e73b3646064d61f267.png" srcset="https://habrastorage.org/r/w390/getpro/habr/upload_files/6df/72d/56c/6df72d56ce1040e73b3646064d61f267.png, https://habrastorage.org/getpro/habr/upload_files/6df/72d/56c/6df72d56ce1040e73b3646064d61f267.png 2x" class="tm-event-card__image"> <!----> <div class="tm-event-card__info"><a href="/ru/events/#events_191" class="tm-event-card__title-link">DI CONF SMM — большая конференция по соцсетям в России</a> <div class="tm-event-card__date"><div class="tm-event-card__day"><svg height="24" width="24" class="tm-svg-img tm-event-card__icon"><title>Дата</title> <use xlink:href="/img/megazord-v28.faec9762..svg#calendar"></use></svg> <span>2  марта  </span></div> <div class="tm-event-card__time"><svg height="24" width="24" class="tm-svg-img tm-event-card__icon"><title>Время</title> <use xlink:href="/img/megazord-v28.faec9762..svg#clock"></use></svg> <span>09:30 – 18:00</span></div></div> <div class="tm-event-card__places"><svg height="24" width="24" class="tm-svg-img tm-event-card__icon tm-event-card__icon_place"><title>Место</title> <use xlink:href="/img/megazord-v28.faec9762..svg#geo"></use></svg> <div class="tm-event-card__places-list"><span class="tm-event-card__places-item"><span>Краснодар</span> <span class="tm-event-card__places-separator"> • </span></span><span class="tm-event-card__places-item"><span>Онлайн</span> <!----></span></div></div> <div class="tm-event-card__footer"><a href="/ru/events/#events_191" class="tm-event-card__link">
            Подробнее в календаре
          </a></div></div> <!----></div> <!----></section></div><div class="swiper-slide" data-v-cb9bff32><section id="events_140" tabindex="-1" class="tm-block tm-block tm-block_spacing-none" data-v-cb9bff32><!----> <div class="tm-event-card tm-event-card_is-widget"><img alt="" data-src="https://habrastorage.org/getpro/habr/upload_files/2a5/b6d/6fc/2a5b6d6fc13caf3dabce9c8ab833e282.png" loading="eager" src="https://habrastorage.org/r/w390/getpro/habr/upload_files/2a5/b6d/6fc/2a5b6d6fc13caf3dabce9c8ab833e282.png" srcset="https://habrastorage.org/r/w390/getpro/habr/upload_files/2a5/b6d/6fc/2a5b6d6fc13caf3dabce9c8ab833e282.png, https://habrastorage.org/getpro/habr/upload_files/2a5/b6d/6fc/2a5b6d6fc13caf3dabce9c8ab833e282.png 2x" class="tm-event-card__image"> <!----> <div class="tm-event-card__info"><a href="/ru/events/#events_140" class="tm-event-card__title-link">DevOpsConf 2024: конференция для инженеров и всех, кто должен понимать инженеров</a> <div class="tm-event-card__date"><div class="tm-event-card__day"><svg height="24" width="24" class="tm-svg-img tm-event-card__icon"><title>Дата</title> <use xlink:href="/img/megazord-v28.faec9762..svg#calendar"></use></svg> <span>4 – 5  марта  </span></div> <div class="tm-event-card__time"><svg height="24" width="24" class="tm-svg-img tm-event-card__icon"><title>Время</title> <use xlink:href="/img/megazord-v28.faec9762..svg#clock"></use></svg> <span>10:00 – 20:00</span></div></div> <div class="tm-event-card__places"><svg height="24" width="24" class="tm-svg-img tm-event-card__icon tm-event-card__icon_place"><title>Место</title> <use xlink:href="/img/megazord-v28.faec9762..svg#geo"></use></svg> <div class="tm-event-card__places-list"><span class="tm-event-card__places-item"><span>Москва</span> <span class="tm-event-card__places-separator"> • </span></span><span class="tm-event-card__places-item"><span>Онлайн</span> <!----></span></div></div> <div class="tm-event-card__footer"><a href="/ru/events/#events_140" class="tm-event-card__link">
            Подробнее в календаре
          </a></div></div> <!----></div> <!----></section></div><div class="swiper-slide" data-v-cb9bff32><section id="events_189" tabindex="-1" class="tm-block tm-block tm-block_spacing-none" data-v-cb9bff32><!----> <div class="tm-event-card tm-event-card_is-widget"><img alt="" data-src="https://habrastorage.org/getpro/habr/upload_files/274/868/9b0/2748689b03a269ce21d7913bad0c2920.jpg" loading="eager" src="https://habrastorage.org/r/w390/getpro/habr/upload_files/274/868/9b0/2748689b03a269ce21d7913bad0c2920.jpg" srcset="https://habrastorage.org/r/w390/getpro/habr/upload_files/274/868/9b0/2748689b03a269ce21d7913bad0c2920.jpg, https://habrastorage.org/getpro/habr/upload_files/274/868/9b0/2748689b03a269ce21d7913bad0c2920.jpg 2x" class="tm-event-card__image"> <!----> <div class="tm-event-card__info"><a href="/ru/events/#events_189" class="tm-event-card__title-link">Вебинар «Встраиваемые системы и программирование микроконтроллеров»</a> <div class="tm-event-card__date"><div class="tm-event-card__day"><svg height="24" width="24" class="tm-svg-img tm-event-card__icon"><title>Дата</title> <use xlink:href="/img/megazord-v28.faec9762..svg#calendar"></use></svg> <span>6  марта  </span></div> <div class="tm-event-card__time"><svg height="24" width="24" class="tm-svg-img tm-event-card__icon"><title>Время</title> <use xlink:href="/img/megazord-v28.faec9762..svg#clock"></use></svg> <span>20:00</span></div></div> <div class="tm-event-card__places"><svg height="24" width="24" class="tm-svg-img tm-event-card__icon tm-event-card__icon_place"><title>Место</title> <use xlink:href="/img/megazord-v28.faec9762..svg#geo"></use></svg> <div class="tm-event-card__places-list"><span class="tm-event-card__places-item"><span>Онлайн</span> <!----></span></div></div> <div class="tm-event-card__footer"><a href="/ru/events/#events_189" class="tm-event-card__link">
            Подробнее в календаре
          </a></div></div> <!----></div> <!----></section></div><div class="swiper-slide" data-v-cb9bff32><section id="events_193" tabindex="-1" class="tm-block tm-block tm-block_spacing-none" data-v-cb9bff32><!----> <div class="tm-event-card tm-event-card_is-widget"><img alt="" data-src="https://habrastorage.org/getpro/habr/upload_files/c09/b0f/7c0/c09b0f7c0b7c909a1e36141ac9f38fc1.png" loading="eager" src="https://habrastorage.org/r/w390/getpro/habr/upload_files/c09/b0f/7c0/c09b0f7c0b7c909a1e36141ac9f38fc1.png" srcset="https://habrastorage.org/r/w390/getpro/habr/upload_files/c09/b0f/7c0/c09b0f7c0b7c909a1e36141ac9f38fc1.png, https://habrastorage.org/getpro/habr/upload_files/c09/b0f/7c0/c09b0f7c0b7c909a1e36141ac9f38fc1.png 2x" class="tm-event-card__image"> <!----> <div class="tm-event-card__info"><a href="/ru/events/#events_193" class="tm-event-card__title-link">Вебинар «Применение обучения с подкреплением на финансовых рынках»</a> <div class="tm-event-card__date"><div class="tm-event-card__day"><svg height="24" width="24" class="tm-svg-img tm-event-card__icon"><title>Дата</title> <use xlink:href="/img/megazord-v28.faec9762..svg#calendar"></use></svg> <span>6  марта  </span></div> <div class="tm-event-card__time"><svg height="24" width="24" class="tm-svg-img tm-event-card__icon"><title>Время</title> <use xlink:href="/img/megazord-v28.faec9762..svg#clock"></use></svg> <span>20:00</span></div></div> <div class="tm-event-card__places"><svg height="24" width="24" class="tm-svg-img tm-event-card__icon tm-event-card__icon_place"><title>Место</title> <use xlink:href="/img/megazord-v28.faec9762..svg#geo"></use></svg> <div class="tm-event-card__places-list"><span class="tm-event-card__places-item"><span>Онлайн</span> <!----></span></div></div> <div class="tm-event-card__footer"><a href="/ru/events/#events_193" class="tm-event-card__link">
            Подробнее в календаре
          </a></div></div> <!----></div> <!----></section></div><div class="swiper-slide" data-v-cb9bff32><section id="events_195" tabindex="-1" class="tm-block tm-block tm-block_spacing-none" data-v-cb9bff32><!----> <div class="tm-event-card tm-event-card_is-widget"><img alt="" data-src="https://habrastorage.org/getpro/habr/upload_files/01f/c57/832/01fc578328619f3669a6791566394e14.png" loading="eager" src="https://habrastorage.org/r/w390/getpro/habr/upload_files/01f/c57/832/01fc578328619f3669a6791566394e14.png" srcset="https://habrastorage.org/r/w390/getpro/habr/upload_files/01f/c57/832/01fc578328619f3669a6791566394e14.png, https://habrastorage.org/getpro/habr/upload_files/01f/c57/832/01fc578328619f3669a6791566394e14.png 2x" class="tm-event-card__image"> <!----> <div class="tm-event-card__info"><a href="/ru/events/#events_195" class="tm-event-card__title-link">Вебинар «Методы и принципы разработки ПО для встраиваемых устройств»</a> <div class="tm-event-card__date"><div class="tm-event-card__day"><svg height="24" width="24" class="tm-svg-img tm-event-card__icon"><title>Дата</title> <use xlink:href="/img/megazord-v28.faec9762..svg#calendar"></use></svg> <span>6  марта  </span></div> <div class="tm-event-card__time"><svg height="24" width="24" class="tm-svg-img tm-event-card__icon"><title>Время</title> <use xlink:href="/img/megazord-v28.faec9762..svg#clock"></use></svg> <span>20:00</span></div></div> <div class="tm-event-card__places"><svg height="24" width="24" class="tm-svg-img tm-event-card__icon tm-event-card__icon_place"><title>Место</title> <use xlink:href="/img/megazord-v28.faec9762..svg#geo"></use></svg> <div class="tm-event-card__places-list"><span class="tm-event-card__places-item"><span>Онлайн</span> <!----></span></div></div> <div class="tm-event-card__footer"><a href="/ru/events/#events_195" class="tm-event-card__link">
            Подробнее в календаре
          </a></div></div> <!----></div> <!----></section></div><div class="swiper-slide" data-v-cb9bff32><section id="events_197" tabindex="-1" class="tm-block tm-block tm-block_spacing-none" data-v-cb9bff32><!----> <div class="tm-event-card tm-event-card_is-widget"><img alt="" data-src="https://habrastorage.org/getpro/habr/upload_files/cec/374/6fc/cec3746fc1129aff78a52f8a160051cd.png" loading="eager" src="https://habrastorage.org/r/w390/getpro/habr/upload_files/cec/374/6fc/cec3746fc1129aff78a52f8a160051cd.png" srcset="https://habrastorage.org/r/w390/getpro/habr/upload_files/cec/374/6fc/cec3746fc1129aff78a52f8a160051cd.png, https://habrastorage.org/getpro/habr/upload_files/cec/374/6fc/cec3746fc1129aff78a52f8a160051cd.png 2x" class="tm-event-card__image"> <!----> <div class="tm-event-card__info"><a href="/ru/events/#events_197" class="tm-event-card__title-link">Открытый урок «Пишем онлайн-чат на Golang»</a> <div class="tm-event-card__date"><div class="tm-event-card__day"><svg height="24" width="24" class="tm-svg-img tm-event-card__icon"><title>Дата</title> <use xlink:href="/img/megazord-v28.faec9762..svg#calendar"></use></svg> <span>12  марта  </span></div> <div class="tm-event-card__time"><svg height="24" width="24" class="tm-svg-img tm-event-card__icon"><title>Время</title> <use xlink:href="/img/megazord-v28.faec9762..svg#clock"></use></svg> <span>20:00</span></div></div> <div class="tm-event-card__places"><svg height="24" width="24" class="tm-svg-img tm-event-card__icon tm-event-card__icon_place"><title>Место</title> <use xlink:href="/img/megazord-v28.faec9762..svg#geo"></use></svg> <div class="tm-event-card__places-list"><span class="tm-event-card__places-item"><span>Онлайн</span> <!----></span></div></div> <div class="tm-event-card__footer"><a href="/ru/events/#events_197" class="tm-event-card__link">
            Подробнее в календаре
          </a></div></div> <!----></div> <!----></section></div><div class="swiper-slide" data-v-cb9bff32><section id="events_160" tabindex="-1" class="tm-block tm-block tm-block_spacing-none" data-v-cb9bff32><!----> <div class="tm-event-card tm-event-card_is-widget"><img alt="" data-src="https://habrastorage.org/getpro/habr/upload_files/9e2/1fc/f5c/9e21fcf5cb7e9bae5b0f7cb2fafd6a99.png" loading="eager" src="https://habrastorage.org/r/w390/getpro/habr/upload_files/9e2/1fc/f5c/9e21fcf5cb7e9bae5b0f7cb2fafd6a99.png" srcset="https://habrastorage.org/r/w390/getpro/habr/upload_files/9e2/1fc/f5c/9e21fcf5cb7e9bae5b0f7cb2fafd6a99.png, https://habrastorage.org/getpro/habr/upload_files/9e2/1fc/f5c/9e21fcf5cb7e9bae5b0f7cb2fafd6a99.png 2x" class="tm-event-card__image"> <!----> <div class="tm-event-card__info"><a href="/ru/events/#events_160" class="tm-event-card__title-link">«GoCloud 2024. Облачные грани будущего» — IT-конференция Cloud.ru про облака</a> <div class="tm-event-card__date"><div class="tm-event-card__day"><svg height="24" width="24" class="tm-svg-img tm-event-card__icon"><title>Дата</title> <use xlink:href="/img/megazord-v28.faec9762..svg#calendar"></use></svg> <span>21  марта  </span></div> <div class="tm-event-card__time"><svg height="24" width="24" class="tm-svg-img tm-event-card__icon"><title>Время</title> <use xlink:href="/img/megazord-v28.faec9762..svg#clock"></use></svg> <span>09:00 – 18:00</span></div></div> <div class="tm-event-card__places"><svg height="24" width="24" class="tm-svg-img tm-event-card__icon tm-event-card__icon_place"><title>Место</title> <use xlink:href="/img/megazord-v28.faec9762..svg#geo"></use></svg> <div class="tm-event-card__places-list"><span class="tm-event-card__places-item"><span>Москва</span> <span class="tm-event-card__places-separator"> • </span></span><span class="tm-event-card__places-item"><span>Онлайн</span> <!----></span></div></div> <div class="tm-event-card__footer"><a href="/ru/events/#events_160" class="tm-event-card__link">
            Подробнее в календаре
          </a></div></div> <!----></div> <!----></section></div><div class="swiper-slide" data-v-cb9bff32><section id="events_187" tabindex="-1" class="tm-block tm-block tm-block_spacing-none" data-v-cb9bff32><!----> <div class="tm-event-card tm-event-card_is-widget"><img alt="" data-src="https://habrastorage.org/getpro/habr/upload_files/ffd/8b7/394/ffd8b73941c1f2eb35931067b11406ce.jpg" loading="eager" src="https://habrastorage.org/r/w390/getpro/habr/upload_files/ffd/8b7/394/ffd8b73941c1f2eb35931067b11406ce.jpg" srcset="https://habrastorage.org/r/w390/getpro/habr/upload_files/ffd/8b7/394/ffd8b73941c1f2eb35931067b11406ce.jpg, https://habrastorage.org/getpro/habr/upload_files/ffd/8b7/394/ffd8b73941c1f2eb35931067b11406ce.jpg 2x" class="tm-event-card__image"> <!----> <div class="tm-event-card__info"><a href="/ru/events/#events_187" class="tm-event-card__title-link">Московский туристический хакатон</a> <div class="tm-event-card__date"><div class="tm-event-card__day"><svg height="24" width="24" class="tm-svg-img tm-event-card__icon"><title>Дата</title> <use xlink:href="/img/megazord-v28.faec9762..svg#calendar"></use></svg> <span>23  марта   – 7  апреля  </span></div> <!----></div> <div class="tm-event-card__places"><svg height="24" width="24" class="tm-svg-img tm-event-card__icon tm-event-card__icon_place"><title>Место</title> <use xlink:href="/img/megazord-v28.faec9762..svg#geo"></use></svg> <div class="tm-event-card__places-list"><span class="tm-event-card__places-item"><span>Москва</span> <span class="tm-event-card__places-separator"> • </span></span><span class="tm-event-card__places-item"><span>Онлайн</span> <!----></span></div></div> <div class="tm-event-card__footer"><a href="/ru/events/#events_187" class="tm-event-card__link">
            Подробнее в календаре
          </a></div></div> <!----></div> <!----></section></div><div class="swiper-slide" data-v-cb9bff32><section id="events_199" tabindex="-1" class="tm-block tm-block tm-block_spacing-none" data-v-cb9bff32><!----> <div class="tm-event-card tm-event-card_is-widget"><img alt="" data-src="https://habrastorage.org/getpro/habr/upload_files/a37/b1f/04a/a37b1f04a5feb49b221e0b1f4bf35c60.jpg" loading="eager" src="https://habrastorage.org/r/w390/getpro/habr/upload_files/a37/b1f/04a/a37b1f04a5feb49b221e0b1f4bf35c60.jpg" srcset="https://habrastorage.org/r/w390/getpro/habr/upload_files/a37/b1f/04a/a37b1f04a5feb49b221e0b1f4bf35c60.jpg, https://habrastorage.org/getpro/habr/upload_files/a37/b1f/04a/a37b1f04a5feb49b221e0b1f4bf35c60.jpg 2x" class="tm-event-card__image"> <!----> <div class="tm-event-card__info"><a href="/ru/events/#events_199" class="tm-event-card__title-link">Онлайн-презентация «GitVerse: открой вселенную кода»</a> <div class="tm-event-card__date"><div class="tm-event-card__day"><svg height="24" width="24" class="tm-svg-img tm-event-card__icon"><title>Дата</title> <use xlink:href="/img/megazord-v28.faec9762..svg#calendar"></use></svg> <span>29  марта  </span></div> <div class="tm-event-card__time"><svg height="24" width="24" class="tm-svg-img tm-event-card__icon"><title>Время</title> <use xlink:href="/img/megazord-v28.faec9762..svg#clock"></use></svg> <span>10:00 – 13:00</span></div></div> <div class="tm-event-card__places"><svg height="24" width="24" class="tm-svg-img tm-event-card__icon tm-event-card__icon_place"><title>Место</title> <use xlink:href="/img/megazord-v28.faec9762..svg#geo"></use></svg> <div class="tm-event-card__places-list"><span class="tm-event-card__places-item"><span>Онлайн</span> <!----></span></div></div> <div class="tm-event-card__footer"><a href="/ru/events/#events_199" class="tm-event-card__link">
            Подробнее в календаре
          </a></div></div> <!----></div> <!----></section></div><div class="swiper-slide" data-v-cb9bff32><section id="events_178" tabindex="-1" class="tm-block tm-block tm-block_spacing-none" data-v-cb9bff32><!----> <div class="tm-event-card tm-event-card_is-widget"><img alt="" data-src="https://habrastorage.org/getpro/habr/upload_files/d78/8ad/5cb/d788ad5cb236e7eebb4832ccc5310645.png" loading="eager" src="https://habrastorage.org/r/w390/getpro/habr/upload_files/d78/8ad/5cb/d788ad5cb236e7eebb4832ccc5310645.png" srcset="https://habrastorage.org/r/w390/getpro/habr/upload_files/d78/8ad/5cb/d788ad5cb236e7eebb4832ccc5310645.png, https://habrastorage.org/getpro/habr/upload_files/d78/8ad/5cb/d788ad5cb236e7eebb4832ccc5310645.png 2x" class="tm-event-card__image"> <!----> <div class="tm-event-card__info"><a href="/ru/events/#events_178" class="tm-event-card__title-link">Firebird Conf: конференция для разработчиков и администраторов СУБД Firebird</a> <div class="tm-event-card__date"><div class="tm-event-card__day"><svg height="24" width="24" class="tm-svg-img tm-event-card__icon"><title>Дата</title> <use xlink:href="/img/megazord-v28.faec9762..svg#calendar"></use></svg> <span>6  июня  </span></div> <div class="tm-event-card__time"><svg height="24" width="24" class="tm-svg-img tm-event-card__icon"><title>Время</title> <use xlink:href="/img/megazord-v28.faec9762..svg#clock"></use></svg> <span>09:00 – 20:00</span></div></div> <div class="tm-event-card__places"><svg height="24" width="24" class="tm-svg-img tm-event-card__icon tm-event-card__icon_place"><title>Место</title> <use xlink:href="/img/megazord-v28.faec9762..svg#geo"></use></svg> <div class="tm-event-card__places-list"><span class="tm-event-card__places-item"><span>Москва</span> <!----></span></div></div> <div class="tm-event-card__footer"><a href="/ru/events/#events_178" class="tm-event-card__link">
            Подробнее в календаре
          </a></div></div> <!----></div> <!----></section></div></div> <!----> <button class="swiper-button-prev"><span class="tm-svg-icon__wrapper swiper-button-icon"><svg height="24" width="24" class="tm-svg-img tm-svg-icon"><title>Влево</title> <use xlink:href="/img/megazord-v28.faec9762..svg#arrow-back"></use></svg></span></button> <button class="swiper-button-next"><span class="tm-svg-icon__wrapper swiper-button-icon"><svg height="24" width="24" class="tm-svg-img tm-svg-icon"><title>Вправо</title> <use xlink:href="/img/megazord-v28.faec9762..svg#arrow-back"></use></svg></span></button></div></div> <!----></section> <div class="tm-adfox-banner__container tm-layout-sidebar__banner tm-layout-sidebar__banner_bottom"><!----> <div id="adfox_164725691003361602" class="tm-adfox-banner tm-adfox-banner tm-adfox-banner_variant-medium-rectangle"></div></div></div></div></div></div></div></div></main> <!----></div> <div class="tm-footer-menu"><div class="tm-page-width"><div class="tm-footer-menu__container"><div class="tm-footer-menu__block"><p class="tm-footer-menu__block-title">
          Ваш аккаунт
        </p> <div class="tm-footer-menu__block-content"><ul class="tm-footer-menu__list"><li class="tm-footer-menu__list-item"><a href="/kek/v1/auth/habrahabr/?back=/ru/articles/704592/&amp;hl=ru" rel="nofollow" target="_self">
                Войти
              </a></li><li class="tm-footer-menu__list-item"><a href="/kek/v1/auth/habrahabr-register/?back=/ru/articles/704592/&amp;hl=ru" rel="nofollow" target="_self">
                Регистрация
              </a></li></ul></div></div><div class="tm-footer-menu__block"><p class="tm-footer-menu__block-title">
          Разделы
        </p> <div class="tm-footer-menu__block-content"><ul class="tm-footer-menu__list"><li class="tm-footer-menu__list-item"><a href="/ru/articles/" class="footer-menu__item-link router-link-active">
                Статьи
              </a></li><li class="tm-footer-menu__list-item"><a href="/ru/news/" class="footer-menu__item-link">
                Новости
              </a></li><li class="tm-footer-menu__list-item"><a href="/ru/hubs/" class="footer-menu__item-link">
                Хабы
              </a></li><li class="tm-footer-menu__list-item"><a href="/ru/companies/" class="footer-menu__item-link">
                Компании
              </a></li><li class="tm-footer-menu__list-item"><a href="/ru/users/" class="footer-menu__item-link">
                Авторы
              </a></li><li class="tm-footer-menu__list-item"><a href="/ru/sandbox/" class="footer-menu__item-link">
                Песочница
              </a></li></ul></div></div><div class="tm-footer-menu__block"><p class="tm-footer-menu__block-title">
          Информация
        </p> <div class="tm-footer-menu__block-content"><ul class="tm-footer-menu__list"><li class="tm-footer-menu__list-item"><a href="/ru/docs/help/" class="footer-menu__item-link">
                Устройство сайта
              </a></li><li class="tm-footer-menu__list-item"><a href="/ru/docs/authors/codex/" class="footer-menu__item-link">
                Для авторов
              </a></li><li class="tm-footer-menu__list-item"><a href="/ru/docs/companies/corpblogs/" class="footer-menu__item-link">
                Для компаний
              </a></li><li class="tm-footer-menu__list-item"><a href="/ru/docs/docs/transparency/" class="footer-menu__item-link">
                Документы
              </a></li><li class="tm-footer-menu__list-item"><a href="https://account.habr.com/info/agreement/?hl=ru_RU" target="_blank">
                Соглашение
              </a></li><li class="tm-footer-menu__list-item"><a href="https://account.habr.com/info/confidential/?hl=ru_RU" target="_blank">
                Конфиденциальность
              </a></li></ul></div></div><div class="tm-footer-menu__block"><p class="tm-footer-menu__block-title">
          Услуги
        </p> <div class="tm-footer-menu__block-content"><ul class="tm-footer-menu__list"><li class="tm-footer-menu__list-item"><a href="https://company.habr.com/ru/corporate-blogs/" target="_blank">
                Корпоративный блог
              </a></li><li class="tm-footer-menu__list-item"><a href="https://company.habr.com/ru/advertising/" target="_blank">
                Медийная реклама
              </a></li><li class="tm-footer-menu__list-item"><a href="https://company.habr.com/ru/native-special/" target="_blank">
                Нативные проекты
              </a></li><li class="tm-footer-menu__list-item"><a href="https://company.habr.com/ru/education-programs/" target="_blank">
                Образовательные программы
              </a></li><li class="tm-footer-menu__list-item"><a href="https://company.habr.com/ru/hello-startup/" target="_blank">
                Стартапам
              </a></li></ul></div></div></div></div></div> <div class="tm-footer"><div class="tm-page-width"><div class="tm-footer__container"><!----> <div class="tm-footer__social"><a href="https://www.facebook.com/habrahabr.ru" rel="nofollow noopener noreferrer" target="_blank" class="tm-svg-icon__wrapper tm-social-icons__icon"><svg height="24" width="24" class="tm-svg-img tm-svg-icon"><title>Facebook</title> <use xlink:href="/img/new-social-icons-sprite.svg#social-logo-facebook"></use></svg></a><a href="https://twitter.com/habr_com" rel="nofollow noopener noreferrer" target="_blank" class="tm-svg-icon__wrapper tm-social-icons__icon"><svg height="24" width="24" class="tm-svg-img tm-svg-icon"><title>Twitter</title> <use xlink:href="/img/new-social-icons-sprite.svg#social-logo-twitter"></use></svg></a><a href="https://vk.com/habr" rel="nofollow noopener noreferrer" target="_blank" class="tm-svg-icon__wrapper tm-social-icons__icon"><svg height="24" width="24" class="tm-svg-img tm-svg-icon"><title>VK</title> <use xlink:href="/img/new-social-icons-sprite.svg#social-logo-vk"></use></svg></a><a href="https://telegram.me/habr_com" rel="nofollow noopener noreferrer" target="_blank" class="tm-svg-icon__wrapper tm-social-icons__icon"><svg height="24" width="24" class="tm-svg-img tm-svg-icon"><title>Telegram</title> <use xlink:href="/img/new-social-icons-sprite.svg#social-logo-telegram"></use></svg></a><a href="https://www.youtube.com/channel/UCd_sTwKqVrweTt4oAKY5y4w" rel="nofollow noopener noreferrer" target="_blank" class="tm-svg-icon__wrapper tm-social-icons__icon"><svg height="24" width="24" class="tm-svg-img tm-svg-icon"><title>Youtube</title> <use xlink:href="/img/new-social-icons-sprite.svg#social-logo-youtube"></use></svg></a><a href="https://dzen.ru/habr" rel="nofollow noopener noreferrer" target="_blank" class="tm-svg-icon__wrapper tm-social-icons__icon"><svg height="24" width="24" class="tm-svg-img tm-svg-icon"><title>Яндекс Дзен</title> <use xlink:href="/img/new-social-icons-sprite.svg#social-logo-dzen"></use></svg></a></div> <DIV class="v-portal" style="display:none;"></DIV> <button class="tm-footer__link"><!---->
        Настройка языка
      </button> <a href="/ru/feedback/" class="tm-footer__link">
        Техническая поддержка
      </a> <div class="tm-footer-copyright"><span class="tm-copyright"><span class="tm-copyright__years">© 2006–2024, </span> <span class="tm-copyright__name"><a href="https://company.habr.com/" rel="noopener" target="_blank" class="tm-copyright__link">Habr</a></span></span></div></div></div></div> <!----></div> <div class="vue-portal-target"></div></div>
<script>window.__INITIAL_STATE__={"adblock":{"hasAcceptableAdsFilter":false,"hasAdblock":false},"articlesList":{"articlesList":{"704592":{"id":"704592","timePublished":"2022-12-09T16:23:20+00:00","isCorporative":false,"lang":"ru","titleHtml":"Введение в библиотеку Transformers и платформу Hugging Face","leadData":{"textHtml":"\u003Cp\u003EБиблиотека Transformers предоставляет доступ к огромному кол-ву современных предобученных моделей глубокого обучения. В основном основаных на архитектуре трансформеров. Модели решают весьма разнообразный спектр задач: NLP, CV, Audio, Multimodal, Reinforcement Learning, Time Series.\u003C\u002Fp\u003E\u003Cp\u003EВ этой статье пройдемся по основным ее возможностям и попробуем их на практике.\u003C\u002Fp\u003E\u003Cp\u003E\u003C\u002Fp\u003E","imageUrl":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F60f\u002F1a5\u002Faaf\u002F60f1a5aafe57e4d36ffbc1c3ac0e5113.png","buttonTextHtml":"Читать далее","image":{"url":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F60f\u002F1a5\u002Faaf\u002F60f1a5aafe57e4d36ffbc1c3ac0e5113.png","fit":"cover","positionY":47,"positionX":0}},"editorVersion":"2.0","postType":"article","postLabels":[{"type":"technotext2022","typeOf":"technotext","title":"Технотекст 2022","data":{"url":"https:\u002F\u002Fhabr.com\u002Fru\u002Ftechnotext\u002F2022\u002F"}}],"author":{"id":"510086","alias":"slivka_83","fullname":"Вячеслав","avatarUrl":"\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Favatars\u002F9c2\u002F473\u002F9b3\u002F9c24739b37b7521529b59008baf7bf29.png","speciality":"MLE","scoreStats":{"score":25,"votesCount":25},"rating":-1,"relatedData":null,"contacts":[{"title":"Telegram","url":"https:\u002F\u002Ftelegram.me\u002Fslivka_83","value":"slivka_83","siteTitle":null,"favicon":null}],"authorContacts":[{"title":"Telegram","url":"https:\u002F\u002Ftelegram.me\u002Fslivka_83","value":"slivka_83","siteTitle":null,"favicon":null}],"paymentDetails":{"paymentYandexMoney":null,"paymentPayPalMe":null,"paymentWebmoney":null},"donationsMethod":null},"statistics":{"commentsCount":12,"favoritesCount":156,"readingCount":43352,"score":23,"votesCount":23,"votesCountPlus":23,"votesCountMinus":0},"hubs":[{"id":"340","alias":"python","type":"collective","title":"Python","titleHtml":"Python","isProfiled":true,"relatedData":null},{"id":"7152","alias":"data_mining","type":"collective","title":"Data Mining","titleHtml":"Data Mining","isProfiled":true,"relatedData":null},{"id":"17795","alias":"bigdata","type":"collective","title":"Big Data","titleHtml":"Big Data","isProfiled":true,"relatedData":null},{"id":"19439","alias":"machine_learning","type":"collective","title":"Машинное обучение","titleHtml":"Машинное обучение","isProfiled":true,"relatedData":null},{"id":"21922","alias":"artificial_intelligence","type":"collective","title":"Искусственный интеллект","titleHtml":"Искусственный интеллект","isProfiled":false,"relatedData":null}],"flows":[{"id":"1","alias":"develop","title":"Разработка","titleHtml":"Разработка"},{"id":"7","alias":"popsci","title":"Научпоп","titleHtml":"Научпоп"}],"relatedData":null,"textHtml":"\u003Cdiv xmlns=\"http:\u002F\u002Fwww.w3.org\u002F1999\u002Fxhtml\"\u003E\u003Cp\u003EИсходники: \u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fhuggingface\u002Ftransformers\" rel=\"noopener noreferrer nofollow\"\u003E\u003Cu\u003Ehttps:\u002F\u002Fgithub.com\u002Fhuggingface\u002Ftransformers\u003C\u002Fu\u003E\u003C\u002Fa\u003E\u003Cu\u003E\u003Cbr\u002F\u003E\u003C\u002Fu\u003EДокументация: \u003Ca href=\"https:\u002F\u002Fhuggingface.co\u002Fdocs\u002Ftransformers\u002Fmain\u002Fen\u002Findex\" rel=\"noopener noreferrer nofollow\"\u003E\u003Cu\u003Ehttps:\u002F\u002Fhuggingface.co\u002Fdocs\u002Ftransformers\u002Fmain\u002Fen\u002Findex\u003C\u002Fu\u003E\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003Cp\u003EПлатформа Hugging Face это коллекция готовых современных предварительно обученных Deep Learning моделей. А библиотека Transformers предоставляет инструменты и интерфейсы для их простой загрузки и использования. Это позволяет вам экономить время и ресурсы, необходимые для обучения моделей с нуля.\u003C\u002Fp\u003E\u003Cp\u003EМодели решают весьма разнообразный спектр задач:\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli\u003E\u003Cp\u003ENLP: classification, NER, question answering, language modeling, summarization, translation, multiple choice, text generation.\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003ECV: classification, object detection,segmentation.\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003EAudio: classification, automatic speech recognition.\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003EMultimodal: table question answering, optical character recognition, information extraction from scanned documents, video classification, visual question answering.\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003EReinforcement Learning\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003ETime Series\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp\u003EОдна и та же задача может решаться различными архитектурами и их список впечатляет - более 150 на текущий момент. Из наиболее известных: \u003Ca href=\"https:\u002F\u002Fhuggingface.co\u002Fdocs\u002Ftransformers\u002Fmain\u002Fen\u002Fmodel_doc\u002Fvit\" rel=\"noopener noreferrer nofollow\"\u003E\u003Cu\u003EVision Transformer (ViT)\u003C\u002Fu\u003E\u003C\u002Fa\u003E, \u003Ca href=\"https:\u002F\u002Fhuggingface.co\u002Fdocs\u002Ftransformers\u002Fmain\u002Fen\u002Fmodel_doc\u002Ft5\" rel=\"noopener noreferrer nofollow\"\u003E\u003Cu\u003ET5\u003C\u002Fu\u003E\u003C\u002Fa\u003E, \u003Ca href=\"https:\u002F\u002Fhuggingface.co\u002Fdocs\u002Ftransformers\u002Fmain\u002Fen\u002Fmodel_doc\u002Fresnet\" rel=\"noopener noreferrer nofollow\"\u003E\u003Cu\u003EResNet\u003C\u002Fu\u003E\u003C\u002Fa\u003E, \u003Ca href=\"https:\u002F\u002Fhuggingface.co\u002Fdocs\u002Ftransformers\u002Fmain\u002Fen\u002Fmodel_doc\u002Fbert\" rel=\"noopener noreferrer nofollow\"\u003E\u003Cu\u003EBERT\u003C\u002Fu\u003E\u003C\u002Fa\u003E, \u003Ca href=\"https:\u002F\u002Fhuggingface.co\u002Fdocs\u002Ftransformers\u002Fmain\u002Fen\u002Fmodel_doc\u002Fgpt2\" rel=\"noopener noreferrer nofollow\"\u003E\u003Cu\u003EGPT2\u003C\u002Fu\u003E\u003C\u002Fa\u003E. На этих архитектурах обучены более 60 000 моделей.\u003C\u002Fp\u003E\u003Cp\u003EМодели Transformers поддерживают три фреймворках: PyTorch, TensorFlow и JAX. Для На PyTorch’а доступны почти все архитектуры. А вот для остальных надо смотреть совместимость: \u003Ca href=\"https:\u002F\u002Fhuggingface.co\u002Fdocs\u002Ftransformers\u002Fmain\u002Fen\u002Findex#supported-frameworks\" rel=\"noopener noreferrer nofollow\"\u003E\u003Cu\u003Ehttps:\u002F\u002Fhuggingface.co\u002Fdocs\u002Ftransformers\u002Fmain\u002Fen\u002Findex#supported-frameworks\u003C\u002Fu\u003E\u003C\u002Fa\u003E\u003Cu\u003E\u003Cbr\u002F\u003E\u003C\u002Fu\u003EТакже модели можно экспортировать в форматы ONNX и TorchScript.\u003C\u002Fp\u003E\u003Cp\u003ETransformers не является набором модулей, из которых составляется нейронная сеть, как например PyTorch. Вместо это Transformers предоставляет несколько высокоуровневых абстракций, которые позволяют работать с моделями в несколько строк кода.\u003C\u002Fp\u003E\u003Cp\u003EНачнем с установки…\u003C\u002Fp\u003E\u003Ch2\u003EУстановка\u003C\u002Fh2\u003E\u003Cp\u003EНам понадобится сами трансформеры:\u003C\u002Fp\u003E\u003Cpre\u003E\u003Ccode class=\"bash\"\u003Epip install transformers\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\u003Cp\u003EИ т.к. мы будем писать на торче, то:\u003C\u002Fp\u003E\u003Cpre\u003E\u003Ccode class=\"bash\"\u003Epip install torch\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\u003Cp\u003EЕще нам понадобится библиотека evaluate, которая предоставляет различные ML метрики:\u003C\u002Fp\u003E\u003Cpre\u003E\u003Ccode class=\"bash\"\u003Epip install evaluate\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\u003Ch2\u003EПоиск моделей\u003C\u002Fh2\u003E\u003Cp\u003EПрежде чем приступать к коду нам нужно формализовать нашу задачу до одного из общепринятых классов и найти подходящую для нее модель на хабе Hugging Face: \u003Ca href=\"https:\u002F\u002Fhuggingface.co\u002Fmodels\" rel=\"noopener noreferrer nofollow\"\u003E\u003Cu\u003Ehttps:\u002F\u002Fhuggingface.co\u002Fmodels\u003C\u002Fu\u003E\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003Cfigure class=\"full-width \"\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F1fa\u002F262\u002F9d5\u002F1fa2629d5c8a96de953971af60a11162.png\" width=\"1536\" height=\"816\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F1fa\u002F262\u002F9d5\u002F1fa2629d5c8a96de953971af60a11162.png\"\u002F\u003E\u003Cfigcaption\u003E\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Cp\u003EСлева вы можете увидеть ряд фильтров:\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli\u003E\u003Cp\u003EКласс задачи\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003EПоддерживаемый фреймворк глубокого обучения\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003EНа каком датасете происходило обучение\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003EЯзык, на котором училась модель (если это NLP задача).\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp\u003EТакже вы сможете поискать модель по названию — часто название модели содержит ее предназначение или архитектуру. Например, NLP-модели для классификации токсичности текста могут содержать “toxic” в названии. А берто-подобные архитектуры содержат слово “bert”.\u003C\u002Fp\u003E\u003Cp\u003EНа ваш поиск может вывалится множество моделей, поскольку для одной и той же задачи имеется множество предобученных моделей на разных архитектурах. И вам нужно выбрать подходящую. Что значит “подходящую”? Тут на вкус и цвет фломастеры разные: кому-то важнее точность, кому-то универсальность, а кому-то размер модели — выбирайте :)\u003C\u002Fp\u003E\u003Cp\u003EПровалившись в конкретную модель вы сможете найти:\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli\u003E\u003Cp\u003EБолее подробное описание модели\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003EКол-во классов, которые предсказывает модель\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003EПримеры кода\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003EБенчмарки\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003EИ возможность поэкспериментировать\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cblockquote\u003E\u003Cp\u003EБолее подробно про различные классы задач и как они решаются можете почитать здесь: \u003Ca href=\"https:\u002F\u002Fhuggingface.co\u002Ftasks\" rel=\"noopener noreferrer nofollow\"\u003E\u003Cu\u003Ehttps:\u002F\u002Fhuggingface.co\u002Ftasks\u003C\u002Fu\u003E\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003C\u002Fblockquote\u003E\u003Cfigure class=\"full-width \"\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F1e7\u002F159\u002F83a\u002F1e715983a83f6bed950edcd450cc5e5e.png\" width=\"1536\" height=\"815\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F1e7\u002F159\u002F83a\u002F1e715983a83f6bed950edcd450cc5e5e.png\"\u002F\u003E\u003Cfigcaption\u003E\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Cp\u003EПосле того как вы нашли нужную модель скопируйте ее полное название, далее оно нам понадобится…\u003C\u002Fp\u003E\u003Ch2\u003EИспользование моделей\u003C\u002Fh2\u003E\u003Cp\u003EДля доступа к моделям есть два способа:\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli\u003E\u003Cp\u003EПрямое использование моделей на исходном фреймворке — больше кода, но и больше гибкости.\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003EКласс Pipeline — самый простой способ воспользоваться моделями из transformers. С него и начнем.\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Ch3\u003EPipeline\u003C\u002Fh3\u003E\u003Cp\u003EИспользование любой модели включает в себя минимум два шага: соответствующим образом подготовить данные и непосредственное использование модели. Класс Pipeline объединяет в себе эти два шага.\u003C\u002Fp\u003E\u003Cp\u003EПосмотрим как его задействовать на примере задачи классификации (токсичности) текста:\u003C\u002Fp\u003E\u003Cpre\u003E\u003Ccode class=\"python\"\u003Efrom transformers import pipeline\n\nclf = pipeline(\n    task = 'sentiment-analysis', \n    model = 'SkolkovoInstitute\u002Frussian_toxicity_classifier')\n\ntext = ['У нас в есть убунты и текникал превью.',\n    \t'Как минимум два малолетних дегенерата в треде, мда.']\n\nclf(text)\n\n#вывод\n[{'label': 'neutral', 'score': 0.9872767329216003},\n {'label': 'toxic', 'score': 0.985331654548645}]\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\u003Cp\u003EЗдесь мы:\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli\u003E\u003Cp\u003EВ конструкторе pipeline указали задачу, которую хотим решить, а также название конкретной модели из хаба Hugging Face (\u003Ca href=\"https:\u002F\u002Fhuggingface.co\u002Fmodels\" rel=\"noopener noreferrer nofollow\"\u003E\u003Cu\u003Ehttps:\u002F\u002Fhuggingface.co\u002Fmodels\u003C\u002Fu\u003E\u003C\u002Fa\u003E).\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003EЗадали набор документов, в которых нужно найти токсичный текст.\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003EНа выходе модель для каждого примера вывела наиболее вероятный класс и его скор.\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cblockquote\u003E\u003Cp\u003EС полным списком наименований задач, которые поддерживаются Pipeline вы можете ознакомится здесь: \u003Ca href=\"https:\u002F\u002Fhuggingface.co\u002Fdocs\u002Ftransformers\u002Fmain\u002Fen\u002Fmain_classes\u002Fpipelines\" rel=\"noopener noreferrer nofollow\"\u003E\u003Cu\u003Ehttps:\u002F\u002Fhuggingface.co\u002Fdocs\u002Ftransformers\u002Fmain\u002Fen\u002Fmain_classes\u002Fpipelines\u003C\u002Fu\u003E\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003C\u002Fblockquote\u003E\u003Cblockquote\u003E\u003Cp\u003EПри первом обращении к какой-либо модели произойдет ее загрузка. При повторном обращении к этой модели загрузка будет производится из кэша.\u003C\u002Fp\u003E\u003C\u002Fblockquote\u003E\u003Cp\u003EЧто тут можно улучшить:\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli\u003E\u003Cp\u003EПомимо конкретной модели в pipeline можно передать tokenizer. Токенайзер используется в NLP задачах и отвечает за предварительную обработку текста и конвертирует их в массив чисел, которые затем поступают на вход модели (об этом подробнее ниже). \u003Cbr\u002F\u003E\u003Cbr\u002F\u003EОбычно для модели используется точно такой же tokenizer, который использовался при обучении (только так можно гарантировать корректность ее работы). Но если по каким-либо причинам вам потребовался другой, то его можно задать примерно так:\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cpre\u003E\u003Ccode class=\"python\"\u003Epipeline(\n    task = 'question-answering', \n    model = 'distilbert-base-cased-distilled-squad', \n    tokenizer = 'bert-base-cased')\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\u003Cul\u003E\u003Cli\u003E\u003Cp\u003EПо умолчанию классификатор возвращает наиболее вероятный класс, но вы можете вернуть и все значения:\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cpre\u003E\u003Ccode class=\"python\"\u003Eclf(text, top_k=None)\n\n#вывод\n[[{'label': 'neutral', 'score': 0.9872767329216003},\n  {'label': 'toxic', 'score': 0.012723307125270367}],\n [{'label': 'neutral', 'score': 0.01466838177293539},\n  {'label': 'toxic', 'score': 0.985331654548645}]]\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\u003Cul\u003E\u003Cli\u003E\u003Cp\u003EЕсли все данные, которые нужно обработать, не влазят в память, то можно задействовать генератор, который будет поштучно загружать данные в память и подавать их в модель:\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cpre\u003E\u003Ccode class=\"python\"\u003Efrom transformers import pipeline\n\nclf = pipeline(\n    task = 'sentiment-analysis', \n    model = 'SkolkovoInstitute\u002Frussian_toxicity_classifier')\n\ntext = ['У нас в есть убунты и текникал превью.',\n        'Как минимум два малолетних дегенерата в треде, мда.']\n\ndef data(text):\n    for row in text:\n        yield row\n\nfor out in clf(data(text)):\n    print(out)\n\n#вывод\n{'label': 'neutral', 'score': 0.9872767329216003}\n{'label': 'toxic', 'score': 0.985331654548645}\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\u003Ch3\u003EPyTorch\u003C\u002Fh3\u003E\u003Cp\u003EА теперь посмотрим как использовать модель на нативном Торче. Будем классифицировать котиков :)\u003C\u002Fp\u003E\u003Cpre\u003E\u003Ccode class=\"python\"\u003Eimport torch\nimport requests\nfrom PIL import Image\nfrom io import BytesIO\nfrom transformers import AutoImageProcessor, AutoModelForImageClassification\n\nresponse = requests.get(\n    'https:\u002F\u002Fgithub.com\u002Flaxmimerit\u002Fdog-cat-full-dataset\u002Fblob\u002Fmaster\u002Fdata\u002Ftrain\u002Fcats\u002Fcat.10055.jpg?raw=true')\nimg = Image.open(BytesIO(response.content))\n\nimg_proc = AutoImageProcessor.from_pretrained(\n    'google\u002Fvit-base-patch16-224')\nmodel = AutoModelForImageClassification.from_pretrained(\n    'google\u002Fvit-base-patch16-224')\n\ninputs = img_proc(img, return_tensors='pt')\n\nwith torch.no_grad():\n    logits = model(**inputs).logits\n\npredicted_id = logits.argmax(-1).item()\npredicted_label = model.config.id2label[predicted_id]\nprint(predicted_id, '-', predicted_label)\n\n#вывод\n281 - tabby, tabby cat\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\u003Cp\u003EТут мы:\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli\u003E\u003Cp\u003EИмпортируем два AutoClass’а: AutoImageProcessor и AutoModelForImageClassification. \u003Cbr\u002F\u003EAutoClass (начинается с Auto) это специальный класс, который автоматически извлекает архитектуру предварительно обученной модели по ее имени или пути.\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003EЗагружаем картинку по URL.\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003EЗагружаем ImageProcessor. Это аналог токенайзера, но только для картинок — выравнивает размеры картинок, нормализует и т.д. (об ImageProcessor чуть подробнее ниже). В предыдущем варианте предобработкой занимался сам Pipeline где-то в своих недрах. Сейчас же нам придется заниматься этим самостоятельно.\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003EЗагружаем модель. Сама модель представляет собой PyTorch nn.Module, который вы можете использовать как обычно при работе с торчом.\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003EОбрабатываем картинку посредством ImageProcessor. ImageProcessor возвращает словарь, который подаем на вход модели с оператором распаковки (**).\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003EВсе модели Transformers возвращают логиты, которые идут перед последней функцией активации (например, softmax). Соответственно, нам самим необходимо их обработать, чтобы получить на выходе вероятность или класс.\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Ch4\u003EАвтоматическое определение архитектуры\u003C\u002Fh4\u003E\u003Cp\u003EДля каждой архитектуры и каждой задачи под нее есть свой специальный именной класс. Например: BertForSequenceClassification, GPT2ForSequenceClassification, RobertaForSequenceClassification и т.д. Также и для их предобработчиков: BertTokenizer, .GPT2Tokenizer и т.д.\u003C\u002Fp\u003E\u003Cp\u003EЧтобы каждый раз не заморачиваться с определением точного названия класса в Transformers завезли так называемый AutoClass. AutoClass позволяет автоматически считывать всю метаинформацию (архитектуру и пр.) из предварительно обученной модели при ее загрузке:\u003C\u002Fp\u003E\u003Cpre\u003E\u003Ccode class=\"python\"\u003Eimg_proc = AutoImageProcessor.from_pretrained(\n    'google\u002Fvit-base-patch16-224')\nmodel = AutoModelForImageClassification.from_pretrained(\n    'google\u002Fvit-base-patch16-224')\n\ntokenizer = AutoTokenizer.from_pretrained(\n    'SkolkovoInstitute\u002Frussian_toxicity_classifier')\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    'SkolkovoInstitute\u002Frussian_toxicity_classifier')\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\u003Cp\u003EКаждый автокласс привязан к определенной задаче. С полным списком автоклассов можете ознакомится здесь: \u003Ca href=\"https:\u002F\u002Fhuggingface.co\u002Fdocs\u002Ftransformers\u002Fmain\u002Fen\u002Fmodel_doc\u002Fauto#transformers.AutoModel\" rel=\"noopener noreferrer nofollow\"\u003E\u003Cu\u003Ehttps:\u002F\u002Fhuggingface.co\u002Fdocs\u002Ftransformers\u002Fmain\u002Fen\u002Fmodel_doc\u002Fauto\u003C\u002Fu\u003E\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003Cblockquote\u003E\u003Cp\u003EЕсли вы обучаете модель с нуля, то вам нужно импортировать точный конечный класс.\u003C\u002Fp\u003E\u003C\u002Fblockquote\u003E\u003Ch2\u003EДообучение\u003C\u002Fh2\u003E\u003Cp\u003EНе часто вам потребуются модели как есть. В соревнованиях, а тем более в работе вам скорее всего придется дообучить модель на своем датасете. И тут вас есть несколько вариантов…\u003C\u002Fp\u003E\u003Ch3\u003ETrainer\u003C\u002Fh3\u003E\u003Cp\u003EСамый простой способ - воспользоваться классом Trainer. Это аналог Pipline’а. Только он предназначен для организации упрощенного процесса обучения.\u003C\u002Fp\u003E\u003Cpre\u003E\u003Ccode class=\"python\"\u003Eimport datasets\nimport evaluate\nimport pandas as pd\nimport numpy as np\nfrom datasets import Dataset\nfrom sklearn.model_selection import train_test_split\nfrom transformers import (AutoTokenizer, AutoModelForSequenceClassification, \n                          TrainingArguments, Trainer)\n\n# Загружаем данные\ndf = pd.read_csv('toxic.csv')\ndf.columns = ['text','label']\ndf['label'] = df['label'].astype(int)\n\n# Конвертируем датафрейм в Dataset\ntrain, test = train_test_split(df, test_size=0.3)\ntrain = Dataset.from_pandas(train)\ntest = Dataset.from_pandas(test)\n\n# Выполняем предобработку текста\ntokenizer = AutoTokenizer.from_pretrained(\n    'SkolkovoInstitute\u002Frussian_toxicity_classifier')\n\ndef tokenize_function(examples):\n\treturn tokenizer(examples['text'], padding='max_length', truncation=True)\n\ntokenized_train = train.map(tokenize_function)\ntokenized_test = test.map(tokenize_function)\n\n# Загружаем предобученную модель\nmodel = AutoModelForSequenceClassification.from_pretrained(\n\t'SkolkovoInstitute\u002Frussian_toxicity_classifier',\n\tnum_labels=2)\n\n# Задаем параметры обучения\ntraining_args = TrainingArguments(\n\toutput_dir = 'test_trainer_log',\n\tevaluation_strategy = 'epoch',\n\tper_device_train_batch_size = 6,\n\tper_device_eval_batch_size = 6,\n\tnum_train_epochs = 5,\n\treport_to='none')\n\n# Определяем как считать метрику\nmetric = evaluate.load('f1')\ndef compute_metrics(eval_pred):\n\tlogits, labels = eval_pred\n\tpredictions = np.argmax(logits, axis=-1)\n\treturn metric.compute(predictions=predictions, references=labels)\n\n# Выполняем обучение\ntrainer = Trainer(\n\tmodel = model,\n\targs = training_args,\n\ttrain_dataset = tokenized_train,\n\teval_dataset = tokenized_test,\n\tcompute_metrics = compute_metrics)\n\ntrainer.train()\n\n# Сохраняем модель\nsave_directory = '.\u002Fpt_save_pretrained'\n#tokenizer.save_pretrained(save_directory)\nmodel.save_pretrained(save_directory)\n#alternatively save the trainer\n#trainer.save_model('CustomModels\u002FCustomHamSpam')\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\u003Cfigure class=\"full-width \"\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F090\u002F161\u002F072\u002F090161072e1b347bed5e63e490f0834f.png\" width=\"894\" height=\"336\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F090\u002F161\u002F072\u002F090161072e1b347bed5e63e490f0834f.png\"\u002F\u003E\u003Cfigcaption\u003E\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Cp\u003EЧто мы тут делаем:\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli\u003E\u003Cp\u003EЗагружаем данные в пандас.\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003EПереводим датафрейм в класс Dataset, которые можно подавать на вход модели при обучении.\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003EПрименяем к каждой строке датасета (тексту) токенизатор, чтобы перевести его в массив чисел.\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003EПодгружаем предварительно обученную модель.\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003EОпределяем экземпляр класса TrainingArguments. В нем задаются гиперпараметры, которые будут использоваться при обучении модели, а также специальные флаги, которые активируют различные варианты обучения.\u003Cbr\u002F\u003EТ.к. класс универсальный и предназначен для обучения разных архитектур и задач, то параметров у него довольно много. Подробнее ознакомится с ними можете здесь: \u003Ca href=\"https:\u002F\u002Fhuggingface.co\u002Fdocs\u002Ftransformers\u002Fmain_classes\u002Ftrainer#transformers.TrainingArguments\" rel=\"noopener noreferrer nofollow\"\u003E\u003Cu\u003Ehttps:\u002F\u002Fhuggingface.co\u002Fdocs\u002Ftransformers\u002Fmain_classes\u002Ftrainer#transformers.TrainingArguments\u003C\u002Fu\u003E\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003Cp\u003EСейчас зададим только: куда сохранять промежуточные результаты во время обучения, стратегию обучения (в данном случае — по эпохам), кол-во эпох и размер батча.\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003EПосредством модуля Evaluate определяем интересующую нас метрику и задаем функцию, которая будет выполнять расчет в процессе обучения.\u003Cbr\u002F\u003E\u003Cbr\u002F\u003EЗ.Ы. Не забываем что все модели возвращают логиты, которые необходимо соответствующим образом преобразовать.\u003Cbr\u002F\u003E\u003Cbr\u002F\u003EИспользовать библиотеку Evaluate не обязательно. Можно хоть из скалерна брать метрики. Главно определить функцию, которая будет производить расчеты.\u003Cbr\u002F\u003E\u003Cbr\u002F\u003EСписок доступных метрик: \u003Ca href=\"https:\u002F\u002Fhuggingface.co\u002Fevaluate-metric\" rel=\"noopener noreferrer nofollow\"\u003E\u003Cu\u003Ehttps:\u002F\u002Fhuggingface.co\u002Fevaluate-metric\u003C\u002Fu\u003E\u003C\u002Fa\u003E\u003Cu\u003E\u003Cbr\u002F\u003E\u003C\u002Fu\u003Eтут же вы можете с ними поэкспериментировать.\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003EСоздаем объект Trainer и подгружаем в него все ранее определенные компоненты: модель, аргументы, датасеты, функцию оценки. И запускаем обучение.\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003EПосле обучения сохраняем результат на диск.\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp\u003EВ последующем мы можем загрузить нашу модель так:\u003C\u002Fp\u003E\u003Cpre\u003E\u003Ccode class=\"python\"\u003Emodel = AutoModelForSequenceClassification.from_pretrained(\n\t'.\u002Fpt_save_pretrained')\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\u003Cblockquote\u003E\u003Cp\u003ETrainer поддерживает поиск гиперпараметров посредством специализированных пакетов: optuna, sigopt, raytune и wandb. Более подрбно: \u003Ca href=\"https:\u002F\u002Fhuggingface.co\u002Fdocs\u002Ftransformers\u002Fhpo_train\" rel=\"noopener noreferrer nofollow\"\u003E\u003Cu\u003Ehttps:\u002F\u002Fhuggingface.co\u002Fdocs\u002Ftransformers\u002Fhpo_train\u003C\u002Fu\u003E\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003C\u002Fblockquote\u003E\u003Ch3\u003EPyTorch\u003C\u002Fh3\u003E\u003Cp\u003EТеперь задействуем классический алгоритм обучения на торче:\u003C\u002Fp\u003E\u003Cpre\u003E\u003Ccode class=\"python\"\u003Eimport torch\nimport evaluate\nimport pandas as pd\nfrom tqdm.auto import tqdm\nfrom datasets import Dataset\nfrom torch.optim import AdamW\nfrom torch.utils.data import DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom transformers import (AutoTokenizer, \n                          AutoModelForSequenceClassification, get_scheduler)\n\n# Загружаем данные\ndf = pd.read_csv('toxic.csv')\ndf.columns = ['text','label']\ndf['label'] = df['label'].astype(int)\n\n# Конвертируем датафрейм в Dataset\ntrain, test = train_test_split(df, test_size=0.2)\ntrain = Dataset.from_pandas(train)\ntest = Dataset.from_pandas(test)\n\n# Выполняем предобработку текста\ntokenizer = AutoTokenizer.from_pretrained(\n\t'SkolkovoInstitute\u002Frussian_toxicity_classifier')\n\ndef tokenize_function(examples):\n\treturn tokenizer(examples['text'], padding='max_length', truncation=True)\n\ndef ds_preproc(ds):\n\tds = ds.map(tokenize_function)\n\tds = ds.remove_columns(['text', 'index_level_0'])\n\tds = ds.rename_column('label', 'labels')\n\tds.set_format('torch')\n\treturn ds\n\ntokenized_train = ds_preproc(train)\ntokenized_test = ds_preproc(test)\n\n# Создаем даталоадер\ntrain_dataloader = DataLoader(tokenized_train, shuffle=True, batch_size=8)\ntest_dataloader = DataLoader(tokenized_test, batch_size=8)\n\n# Загружаем модель и указываем кол-во классов\nmodel = AutoModelForSequenceClassification.from_pretrained(\n\t'SkolkovoInstitute\u002Frussian_toxicity_classifier',\n\tnum_labels=2)\n\n# Задаем оптимайзер и шедулер\noptimizer = AdamW(model.parameters(), lr=5e-6)\n\nnum_epochs = 5\nnum_training_steps = num_epochs * len(train_dataloader)\n\nlr_scheduler = get_scheduler(\n\tname = 'linear',\n\toptimizer = optimizer,\n\tnum_warmup_steps = 0,\n\tnum_training_steps = num_training_steps)\n\ndevice = 'cuda'\nmodel.to(device)\n\n# Выполняем цикл...\nfor epoch in tqdm(range(num_epochs)):\n\n\t#... обучения\n\tmodel.train()\n\tfor batch in tqdm(train_dataloader, leave=False):\n    \tbatch = {k: v.to(device) for k, v in batch.items()}\n    \toutputs = model(**batch)\n    \tloss = outputs.loss\n    \tloss.backward()\n    \toptimizer.step()\n    \tlr_scheduler.step()\n    \toptimizer.zero_grad()\n\n\t#... оценки\n\tmetric = evaluate.load('f1')\n\n\tmodel.eval()\n\tfor batch in tqdm(test_dataloader, leave=False):\n    \tbatch = {k: v.to(device) for k, v in batch.items()}\n    \twith torch.no_grad():\n        \toutputs = model(**batch)\n    \tlogits = outputs.logits\n    \tpredictions = torch.argmax(logits, dim=-1)\n\n    \tmetric.add_batch(predictions=predictions, references=batch['labels'])\n\n\tprint(f'epoch {epoch} -', metric.compute())\n\n# Сохраняем модель\nsave_directory = '.\u002Fpt_save_pretrained'\nmodel.save_pretrained(save_directory)\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\u003Cp\u003EЭтот пайплайн не сильно отличается от других процессов обучения нейронных сетей:\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli\u003E\u003Cp\u003EЗагружаем датасет в пандас, разбиваем на трейн\u002Fтест.\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003EКонвертируем датафрейм в класс Dataset, который принимает на вход модель.\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003EПереводим текст в токены, токены в массив чисел. Также очищаем датасет от лишних полей (иначе модель будет ругаться).\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003EФормируем DataLoader для тренировочных и тестовых данных, чтобы мы могли \u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003EОпределяем оптимизатор и шедулер.\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003EЗагружаем модель и указываем кол-во классов, которые должна выучить модель.\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003EЗадаем оптимизатор и планировщик. Причем оптимизатор родной для торча, а планировщик из библиотеки трансформеров.\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003EИтерируемся по эпохам. На каждой выполняем цикл обучения и цикл оценки.\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003EСохраняем модель на диске.\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Ch3\u003EЭмбединги\u003C\u002Fh3\u003E\u003Cp\u003EТретий способ дообучить модель — вытащить из нее эмбеддинги (актуально для NLP задач) и обучить какую-либо классическую модель, используя эти эмбединги как фичи.\u003C\u002Fp\u003E\u003Cp\u003EВытащить эмбединги можно двумя способами.\u003C\u002Fp\u003E\u003Col\u003E\u003Cli\u003E\u003Cp\u003EПервый - ручной:\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003C\u002Fol\u003E\u003Cpre\u003E\u003Ccode class=\"python\"\u003Eimport torch\nimport pandas as pd\nfrom transformers import AutoTokenizer, AutoModel\n\n#Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\n\ttoken_embeddings = model_output[0] #First element of model_output contains all token embeddings\n\tinput_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n\tsum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n\tsum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\treturn sum_embeddings \u002F sum_mask\n\n#Sentences we want sentence embeddings for\nsentences = ['This framework generates embeddings for each input sentence',\n             'Sentences are passed as a list of string.',\n             'The quick brown fox jumps over the lazy dog.']\n\n#Load AutoModel from huggingface model repository\ntokenizer = AutoTokenizer.from_pretrained(\n\t'sentence-transformers\u002Fall-MiniLM-L6-v2')\nmodel = AutoModel.from_pretrained(\n\t'sentence-transformers\u002Fall-MiniLM-L6-v2')\n\n#Tokenize sentences\nencoded_input = tokenizer(\n\tsentences, \n\tpadding=True, \n\ttruncation=True, \n\tmax_length=128, \n\treturn_tensors='pt')\n\n#Compute token embeddings\nwith torch.no_grad():\n\tmodel_output = model(encoded_input)\n\n#Perform pooling. In this case, mean pooling\nsentence_embeddings = mean_pooling(\n\tmodel_output,\n\tencoded_input['attention_mask'])\n\ndf = pd.DataFrame(sentence_embeddings).astype('float')\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\u003Col start=\"2\"\u003E\u003Cli\u003E\u003Cp\u003EАльтернативно можно задействовать отдельную библиотеку —  SentenceTransformers (ставится через пип):\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003C\u002Fol\u003E\u003Cpre\u003E\u003Ccode class=\"python\"\u003Efrom sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer('SkolkovoInstitute\u002Frussian_toxicity_classifier')\n\ntext = ['У нас в есть убунты и текникал превью.',\n    \t'Как минимум два малолетних дегенерата в треде, мда.']\n\nembeddings = model.encode(text)\n\ndf = pd.DataFrame(embeddings)\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\u003Cfigure class=\"full-width \"\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F98a\u002Fb09\u002Fe32\u002F98ab09e32d56a658ff21ddab03c86b57.png\" width=\"993\" height=\"136\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F98a\u002Fb09\u002Fe32\u002F98ab09e32d56a658ff21ddab03c86b57.png\"\u002F\u003E\u003Cfigcaption\u003E\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Cp\u003EКак видите, SentenceTransformers может подгружать нужные модели с хаба Hugging Face.\u003C\u002Fp\u003E\u003Ch2\u003EПредварительная обработка\u003C\u002Fh2\u003E\u003Cp\u003EЧуть поподробнее разберем в чем заключается предварительная обработка текста и картинок…\u003C\u002Fp\u003E\u003Ch3\u003EТокенайзер\u003C\u002Fh3\u003E\u003Cp\u003EТокенайзер используется в моделях, которые так или иначе работают с текстом. Компьютер не умеет напрямую работать с текстом - только с числами. И тут в дело вступает токенайзер: он преобразует текст в массив чисел, которые затем поступают на вход модели.\u003C\u002Fp\u003E\u003Cp\u003EПо сути каждый токенайзер состоит из набора правил и глобально эти правила решают две задачи:\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli\u003E\u003Cp\u003EКак поделить предложение на токены. В самом простом случае поделить можно на слова, а критерий разделения - пробел. Но в действительности способов гораздо больше и они довольно сложные.\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003EКак привести разнородные предложения к одной длине. Очевидно что тексты бывают разной длины. Но все последовательности, подаваемые на вход модели должны иметь одинаковую длину.\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp\u003EСначала посмотрим как токенайзер разбивает предложение на токены:\u003C\u002Fp\u003E\u003Cpre\u003E\u003Ccode class=\"python\"\u003Efrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\n\t'SkolkovoInstitute\u002Frussian_toxicity_classifier')\n\ntokenizer.tokenize('У нас в есть убунты и текникал превью.')\n\n#вывод\n['У','нас','в','есть','убу','##нты','и','тек','##ника','##л','превью','.']\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\u003Cp\u003EОбратите внимание на значки # в токенах. Так токенайзер выделяет подслова в словах. Это сделано, чтобы не запоминать кучу редких слов и уменьшить хранимый словарный запас. Также это позволяет модели обрабатывать слова, которые она никогда раньше не видела.\u003C\u002Fp\u003E\u003Cp\u003EЭто один из видов токенизации. Всего в трансформерах используются три основных вида токенизации: \u003Ca href=\"https:\u002F\u002Fhuggingface.co\u002Fdocs\u002Ftransformers\u002Fmain\u002Fen\u002Ftokenizer_summary#byte-pair-encoding\" rel=\"noopener noreferrer nofollow\"\u003E\u003Cu\u003EByte-Pair Encoding (BPE)\u003C\u002Fu\u003E\u003C\u002Fa\u003E,\u003Ca href=\"https:\u002F\u002Fhuggingface.co\u002Fdocs\u002Ftransformers\u002Fmain\u002Fen\u002Ftokenizer_summary#wordpiece\" rel=\"noopener noreferrer nofollow\"\u003E \u003Cu\u003EWordPiece\u003C\u002Fu\u003E\u003C\u002Fa\u003E, \u003Ca href=\"https:\u002F\u002Fhuggingface.co\u002Fdocs\u002Ftransformers\u002Fmain\u002Fen\u002Ftokenizer_summary#sentencepiece\" rel=\"noopener noreferrer nofollow\"\u003E\u003Cu\u003ESentencePiece\u003C\u002Fu\u003E\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\u003Cp\u003EТеперь посмотрим, что подается на вход модели:\u003C\u002Fp\u003E\u003Cpre\u003E\u003Ccode class=\"python\"\u003Etext = 'У нас в есть убунты и текникал превью.'\nencoding = tokenizer(text)\nprint(encoding)\n\n#вывод\n'input_ids': [101, 486, 1159, 340, 999, 63692, 10285, 322, 3100, 1352, 343, 85379, 132, 102]\n'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\u003Cp\u003EТокенайзер возвращает словарь, содержащий:\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli\u003E\u003Cp\u003Einput_ids — массив чисел, каждое из которых соответствует одному токену.\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003Eattention_mask — указывает модели, какие токены следует учитывать, а какие игнорировать.\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003Etoken_type_ids — используется в специальных моделях, в которые подаются пары последовательностей. Например, вопрос-ответ. Тогда в token_type_ids эти две последовательности будут обозначены разными метками.\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp\u003EДанный словарь подается на вход модели с оператором распаковки (**).\u003C\u002Fp\u003E\u003Cp\u003EТеперь посмотрим как токенайзер выравнивает длину предложений. За это отвечают три параметра:\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli\u003E\u003Cp\u003Epadding — тензоры подаваемые в модель должны иметь одинаковую длину. Если этот параметр = True, то коротки последовательности дополняются служебными токенами до длины самой длинной последовательности.\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003Etruncation — очень длинные последовательности тоже плохо. Если параметр = True, то все последовательности усекаются до максимальной длины.\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003Emax_length — указываем до скольки токенов усекать последовательность.\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cpre\u003E\u003Ccode class=\"python\"\u003Etext = ['У нас в есть убунты',\n    \t'Как минимум два малолетних дегенерата в треде, мда.']\n\nencoding = tokenizer(\n\ttext,\n\tpadding=True,\n\ttruncation=True,\n\tmax_length=512)\n\nprint(encoding)\n\n#вывод\n{'input_ids': [\n[101, 486, 1159, 340, 999, 63692, 10285, 102, 0, 0, 0, 0, 0], \n[101, 1235, 3932, 1617, 53502, 97527, 303, 340, 39685, 128, 48557, 132, 102]], 'token_type_ids': [\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], \n'attention_mask': [\n[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0], \n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\u003Cp\u003EОбратите внимание, что каждая последовательность представлена одинаковым количеством чисел (выровнены по самому длинному предложению). При этом короткие предложения дополнены нулями. А чтобы модель не обращала на них внимания, соответствующие токены в attention_mask обозначены нулями.\u003C\u002Fp\u003E\u003Cp\u003EПо каким правилам происходит усечение и дополнение токенов читайте здесь: \u003Ca href=\"https:\u002F\u002Fhuggingface.co\u002Fdocs\u002Ftransformers\u002Fmain\u002Fen\u002Fpad_truncation\" rel=\"noopener noreferrer nofollow\"\u003E\u003Cu\u003Ehttps:\u002F\u002Fhuggingface.co\u002Fdocs\u002Ftransformers\u002Fmain\u002Fen\u002Fpad_truncation\u003C\u002Fu\u003E\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003Ch3\u003EImageProcessor\u003C\u002Fh3\u003E\u003Cp\u003EImageProcessor отвечает за подготовку данных CV задач. Его работа несколько проще. По сути он переводит все пиксели в числа и при необходимости выравнивает изображения до одинаковой длины\u002Fширины.\u003C\u002Fp\u003E\u003Cpre\u003E\u003Ccode class=\"python\"\u003Efrom transformers import AutoImageProcessor\nfrom PIL import Image\nfrom io import BytesIO\nimport requests\n\nresponse = requests.get(\n\t'https:\u002F\u002Fgithub.com\u002Flaxmimerit\u002Fdog-cat-full-dataset\u002Fblob\u002Fmaster\u002Fdata\u002Ftrain\u002Fcats\u002Fcat.10055.jpg?raw=true')\nimg = Image.open(BytesIO(response.content))\n\nimage_processor = AutoImageProcessor.from_pretrained(\n\t'google\u002Fvit-base-patch16-224')\n\ninputs = image_processor(img, return_tensors='pt')\n\n#вывод\n{'pixel_values': tensor([[[[ 0.4275,  0.4275,  0.4196,  ...,  0.0902,  0.1216,  0.0667],\n          [ 0.4431,  0.4353,  0.4118,  ...,  0.0902,  0.0588,  0.0118],\n          [ 0.4431,  0.4353,  0.4039,  ...,  0.1686,  0.1059,  0.0431],\n          ...,\n          [-0.1373, -0.0745, -0.0431,  ...,  0.2941,  0.2863,  0.2627],\n          [-0.1529, -0.1137, -0.0588,  ...,  0.2784,  0.2627,  0.2627],\n          [-0.1529, -0.1294, -0.0745,  ...,  0.2706,  0.2392,  0.2392]],\n         [[ 0.4275,  0.4431,  0.4588,  ...,  0.0275,  0.0667,  0.0588],\n          [ 0.4431,  0.4510,  0.4510,  ...,  0.0275,  0.0039,  0.0039],\n          [ 0.4431,  0.4431,  0.4431,  ...,  0.1059,  0.0510,  0.0275],\n          ...,\n\n          [-0.2392, -0.1765, -0.1451,  ...,  0.1922,  0.1922,  0.1765],\n          [-0.2549, -0.2157, -0.1608,  ...,  0.1765,  0.1765,  0.1922],\n          [-0.2549, -0.2314, -0.1765,  ...,  0.1686,  0.1529,  0.1765]],\n         [[ 0.4431,  0.4510,  0.4275,  ..., -0.0902, -0.0824, -0.0980],\n          [ 0.4588,  0.4588,  0.4275,  ..., -0.0980, -0.1451, -0.1529],\n          [ 0.4588,  0.4510,  0.4118,  ..., -0.0196, -0.0980, -0.1294],\n          ...,\n          [-0.3647, -0.3020, -0.2706,  ...,  0.0667,  0.0667,  0.0510],\n          [-0.3804, -0.3490, -0.2941,  ...,  0.0431,  0.0353,  0.0431],\n          [-0.3882, -0.3647, -0.3098,  ...,  0.0353,  0.0118,  0.0275]]]])}\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\u003Cblockquote\u003E\u003Cp\u003EДля некоторых моделей ImageProcessor выполняет еще и постобработку. Например, преобразует логиты в маски сегментации.\u003C\u002Fp\u003E\u003C\u002Fblockquote\u003E\u003Ch2\u003EПрочее\u003C\u002Fh2\u003E\u003Ch3\u003EШаринг\u003C\u002Fh3\u003E\u003Cp\u003EЕсли вы обучили\u002Fдообучили хорошую модель, то можете поделиться ею с сообществом. Подробная инструкция как это сделать: \u003Ca href=\"https:\u002F\u002Fhuggingface.co\u002Fdocs\u002Ftransformers\u002Fmodel_sharing\" rel=\"noopener noreferrer nofollow\"\u003E\u003Cu\u003Ehttps:\u002F\u002Fhuggingface.co\u002Fdocs\u002Ftransformers\u002Fmodel_sharing\u003C\u002Fu\u003E\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003Cp\u003EТакже вы можете создать и загрузить в хаб Hugging Face свой кастомный Pipeline: \u003Ca href=\"https:\u002F\u002Fhuggingface.co\u002Fdocs\u002Ftransformers\u002Fmain\u002Fen\u002Fadd_new_pipeline#how-to-create-a-custom-pipeline\" rel=\"noopener noreferrer nofollow\"\u003E\u003Cu\u003Ehttps:\u002F\u002Fhuggingface.co\u002Fdocs\u002Ftransformers\u002Fmain\u002Fen\u002Fadd_new_pipeline#how-to-create-a-custom-pipeline\u003C\u002Fu\u003E\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003Ch3\u003EДатасеты\u003C\u002Fh3\u003E\u003Cp\u003EПлатформа Hugging Face предоставляет много готовых датасетов для аудио, CV и NLP задач, которые вы можете использовать для своих целей. Найти нужный датасет вы сможете в специальном хабе. По аналогии с моделями воспользуйтесь фильтрами, чтобы отыскать нужный вам датасет:\u003C\u002Fp\u003E\u003Cp\u003E\u003Ca href=\"https:\u002F\u002Fhuggingface.co\u002Fdatasets\" rel=\"noopener noreferrer nofollow\"\u003E\u003Cu\u003Ehttps:\u002F\u002Fhuggingface.co\u002Fdatasets\u003C\u002Fu\u003E\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003Cp\u003EЗагружать примерно так:\u003C\u002Fp\u003E\u003Cpre\u003E\u003Ccode class=\"python\"\u003Efrom datasets import load_dataset\n\ndataset = load_dataset('rotten_tomatoes')\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\u003Cp\u003EБолее подробно изучить функционал датасетов сможете здесь: \u003Ca href=\"https:\u002F\u002Fhuggingface.co\u002Fdocs\u002Fdatasets\u002Findex\" rel=\"noopener noreferrer nofollow\"\u003E\u003Cu\u003Ehttps:\u002F\u002Fhuggingface.co\u002Fdocs\u002Fdatasets\u002Findex\u003C\u002Fu\u003E\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003Ch2\u003EЧто дальше?\u003C\u002Fh2\u003E\u003Cp\u003EА дальше изучаем и воспроизводим кучу готовых примеров:\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli\u003E\u003Cp\u003E\u003Ca href=\"https:\u002F\u002Fhuggingface.co\u002Fdocs\u002Ftransformers\u002Fnotebooks\" rel=\"noopener noreferrer nofollow\"\u003E\u003Cu\u003Ehttps:\u002F\u002Fhuggingface.co\u002Fdocs\u002Ftransformers\u002Fnotebooks\u003C\u002Fu\u003E\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003E\u003Ca href=\"https:\u002F\u002Fhuggingface.co\u002Fdocs\u002Ftransformers\u002Fcommunity#community-notebooks\" rel=\"noopener noreferrer nofollow\"\u003E\u003Cu\u003Ehttps:\u002F\u002Fhuggingface.co\u002Fdocs\u002Ftransformers\u002Fcommunity#community-notebooks\u003C\u002Fu\u003E\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003E\u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fhuggingface\u002Ftransformers\u002Ftree\u002Fmain\u002Fexamples\" rel=\"noopener noreferrer nofollow\"\u003E\u003Cu\u003Ehttps:\u002F\u002Fgithub.com\u002Fhuggingface\u002Ftransformers\u002Ftree\u002Fmain\u002Fexamples\u003C\u002Fu\u003E\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003E\u003Ca href=\"https:\u002F\u002Fgithub.com\u002FNielsRogge\u002FTransformers-Tutorials\" rel=\"noopener noreferrer nofollow\"\u003E\u003Cu\u003Ehttps:\u002F\u002Fgithub.com\u002FNielsRogge\u002FTransformers-Tutorials\u003C\u002Fu\u003E\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp\u003EОтдельно стоит отметить целый курс, посвященный трансформерам: \u003Ca href=\"https:\u002F\u002Fhuggingface.co\u002Fcourse\u002Fchapter1\u002F1\" rel=\"noopener noreferrer nofollow\"\u003E\u003Cu\u003Ehttps:\u002F\u002Fhuggingface.co\u002Fcourse\u002Fchapter1\u002F1\u003C\u002Fu\u003E\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003Cfigure class=\"\"\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw1560\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F298\u002F70a\u002Fe7f\u002F29870ae7f525c03a7622c082bb407ac6.png\" width=\"175\" height=\"175\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F298\u002F70a\u002Fe7f\u002F29870ae7f525c03a7622c082bb407ac6.png\"\u002F\u003E\u003Cfigcaption\u003E\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Cp\u003EКод из статьи:\u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fslivka83\u002Farticle\u002Ftree\u002Fmain\u002Fhydra\" rel=\"noopener noreferrer nofollow\"\u003E \u003C\u002Fa\u003E\u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fslivka83\u002Farticle\u002Fblob\u002Fmain\u002Ftransformers\u002FTransformers.ipynb\" rel=\"noopener noreferrer nofollow\"\u003E\u003Cu\u003Ehttps:\u002F\u002Fgithub.com\u002Fslivka83\u002Farticle\u002Fblob\u002Fmain\u002Ftransformers\u002FTransformers.ipynb\u003C\u002Fu\u003E\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003Cp\u003E\u003Ca href=\"https:\u002F\u002Ft.me\u002Fds_private_sharing\" rel=\"noopener noreferrer nofollow\"\u003E\u003Cu\u003EМой телеграм-канал\u003C\u002Fu\u003E\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003Cp\u003E\u003C\u002Fp\u003E\u003C\u002Fdiv\u003E","tags":[{"titleHtml":"transformers"},{"titleHtml":"bert"},{"titleHtml":"gpt"},{"titleHtml":"machine leraning"},{"titleHtml":"data science"},{"titleHtml":"deep learning"}],"metadata":{"stylesUrls":[],"scriptUrls":[],"shareImageUrl":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F60f\u002F1a5\u002Faaf\u002F60f1a5aafe57e4d36ffbc1c3ac0e5113.png","shareImageWidth":1200,"shareImageHeight":630,"vkShareImageUrl":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F60f\u002F1a5\u002Faaf\u002F60f1a5aafe57e4d36ffbc1c3ac0e5113.png","schemaJsonLd":"{\"@context\":\"http:\\\u002F\\\u002Fschema.org\",\"@type\":\"Article\",\"mainEntityOfPage\":{\"@type\":\"WebPage\",\"@id\":\"https:\\\u002F\\\u002Fhabr.com\\\u002Fru\\\u002Farticles\\\u002F704592\\\u002F\"},\"headline\":\"Введение в библиотеку Transformers и платформу Hugging Face\",\"datePublished\":\"2022-12-09T19:23:20+03:00\",\"dateModified\":\"2022-12-10T19:07:37+03:00\",\"author\":{\"@type\":\"Person\",\"name\":\"Вячеслав\"},\"publisher\":{\"@type\":\"Organization\",\"name\":\"Habr\",\"logo\":{\"@type\":\"ImageObject\",\"url\":\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fwebt\\\u002Fa_\\\u002Flk\\\u002F9m\\\u002Fa_lk9mjkccjox-zccjrpfolmkmq.png\"}},\"description\":\"Исходники: https:\\\u002F\\\u002Fgithub.com\\\u002Fhuggingface\\\u002FtransformersДокументация: https:\\\u002F\\\u002Fhuggingface.co\\\u002Fdocs\\\u002Ftransformers\\\u002Fmain\\\u002Fen\\\u002FindexПлатформа Hugging Face это коллекция го...\",\"url\":\"https:\\\u002F\\\u002Fhabr.com\\\u002Fru\\\u002Farticles\\\u002F704592\\\u002F#post-content-body\",\"about\":[\"h_python\",\"h_data_mining\",\"h_bigdata\",\"h_machine_learning\",\"h_artificial_intelligence\",\"f_develop\",\"f_popsci\"],\"image\":[\"https:\\\u002F\\\u002Fhabr.com\\\u002Fshare\\\u002Fpublication\\\u002F704592\\\u002F5ab0d3a9f222402a1a974810e134c148\\\u002F\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fupload_files\\\u002F1fa\\\u002F262\\\u002F9d5\\\u002F1fa2629d5c8a96de953971af60a11162.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fupload_files\\\u002F1e7\\\u002F159\\\u002F83a\\\u002F1e715983a83f6bed950edcd450cc5e5e.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fupload_files\\\u002F090\\\u002F161\\\u002F072\\\u002F090161072e1b347bed5e63e490f0834f.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fupload_files\\\u002F98a\\\u002Fb09\\\u002Fe32\\\u002F98ab09e32d56a658ff21ddab03c86b57.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fupload_files\\\u002F298\\\u002F70a\\\u002Fe7f\\\u002F29870ae7f525c03a7622c082bb407ac6.png\"]}","metaDescription":"Исходники: https:\u002F\u002Fgithub.com\u002Fhuggingface\u002Ftransformers Документация: https:\u002F\u002Fhuggingface.co\u002Fdocs\u002Ftransformers\u002Fmain\u002Fen\u002Findex Платформа Hugging Face это коллекция готовых современных предварительно...","mainImageUrl":null,"amp":true,"customTrackerLinks":[]},"polls":[],"commentsEnabled":{"status":true,"reason":null},"rulesRemindEnabled":false,"votesEnabled":true,"status":"published","plannedPublishTime":null,"checked":null,"hasPinnedComments":false,"format":"tutorial","banner":null,"multiwidget":null,"readingTime":17,"complexity":null,"isEditorial":false}},"articlesIds":{},"isLoading":false,"pagesCount":{},"route":{},"reasonsList":null,"postReasonsList":null,"view":"cards","lastVisitedRoute":{},"ssrCommentsArticleIds":["530800","770424","752454","786952"],"viewedPosts":[],"myFeedFilter":{"complexity":"all","score":"all","types":["articles","posts","news"]},"karma":{"userReasonsList":null}},"authorContribution":{"authors":{}},"betaTest":{"currentAnnouncement":null,"announcements":{},"announcementCards":null,"announcementComments":{},"announcementCommentThreads":{},"announcementCommentingStatuses":{},"archivedList":[]},"authorStatistics":{"articleRefs":{},"articleIds":{},"pagesCount":{},"route":{},"viewsCount":[],"maxStatsCount":{}},"career":{"seoLandings":[{"title":"Django разработчик","vacanciesCount":42,"itemUrl":"https:\u002F\u002Fcareer.habr.com\u002Fvacancies\u002Fdjango_razrabotchik","itemHubs":["django","python"]},{"title":"Data Scientist","vacanciesCount":73,"itemUrl":"https:\u002F\u002Fcareer.habr.com\u002Fvacancies\u002Fdata_scientist","itemHubs":["bigdata","r","data_mining","python","machine_learning"]},{"title":"Python разработчик","vacanciesCount":129,"itemUrl":"https:\u002F\u002Fcareer.habr.com\u002Fvacancies\u002Fprogrammist_python","itemHubs":["django","flask","python"]}],"hubs":"python,data_mining,bigdata,machine_learning,artificial_intelligence"},"comments":{"articleComments":{},"articlePinnedComments":{},"searchCommentsResults":null,"pagesCount":null,"commentAccess":{},"scrollParents":{},"pageArticleComments":{"lastViewedComment":0,"postId":null,"lastCommentTimestamp":"","moderated":[],"moderatedIds":[],"commentRoute":""}},"companies":{"companyRefs":{},"companyIds":{},"companyTopIds":{},"pagesCount":{},"companyProfiles":{},"companiesCategories":[],"companiesCategoriesTotalCount":0,"companiesWidgets":{},"companiesWorkers":{},"companiesFans":{},"multiwidgets":{},"route":{},"isLoading":false,"companyWorkersLoading":false,"companyFansLoading":false,"multiwidgetLoading":false,"vacancies":{},"companiesGalleries":{},"companiesBanners":{},"companiesLandingVacancies":{},"companiesTechnologies":{},"workplaceInfo":null},"companyAdmin":{"companyInfo":null,"companyInfoLoading":false,"faqArticles":null,"brandingPreviewImageUrl":null,"jivoStatus":0,"adminNotifications":null},"companyAdd":{"currentStep":"","stepsData":{},"uncompletedSteps":[],"isStepLoading":true,"isStepCommitting":false,"isInitialized":false,"agreementContent":""},"companiesContribution":{"hubs":{},"flows":{},"companyRefs":{}},"companyHubsContribution":{"contributionRefs":{"hubRefs":{},"hubIds":{}}},"conversation":{"messages":[],"respondent":null,"isLoadMore":false},"conversations":{"conversations":[],"pagesCount":0},"docs":{"menu":{},"articles":{},"mainMenu":[],"loading":{"main":false,"dropdown":false,"article":false}},"feature":{"isProbablyVisible":true},"flows":{"flows":[{"alias":"develop","id":"1","route":{"name":"FLOW_PAGE","params":{"flowName":"develop"}}},{"alias":"admin","id":"6","route":{"name":"FLOW_PAGE","params":{"flowName":"admin"}}},{"alias":"design","id":"2","route":{"name":"FLOW_PAGE","params":{"flowName":"design"}}},{"alias":"management","id":"3","route":{"name":"FLOW_PAGE","params":{"flowName":"management"}}},{"alias":"marketing","id":"4","route":{"name":"FLOW_PAGE","params":{"flowName":"marketing"}}},{"alias":"popsci","id":"7","route":{"name":"FLOW_PAGE","params":{"flowName":"popsci"}}}],"updates":{}},"global":{"isPwa":false,"device":"desktop","isHabrCom":true},"hubs":{"hubRefs":{},"hubIds":{},"pagesCount":{},"isLoading":false,"route":{}},"hubsBlock":{"hubRefs":{},"hubIds":{}},"i18n":{"fl":"ru","hl":"ru"},"info":{"welcomePage":{},"isLoading":true},"location":{"urlStruct":{"protocol":null,"slashes":null,"auth":null,"host":null,"port":null,"hostname":null,"hash":null,"query":{},"pathname":"\u002Fru\u002Farticles\u002F704592\u002F","path":"\u002Fru\u002Farticles\u002F704592\u002F","href":"\u002Fru\u002Farticles\u002F704592\u002F"}},"me":{"user":null,"uuid":null,"ppgDemanded":false,"karmaResetInfo":{"canReincarnate":null,"wasReincarnated":null,"currentScore":null},"notes":null,"userUpdates":{"feeds":{"newPostsCount":null,"newThreadsCount":null,"newNewsCount":null,"newCount":null},"conversationUnreadCount":0}},"modal":{"modals":[]},"mostReadingList":{"mostReadingListIds":[],"mostReadingListRefs":null,"promoPost":null},"ppa":{"articles":{},"card":null,"transactions":null,"totalTransactions":null,"isAccessible":null},"projectsBlocks":{"activeBlocks":{"questions":"project-block-article"}},"promoData":{"isLoading":false,"hasLoaded":false,"featurer":null,"megaposts":null,"promoLinks":null,"promoPosts":null},"pullRefresh":{"shouldRefresh":false},"sandbox":{"articleIds":[],"articleRefs":{},"pagesCount":null,"route":{},"lastVisitedRoute":{},"isLoading":false},"search":{"searchQueryError":null},"settingsOther":{"inputs":{"uiLang":{"errors":[],"ref":null,"value":""},"articlesLangEnglish":{"errors":[],"ref":null,"value":false},"articlesLangRussian":{"errors":[],"ref":null,"value":false},"agreement":{"errors":[],"ref":null,"value":false},"email":{"errors":[],"ref":null,"value":true},"digest":{"errors":[],"ref":null,"value":true}}},"similarList":{"similarListIds":[],"similarListRefs":null},"ssr":{"error":null,"isDataLoaded":false,"isDataLoading":false,"isHydrationFailed":false,"isServer":false},"stories":{"stories":[{"id":"story-705","author":{"logo":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F891\u002Fd83\u002F7fa\u002F891d837fad39e1405c60deb68f713be0.png","title":"Яндекс 360","link":"https:\u002F\u002Fu.habr.com\u002Fya360_main"},"title":"Яндекс 360 призывает героев бэкенда","lang":"ru","startTime":"2024-03-04T15:35:00+03:00","finishTime":"2024-03-17T23:59:00+03:00","slides":[{"id":"story-705_1","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F350\u002F567\u002F3e7\u002F3505673e7d5c17777c7f999d7adf2fe8.jpg","button":null},{"id":"story-705_2","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fe82\u002F850\u002Ff95\u002Fe82850f95c9b7916ff87ef44ba9b8ab0.jpg","button":{"title":"Стану адептом Диска","link":"https:\u002F\u002Fu.habr.com\u002Fya360_2","colorType":"light"}},{"id":"story-705_3","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F2a8\u002F8c1\u002Fe71\u002F2a88c1e71df1f2f98a22a8e053636643.jpg","button":{"title":"Погрузиться в задачу","link":"https:\u002F\u002Fu.habr.com\u002Fya360_2","colorType":"light"}},{"id":"story-705_4","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fc9d\u002Ffd5\u002F238\u002Fc9dfd52381596a1fb542a75d57330d0e.jpg","button":{"title":"Защищу мир от спама","link":"https:\u002F\u002Fu.habr.com\u002Fya360_3","colorType":"light"}},{"id":"story-705_5","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F868\u002Fb2a\u002F934\u002F868b2a934ef52e8912de13f951f9da4c.jpg","button":{"title":"Стартовать кампанию","link":"https:\u002F\u002Fu.habr.com\u002Fya360_3","colorType":"light"}},{"id":"story-705_6","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F461\u002Fbce\u002Fd2d\u002F461bced2d4ede474ef17b346ccb22a9b.jpg","button":{"title":"Разовью биллинг","link":"https:\u002F\u002Fu.habr.com\u002Fya360_4n","colorType":"light"}},{"id":"story-705_7","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F72d\u002F088\u002Faaa\u002F72d088aaa6bf724e19435f710b1756f0.jpg","button":{"title":"Выбрать квест","link":"https:\u002F\u002Fu.habr.com\u002Fya360_4n","colorType":"light"}},{"id":"story-705_8","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fe7c\u002F57d\u002F0a4\u002Fe7c57d0a4767079aef5923502ec9a689.jpg","button":{"title":"Возглавить команду","link":"https:\u002F\u002Fu.habr.com\u002Fya360_1","colorType":"light"}},{"id":"story-705_9","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fef5\u002Fd01\u002Fd45\u002Fef5d01d45d2de914486aefdfddcde460.jpg","button":{"title":"Начать миссию","link":"https:\u002F\u002Fu.habr.com\u002Fya360_1","colorType":"light"}}]},{"id":"story-703","author":{"logo":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F190\u002Ff72\u002F526\u002F190f72526aab9f12ca8f9c3d969fb787.png","title":"Хабр Новости","link":"https:\u002F\u002Fu.habr.com\u002FiGq2c"},"title":"Бесплатный ИИ-курс","lang":"ru","startTime":"2024-03-04T10:43:00+03:00","finishTime":"2024-03-08T10:43:00+03:00","slides":[{"id":"story-703_1","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F354\u002F8ff\u002Fa21\u002F3548ffa216b96b6b35433a547381d451.jpg","button":{"title":"Подробнее","link":"https:\u002F\u002Fu.habr.com\u002FiGq2c","colorType":"light"}},{"id":"story-703_2","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F634\u002F972\u002F0a6\u002F6349720a6f286aacaea44512b10c352a.jpg","button":{"title":"Читать новость","link":"https:\u002F\u002Fu.habr.com\u002FiGq2c","colorType":"light"}}]},{"id":"story-620","author":{"logo":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fd8f\u002Ffdf\u002F0a5\u002Fd8ffdf0a514940e9ed9d57e5b439cafb.png","title":"Хабр","link":null},"title":"Полезные книги для библиотеки айтишника","lang":"ru","startTime":"2024-03-04T00:47:00+03:00","finishTime":"2024-03-12T23:59:00+03:00","slides":[{"id":"story-620_1","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F3ec\u002F0ec\u002F824\u002F3ec0ec8247b6c8af185bf64c93dfd4d1.jpg","button":null},{"id":"story-620_2","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F172\u002F613\u002F528\u002F1726135286d180c6d5f5b61ddd1e6386.jpg","button":{"title":"Ностальгия","link":"https:\u002F\u002Fhabr.com\u002Fru\u002Fcompanies\u002Fhabr\u002Farticles\u002F521172\u002F","colorType":"dark"}},{"id":"story-620_3","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F563\u002Fb83\u002F473\u002F563b83473264bbddc1605d2c3ebb48cc.jpg","button":{"title":"Учебный план","link":"https:\u002F\u002Fhabr.com\u002Fru\u002Farticles\u002F708752\u002F","colorType":"dark"}},{"id":"story-620_4","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fb4f\u002Fabe\u002F6ac\u002Fb4fabe6acdbd008bb6ffa87078e396d8.jpg","button":{"title":"Мир сетей","link":"https:\u002F\u002Fhabr.com\u002Fru\u002Fcompanies\u002Fskillbox\u002Farticles\u002F751112\u002F","colorType":"light"}},{"id":"story-620_5","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F2b1\u002Fe5b\u002Fd97\u002F2b1e5bd97f424ff094fcd6d6f71b037b.jpg","button":{"title":"Посчитать","link":"https:\u002F\u002Fhabr.com\u002Fru\u002Farticles\u002F753548\u002F","colorType":"light"}},{"id":"story-620_6","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F684\u002F5c4\u002F5d3\u002F6845c45d39380b664a1cb247ddad3dbd.jpg","button":{"title":"Контейнеры","link":"https:\u002F\u002Fhabr.com\u002Fru\u002Fcompanies\u002Fncloudtech\u002Farticles\u002F686942\u002F","colorType":"dark"}},{"id":"story-620_7","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F125\u002F051\u002F0ca\u002F1250510ca5987cde7710f1eefd3d3455.jpg","button":{"title":"Запастись","link":"https:\u002F\u002Fhabr.com\u002Fru\u002Fcompanies\u002Fcloud_mts\u002Farticles\u002F704424\u002F","colorType":"light"}},{"id":"story-620_8","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fe67\u002F95f\u002Fa7f\u002Fe6795fa7fb1acc35c26e940a9a3a1500.jpg","button":{"title":"Три направления","link":"https:\u002F\u002Fhabr.com\u002Fru\u002Fcompanies\u002Fplarium\u002Farticles\u002F435534\u002F","colorType":"dark"}}]},{"id":"story-510","author":{"logo":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F2e8\u002F940\u002Fb09\u002F2e8940b09afc4f34886e7ed0770378ed.png","title":"Хабр IT-гид","link":"https:\u002F\u002Fu.habr.com\u002F83lvb"},"title":"Как новичкам освоиться в IT","lang":"ru","startTime":"2024-03-02T10:00:00+03:00","finishTime":"2024-03-09T18:47:00+03:00","slides":[{"id":"story-510_1","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F01b\u002Fdcd\u002F81d\u002F01bdcd81d712fe9d32aebfa74a166d1b.png","button":null},{"id":"story-510_2","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F61c\u002F76f\u002F1c5\u002F61c76f1c5b36d09f9ce2ffb6b8e771c1.jpg","button":{"title":"Выбрать компанию","link":"https:\u002F\u002Fu.habr.com\u002FYndH4","colorType":"light"}},{"id":"story-510_3","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F855\u002F1ea\u002F29f\u002F8551ea29fee98cea55d939dd334a26e1.jpg","button":{"title":"Превозмогание","link":"https:\u002F\u002Fu.habr.com\u002FoB9OQ","colorType":"light"}},{"id":"story-510_4","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fc68\u002F92b\u002F927\u002Fc6892b92710b96949aaafc313c120f10.jpg","button":{"title":"Статистика и советы","link":"https:\u002F\u002Fu.habr.com\u002FK0NLH","colorType":"light"}},{"id":"story-510_5","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fa7f\u002F8d8\u002F93e\u002Fa7f8d893e12ee3a1e3666121d2a9adec.jpg","button":{"title":"Сравнить","link":"https:\u002F\u002Fu.habr.com\u002FBkp5y","colorType":"light"}},{"id":"story-510_6","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F2c2\u002F02d\u002F384\u002F2c202d384febc2a66697dfa9d67d2c08.jpg","button":{"title":"При чём тут Япония","link":"https:\u002F\u002Fu.habr.com\u002Ftjh7n","colorType":"light"}},{"id":"story-510_7","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F62c\u002Fbe4\u002F22d\u002F62cbe422d44bc721515a9443edc50ab0.jpg","button":{"title":"Мечты vs реальность","link":"https:\u002F\u002Fu.habr.com\u002FLfnj0","colorType":"light"}},{"id":"story-510_8","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fe16\u002Fbf2\u002F07f\u002Fe16bf207fcf5ae3e08272394e56ea1ce.jpg","button":{"title":"Маршрут построен","link":"https:\u002F\u002Fu.habr.com\u002FfAo9O","colorType":"light"}},{"id":"story-510_9","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fd5a\u002F0f5\u002F2b6\u002Fd5a0f52b6486541a51b01e0fc8e899c8.jpg","button":{"title":"Красные цифры","link":"https:\u002F\u002Fu.habr.com\u002FwPS0M","colorType":"light"}}]},{"id":"story-701","author":{"logo":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Ff8f\u002F2be\u002F41c\u002Ff8f2be41c3b7052d92049bce9cd562af.png","title":"Хабр","link":null},"title":"Букет котов","lang":"ru","startTime":"2024-02-29T23:13:00+03:00","finishTime":"2024-03-10T23:59:00+03:00","slides":[{"id":"story-701_1","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fa3b\u002F817\u002Fc8e\u002Fa3b817c8ea603fd0a8e9a0016e72fe5a.jpg","button":null},{"id":"story-701_2","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F5f9\u002Fca6\u002F3f1\u002F5f9ca63f1354b32c5b0d5a84be6cd736.jpg","button":{"title":"Падающие коты","link":"https:\u002F\u002Fhabr.com\u002Fru\u002Fcompanies\u002Fbanzai\u002Farticles\u002F488916\u002F","colorType":"light"}},{"id":"story-701_3","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fa5a\u002Feba\u002F7d6\u002Fa5aeba7d6685169548e6dc21de9502c3.jpg","button":{"title":"Древний мир","link":"https:\u002F\u002Fhabr.com\u002Fru\u002Farticles\u002F589947\u002F","colorType":"light"}},{"id":"story-701_4","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F117\u002F265\u002Fbbe\u002F117265bbe2bdc71757c9a5fa80d5c385.jpg","button":{"title":"Начать игру","link":"https:\u002F\u002Fhabr.com\u002Fru\u002Fcompanies\u002Ffirst\u002Farticles\u002F682572\u002F","colorType":"dark"}},{"id":"story-701_5","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fa69\u002Fa00\u002Fb8d\u002Fa69a00b8d15eb79fe62d0a14dfea0298.jpg","button":{"title":"Покормить","link":"https:\u002F\u002Fhabr.com\u002Fru\u002Farticles\u002F747684\u002F","colorType":"light"}},{"id":"story-701_6","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F485\u002F3ab\u002F1c9\u002F4853ab1c97feff2ab9a754573c009297.jpg","button":{"title":"Детали проекта","link":"https:\u002F\u002Fhabr.com\u002Fru\u002Farticles\u002F469893\u002F","colorType":"dark"}},{"id":"story-701_7","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F28b\u002F3ce\u002F9b5\u002F28b3ce9b5f99be08ff4af8eda7e865d0.jpg","button":{"title":"Секрет мурчания","link":"https:\u002F\u002Fhabr.com\u002Fru\u002Fnews\u002F765598\u002F","colorType":"dark"}},{"id":"story-701_8","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F921\u002F098\u002Fe13\u002F921098e1302ce73c80a0f3ae4e4ee9e6.jpg","button":{"title":"Открыть тайну","link":"https:\u002F\u002Fhabr.com\u002Fru\u002Farticles\u002F365075\u002F","colorType":"light"}},{"id":"story-701_9","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fd7c\u002F93f\u002F401\u002Fd7c93f401e792d599d1e7edf92a0051c.jpg","button":{"title":"Красивое","link":"https:\u002F\u002Fhabr.com\u002Fru\u002Fcompanies\u002Fruvds\u002Farticles\u002F751294\u002F","colorType":"dark"}},{"id":"story-701_10","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fa20\u002Ff37\u002Fe8f\u002Fa20f37e8f56fdb7664e0129a2ba8ff56.jpg","button":{"title":"Грозное оружие","link":"https:\u002F\u002Fhabr.com\u002Fru\u002Fcompanies\u002Fruvds\u002Farticles\u002F554270\u002F","colorType":"light"}}]},{"id":"story-697","author":{"logo":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fd1f\u002Fd37\u002F6b6\u002Fd1fd376b6e1c6f97f2e91fcee6ed4186.png","title":"Хабр","link":null},"title":"Годные статьи из блогов компаний","lang":"ru","startTime":"2024-02-26T11:04:00+03:00","finishTime":"2024-03-05T11:04:00+03:00","slides":[{"id":"story-697_1","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fdc0\u002Fa84\u002Fb72\u002Fdc0a84b72b6ad726c130d138586b37fa.jpg","button":null},{"id":"story-697_2","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Ff0d\u002Fdc8\u002F38d\u002Ff0ddc838d6617553e5d72d23b19e19aa.jpg","button":{"title":"Особый подход","link":"https:\u002F\u002Fhabr.com\u002Fru\u002Fcompanies\u002Fwiseops\u002Farticles\u002F791774\u002F","colorType":"light"}},{"id":"story-697_3","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F6a2\u002Fc71\u002Fd7e\u002F6a2c71d7e37ec7c7c8dfba1f0e3cfdf2.jpg","button":{"title":"Теория и практика","link":"https:\u002F\u002Fhabr.com\u002Fru\u002Fcompanies\u002Fruvds\u002Farticles\u002F792052\u002F","colorType":"dark"}},{"id":"story-697_4","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fa17\u002F253\u002Fe2c\u002Fa17253e2c2957c1327459d4673e1a250.jpg","button":{"title":"Как оценивали","link":"https:\u002F\u002Fhabr.com\u002Fru\u002Fcompanies\u002Fsberdevices\u002Farticles\u002F790470\u002F","colorType":"light"}},{"id":"story-697_5","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Ff8d\u002Fe82\u002Fc6b\u002Ff8de82c6b3919ffdd1023422dfbd7b91.jpg","button":{"title":"Обзор причин","link":"https:\u002F\u002Fhabr.com\u002Fru\u002Fcompanies\u002Fsberbank\u002Farticles\u002F790708\u002F","colorType":"dark"}},{"id":"story-697_6","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F903\u002F357\u002Fb7c\u002F903357b7c104d0dc34c0cd5c364f9c9e.jpg","button":{"title":"От идеи до итога","link":"https:\u002F\u002Fhabr.com\u002Fru\u002Fcompanies\u002Fruvds\u002Farticles\u002F788382\u002F","colorType":"light"}},{"id":"story-697_7","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fe11\u002F9a9\u002F146\u002Fe119a91461855f8745fc17251891be5f.jpg","button":{"title":"Личный опыт","link":"https:\u002F\u002Fhabr.com\u002Fru\u002Fcompanies\u002Fselectel\u002Farticles\u002F791904\u002F","colorType":"light"}},{"id":"story-697_8","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fc95\u002Fe43\u002F331\u002Fc95e433310d55cc50c64f1ef03fb2e18.jpg","button":{"title":"Детали экспермента","link":"https:\u002F\u002Fhabr.com\u002Fru\u002Fcompanies\u002Ftimeweb\u002Farticles\u002F789678\u002F","colorType":"light"}}]},{"id":"story-693","author":{"logo":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F0cf\u002F765\u002F121\u002F0cf76512167ee1882bea64eb2d8f3e7b.png","title":"Хабр","link":"https:\u002F\u002Fu.habr.com\u002Fhabr_events-1"},"title":"Как продвинуть машину времени?","lang":"ru","startTime":"2024-02-20T10:45:00+03:00","finishTime":"2024-03-10T10:45:00+03:00","slides":[{"id":"story-693_1","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fcbd\u002Fc0a\u002Fd0e\u002Fcbdc0ad0e6056f661d1e8fa146c9aa30.png","button":null},{"id":"story-693_2","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F688\u002Fd52\u002Fcc8\u002F688d52cc88ed33a3f4d72d118440c012.png","button":null},{"id":"story-693_3","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F7e0\u002F8a4\u002Fea4\u002F7e08a4ea42768ced281eea267e56c7bb.png","button":null},{"id":"story-693_4","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F9f0\u002F2ae\u002F56c\u002F9f02ae56c1ee55937c0c31afb3743fd4.png","button":{"title":"Добавить событие","link":"https:\u002F\u002Fu.habr.com\u002Fhabr_events-2","colorType":"light"}},{"id":"story-693_5","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fa16\u002F6dc\u002Fc79\u002Fa166dcc79e282edf107842ee8a5d32d0.png","button":null},{"id":"story-693_6","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F987\u002F258\u002F779\u002F987258779f88a78646ef069dad1a4dbb.png","button":{"title":"Хабр Календарь тут","link":"https:\u002F\u002Fu.habr.com\u002Fhabr_events-2","colorType":"light"}}]},{"id":"story-684","author":{"logo":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F921\u002F711\u002F6a7\u002F9217116a7a63fd5722272dcb0784a497.png","title":"Хабр","link":null},"title":"Учим английский","lang":"ru","startTime":"2024-02-05T11:05:00+03:00","finishTime":"2024-03-10T11:05:00+03:00","slides":[{"id":"story-684_1","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F3e7\u002Ff22\u002F9f6\u002F3e7f229f66db5b301ac57509bf8dec1e.png","button":{"title":"Профиль @varagian","link":"https:\u002F\u002Fu.habr.com\u002FphV0O","colorType":"dark"}},{"id":"story-684_2","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F780\u002F841\u002F304\u002F7808413045827f2d4359760e29b1f6c5.jpg","button":{"title":"Слова, слова","link":"https:\u002F\u002Fhabr.com\u002Fru\u002Fcompanies\u002Fepam_systems\u002Farticles\u002F483384\u002F","colorType":"light"}},{"id":"story-684_3","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fa2e\u002F040\u002F42b\u002Fa2e04042b05bc757738d1b39e9a94b45.jpg","button":{"title":"От А0 до С2","link":"https:\u002F\u002Fhabr.com\u002Fru\u002Farticles\u002F711404\u002F","colorType":"dark"}},{"id":"story-684_4","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F469\u002Fef6\u002Fb10\u002F469ef6b104e23e49e6db1b86a54386f6.jpg","button":{"title":"Путь к B2","link":"https:\u002F\u002Fhabr.com\u002Fru\u002Farticles\u002F784376\u002F","colorType":"dark"}},{"id":"story-684_5","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F953\u002F43f\u002F816\u002F95343f81610717241b139f0b38d1c98d.jpg","button":{"title":"От первого лица","link":"https:\u002F\u002Fhabr.com\u002Fru\u002Fcompanies\u002Fit-guide\u002Farticles\u002F785360\u002F","colorType":"light"}},{"id":"story-684_6","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fc05\u002F626\u002F6cc\u002Fc056266ccf35f7eb54ff9cba64485fce.jpg","button":{"title":"Устаревшее","link":"https:\u002F\u002Fhabr.com\u002Fru\u002Farticles\u002F580504\u002F","colorType":"light"}},{"id":"story-684_7","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F2d4\u002Fddb\u002F4e1\u002F2d4ddb4e1f198ff6382d335388497b19.jpg","button":{"title":"Что это за магия","link":"https:\u002F\u002Fhabr.com\u002Fru\u002Farticles\u002F787910\u002F","colorType":"light"}},{"id":"story-684_8","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F1a5\u002Fbd0\u002Fde7\u002F1a5bd0de7a25e08f717b0d26b9830a99.jpg","button":{"title":"Что из чего","link":"https:\u002F\u002Fhabr.com\u002Fru\u002Farticles\u002F43165\u002F","colorType":"light"}},{"id":"story-684_9","image":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F092\u002F0c0\u002F731\u002F0920c0731e7f3470137547c434d08ac5.jpg","button":{"title":"Методы","link":"https:\u002F\u002Fhabr.com\u002Fru\u002Farticles\u002F710628\u002F","colorType":"light"}}]}]},"technotext":{"years":[],"technotextDocForNominees":null,"technotextDocForWinners":null,"technotextInfo":{},"technotextInfoLoading":false,"technotextWinners":{},"technotextWinnersLoading":false},"userHubsContribution":{"contributionRefs":{"hubRefs":{},"hubIds":{}}},"userInvites":{"availableInvites":0,"usedInvitesIds":[],"usedInvitesRefs":{},"usedInvitesPagesCount":0,"unusedInvitesIds":[],"unusedInvitesRefs":{},"unusedInvitesPagesCount":0},"userVotes":{"karmaVotesList":[],"karmaVotesPagesCount":null,"karmaVotesListLoading":false,"commentsVotesList":[],"commentsVotesPagesCount":null,"commentsVotesListLoading":false,"postsVotesList":[],"postsVotesPagesCount":null,"postsVotesListLoading":false,"userVotesList":[],"userVotesPagesCount":null,"userVotesListLoading":false},"users":{"authorRefs":{},"authorIds":{},"pagesCount":{},"authorProfiles":{},"userHubs":{},"userInvitations":{},"authorFollowers":{},"authorFollowed":{},"userSpecialization":{},"karmaStats":[],"statistics":null,"isLoading":false,"authorFollowersLoading":false,"authorFollowedLoading":false,"userHubsLoading":false,"userInvitationsLoading":false,"route":{}},"viewport":{"prevScrollY":{},"scrollY":0,"width":0},"tracker":{"notificationsLoading":false,"notificationsList":[],"notificationsPageCount":0,"pendingMarkNotificationsRead":[],"publicationsLoading":true,"publicationsList":[],"publicationsPageCount":0,"pendingDeletePublications":false,"pendingMarkPublicationsRead":false},"events":{"events":[{"id":"events_176","titleHtml":"Серия занятий «Тренировки по алгоритмам 5.0» от Яндекса","descriptionHtml":"\u003Cp\u003EКак решать алгоритмический блок при найме в IT? Разбираемся на тренировках по алгоритмам от Яндекса.\u003C\u002Fp\u003E\u003Cp\u003EЧетыре недели вы решаете задачи, смотрите лекции и разборы. Чем больше решите задач, тем выше подниметесь в рейтинге.&nbsp;\u003C\u002Fp\u003E\u003Cp\u003EЛучшие участники получат персональные тренировки и пройдут пробные алгоритмические собеседования.&nbsp;\u003C\u002Fp\u003E\u003Cp\u003EУспешные собеседования будут засчитаны при отборе на стажировку или в штат Яндекса. Приём заявок до 28 февраля.\u003C\u002Fp\u003E","imageUrl":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fac9\u002F383\u002F51c\u002Fac938351c65f564f8b0bf373c436de53.png","eventUrl":"https:\u002F\u002Fu.habr.com\u002Fcldr_yaalg","startDay":"2024-03-01","startTime":"19:00","finishDay":"2024-04-19","isOnline":true,"geo":[]},{"id":"events_191","titleHtml":"DI CONF SMM — большая конференция по соцсетям в России","descriptionHtml":"\u003Cp\u003EЧто будет с соцсетями в России в 2024 году?\u003C\u002Fp\u003E\u003Cp\u003EВ сегодняшних реалиях они — «лабиринт» смыслов, контента и метрик.\u003C\u002Fp\u003E\u003Cp\u003EКуда двигаться и как с ними работать?&nbsp;\u003C\u002Fp\u003E\u003Cp\u003EРазберём на DI CONF SMM — большой конференции по соцсетям.\u003C\u002Fp\u003E\u003Cp\u003EЧерез «лабиринт» проведут эксперты: Алексей Ткачук, Роман Бордунов(ex-lead smm AVIASALES), команда VK, Дзен и LiveDune, медицинский центр DEGA, «Мармеладыч», подкаст-студия Red Barn, DIDENOK TEAM и другие.&nbsp;\u003C\u002Fp\u003E\u003Cp\u003EГлавные темы:\u003C\u002Fp\u003E\u003Cp\u003E• креатив: хайп и мемы;\u003C\u002Fp\u003E\u003Cp\u003E• экономика ИИ и нейросети;\u003C\u002Fp\u003E\u003Cp\u003E• подкасты для бизнеса;\u003C\u002Fp\u003E\u003Cp\u003E• стратегия в новых реалиях с разбором площадок;\u003C\u002Fp\u003E\u003Cp\u003E• на что ориентироваться в соцсетях в 2024 году;\u003C\u002Fp\u003E\u003Cp\u003E• маркировка и закон о рекламе;\u003C\u002Fp\u003E\u003Cp\u003E• influence-маркетинг;\u003C\u002Fp\u003E\u003Cp\u003E• брендинг и трендвотчинг\u003C\u002Fp\u003E\u003Cp\u003Eи др.&nbsp;\u003C\u002Fp\u003E\u003Cp\u003EЛекции пройдут в двух залах, а ещё будет воркшоп от отделов дизайна и smm di, много нетворкинга и afterparty.\u003C\u002Fp\u003E\u003Cp\u003EВстречаемся 2 марта по адресу: г. Краснодар, ул. Северная, 405, Инновационный Центр «Аквариум», -1 этаж или онлайн.\u003C\u002Fp\u003E\u003Cp\u003EСкидка 30% от 3 билетов, возможность оплаты по р\u002Fс и сервисом «Долями».\u003C\u002Fp\u003E\u003Cp\u003E\u003C\u002Fp\u003E","imageUrl":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F6df\u002F72d\u002F56c\u002F6df72d56ce1040e73b3646064d61f267.png","eventUrl":"https:\u002F\u002Fclck.ru\u002F396y2A","startDay":"2024-03-02","startTime":"09:30","finishDay":"2024-03-02","finishTime":"18:00","isOnline":true,"geo":["Краснодар"]},{"id":"events_140","titleHtml":"DevOpsConf 2024: конференция для инженеров и всех, кто должен понимать инженеров","descriptionHtml":"\u003Cp\u003E4 и 5 марта состоится DevOpsConf 2024 в Москве.\u003C\u002Fp\u003E\u003Cp\u003EМы готовим самую крупную конференцию для инженеров, CTO, CIO, техлидов и тимлидов в России и СНГ. Более 1000 ваших единомышленников соберутся в Сколково в самом начале весны.\u003C\u002Fp\u003E\u003Cp\u003EC 2009 года конференция значительно выросла, в программе 15-й DevOpsConf: более 80 докладов от ведущих экспертов, 10+ воркшопов, митапы и круглые столы.\u003C\u002Fp\u003E\u003Cp\u003EТемы этого года:\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli\u003E\u003Cp\u003Eуправление в DevOps;\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003Ereliability engineering;\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003Eplatform engineering глазами инженеров из СберМаркет и Яндекс.Go;\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003Eсистемное администрирование;\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003EIT-безопасность и DevSecOps;\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003Eэксплуатация крупных проектов;\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003Eтехнологическая независимость;\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003Eновые инструменты и технологии.\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp\u003EВстречаемся 4 и 5 марта на DevOpsConf 2024 в Сколково.\u003C\u002Fp\u003E","imageUrl":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F2a5\u002Fb6d\u002F6fc\u002F2a5b6d6fc13caf3dabce9c8ab833e282.png","eventUrl":"https:\u002F\u002Fu.habr.com\u002Fcldr_odc","startDay":"2024-03-04","startTime":"10:00","finishDay":"2024-03-05","finishTime":"20:00","isOnline":true,"geo":["Москва"]},{"id":"events_189","titleHtml":"Вебинар «Встраиваемые системы и программирование микроконтроллеров»","descriptionHtml":"\u003Cp\u003EПриглашаем на практический вебинар разработчиков и инженеров, которые интересуются встраиваемыми системами и программированием микроконтроллеров.&nbsp;\u003C\u002Fp\u003E\u003Cp\u003EНа вебинаре мы:\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli\u003E\u003Cp\u003Eпроведём обзор различных микроконтроллеров и их характеристик;\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003Eрассмотрим проектирование встраиваемых систем от идеи до реализации;&nbsp;\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003Eразберём на конкретном примере программирование микроконтроллеров;&nbsp;\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003Eсделаем отладку и тестирование встраиваемых систем.&nbsp;\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp\u003EУрок пройдёт онлайн в 20:00 по мск. После него у вас будет возможность стать студентом программы по специальной цене и приобрести обучение даже в рассрочку.\u003C\u002Fp\u003E","imageUrl":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F274\u002F868\u002F9b0\u002F2748689b03a269ce21d7913bad0c2920.jpg","eventUrl":"https:\u002F\u002Fu.habr.com\u002Fcldr_otus_mic","startDay":"2024-03-06","startTime":"20:00","finishDay":"2024-03-06","isOnline":true,"geo":[]},{"id":"events_193","titleHtml":"Вебинар «Применение обучения с подкреплением на финансовых рынках»","descriptionHtml":"\u003Cp\u003EОбучение с подкреплением — это метод машинного обучения, который широко используется в сфере трейдинга и финансов.&nbsp;\u003C\u002Fp\u003E\u003Cp\u003EВ OTUS стартует курс «Reinforcement Learning», на котором специалисты смогут освоить RL-алгоритмы на практике и использовать их, в том числе, с целью управления финансовым портфелем.\u003C\u002Fp\u003E\u003Cp\u003E6 марта в 20:00 приглашаем на открытое практическое занятие&nbsp; «Применение обучения с подкреплением на финансовых рынках».\u003C\u002Fp\u003E\u003Cp\u003EВы узнаете:\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli\u003E\u003Cp\u003Eкак обучить ИИ предсказывать движения на рынке;\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003Eкак создать модель, которая учится без вашего участия;\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003Eкакие алгоритмы лежат в основе процессов финансовых рынков;\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003Eчто нужно сделать для построения торгового робота;\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003Eкакие алгоритмы необходимы для прогнозирования поведения финансового рынка.\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp\u003EПри поступлении на курс возможны разные способы оплаты и рассрочка платежа.\u003C\u002Fp\u003E","imageUrl":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fc09\u002Fb0f\u002F7c0\u002Fc09b0f7c0b7c909a1e36141ac9f38fc1.png","eventUrl":"https:\u002F\u002Fu.habr.com\u002Fcldr_orl","startDay":"2024-03-06","startTime":"20:00","finishDay":"2024-03-06","isOnline":true,"geo":[]},{"id":"events_195","titleHtml":"Вебинар «Методы и принципы разработки ПО для встраиваемых устройств»","descriptionHtml":"\u003Cp\u003EВ OTUS идёт набор в группу курса «Embedded Developer» — быстрый и интересный старт в Embedded с погружением в три самые важные области: программирование микроконтроллеров, проектирование печатных плат, схемотехника.\u003C\u002Fp\u003E\u003Cp\u003E6 марта в 20:00 мск приглашаем на открытый урок курса «Методы и принципы разработки ПО для встраиваемых устройств».\u003C\u002Fp\u003E\u003Cp\u003EНа вебинаре вы:\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli\u003E\u003Cp\u003Eузнаете ключевые методы и принципы создания эффективного и надёжного ПО для микроконтроллеров и embedded-устройств;\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003Eрассмотрите проектирование и структурирование кода на основе State-машин и RTOS;\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003Eполучите комплексное понимание построения качественного ПО для встроенных систем.\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp\u003EПосле урока можно продолжить обучение по специальной цене и с рассрочкой платежа.\u003C\u002Fp\u003E","imageUrl":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F01f\u002Fc57\u002F832\u002F01fc578328619f3669a6791566394e14.png","eventUrl":"https:\u002F\u002Fu.habr.com\u002Fcldr_oed","startDay":"2024-03-06","startTime":"20:00","finishDay":"2024-03-06","isOnline":true,"geo":[]},{"id":"events_197","titleHtml":"Открытый урок «Пишем онлайн-чат на Golang»","descriptionHtml":"\u003Cp\u003EНапишите онлайн-чат на Golang с нуля за пару часов и добавьте его в портфолио. \u003Cbr\u003E\u003Cbr\u003EНа бесплатном практическом уроке от OTUS вы вместе с опытным экспертом:\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli\u003E\u003Cp\u003Eна примере разработки онлайн-чата разберёте преимущества и особенности языка Go;\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003Eпоговорите о клиент-серверной архитектуре интернета;\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003Eпопрактикуетесь в написании веб-приложения на Go;\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003Eпроведёте небольшое нагрузочное тестирование полученной реализации.\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp\u003EВстречаемся 12 марта в 20:00 мск в рамках курса «Go Developer Basic». После урока возможно продолжить обучение по специальной цене и с рассрочкой платежа.\u003Cbr\u003E\u003Cbr\u003EРегистрируйтесь прямо сейчас, чтобы посетить бесплатный урок и получить запись.\u003C\u002Fp\u003E","imageUrl":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fcec\u002F374\u002F6fc\u002Fcec3746fc1129aff78a52f8a160051cd.png","eventUrl":"https:\u002F\u002Fu.habr.com\u002Fcldr_ogo","startDay":"2024-03-12","startTime":"20:00","finishDay":"2024-03-12","isOnline":true,"geo":[]},{"id":"events_160","titleHtml":"«GoCloud 2024. Облачные грани будущего» — IT-конференция Cloud.ru про облака","descriptionHtml":"\u003Cp\u003EВместе обсудим IT-тренды, поделимся опытом и найдем инновационные решения для ваших задач.&nbsp;\u003C\u002Fp\u003E\u003Cp\u003EЧто ждет участников:\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli\u003E\u003Cp\u003E3 тематических трека про новую облачную платформу, прорывные технологии и клиентские инсайты;\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003E20 сессий с экспертами из мира облачных технологий;\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003Elive-демонстрации облачных платформ и сервисов;\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003Eнетворкинг с разработчиками и идеологами продуктов Cloud.ru.\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp\u003EКлючевые темы GoCloud 2024:\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli\u003E\u003Cp\u003Eоблако и бизнес: кейсы российских компаний вместе с Cloud.ru;\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003Eopen source: последние разработки;\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003ESDS, SDN и Serverless;\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003Eобзор новой облачной платформы Cloud.ru.\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp\u003EКому будет полезно: техническим специалистам, лидерам команд разработки и администрирования, руководителям технологических направлений и R&amp;D, экспертам и лидерам мнений в IT-индустрии.\u003C\u002Fp\u003E\u003Cp\u003EМероприятие пройдёт в Цифровом деловом пространстве по адресу: ул. Покровка, 47, Москва.\u003C\u002Fp\u003E\u003Cp\u003E\u003C\u002Fp\u003E","imageUrl":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002F9e2\u002F1fc\u002Ff5c\u002F9e21fcf5cb7e9bae5b0f7cb2fafd6a99.png","eventUrl":"https:\u002F\u002Fu.habr.com\u002Fcldr_gocloudcon","startDay":"2024-03-21","startTime":"09:00","finishDay":"2024-03-21","finishTime":"18:00","isOnline":true,"geo":["Москва"]},{"id":"events_187","titleHtml":"Московский туристический хакатон","descriptionHtml":"\u003Cp\u003EПродолжается приём заявок на Московский туристический хакатон — соревнование по цифровизации индустрии туризма и гостеприимства. Он пройдёт в два этапа: отборочный онлайн-этап и очный финал в Москве.\u003C\u002Fp\u003E\u003Cp\u003EУчастникам предстоит решить одну задачу из пяти треков на стыке TravelTech и смежных сфер на выбор:\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli\u003E\u003Cp\u003EBookingTech;\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003EFinTech;\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003ERoadTech;\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003EServiceTech;\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003EE-Com.\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp\u003EЗадачи подготовили сервис планирования путешествий RUSSPASS и представители Tutu, Mafin и других компаний, а также Департамент информационных технологий города Москвы.&nbsp;\u003C\u002Fp\u003E\u003Cp\u003EДаты проведения:\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli\u003E\u003Cp\u003Eдо 11 марта — регистрация участников;\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003E23-24 марта — отборочный онлайн-этап хакатона;\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003E6-7 апреля — офлайн финал в Москве;\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003E7 апреля — объявление победителей.\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp\u003EОбщий призовой фонд хакатона — 7 500 000 рублей.\u003C\u002Fp\u003E\u003Cp\u003EПринять участие могут IT-специалисты, продуктологи, разработчики, дизайнеры, аналитики, креаторы и все, кто разрабатывает прорывные технологические и продуктовые решения.\u003C\u002Fp\u003E","imageUrl":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fffd\u002F8b7\u002F394\u002Fffd8b73941c1f2eb35931067b11406ce.jpg","eventUrl":"https:\u002F\u002Fu.habr.com\u002Fcldr_msc_trvl","startDay":"2024-03-23","finishDay":"2024-04-07","isOnline":true,"geo":["Москва"]},{"id":"events_199","titleHtml":"Онлайн-презентация «GitVerse: открой вселенную кода»","descriptionHtml":"\u003Cp\u003EGitVerse — это открытая платформа от Сбера с AI-помощником для совместной разработки и хостинга кода.\u003C\u002Fp\u003E\u003Cp\u003E29 марта в 10:00 мск на онлайн-презентации «GitVerse: открой вселенную кода» команда продукта познакомит всех с новой функциональностью платформы, представит дорожную карту развития и анонсирует инструменты для повышения продуктивности разработчиков.\u003C\u002Fp\u003E\u003Cp\u003E\u003C\u002Fp\u003E","imageUrl":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fa37\u002Fb1f\u002F04a\u002Fa37b1f04a5feb49b221e0b1f4bf35c60.jpg","eventUrl":"https:\u002F\u002Fu.habr.com\u002Fcldr_git","startDay":"2024-03-29","startTime":"10:00","finishDay":"2024-03-29","finishTime":"13:00","isOnline":true,"geo":[]},{"id":"events_178","titleHtml":"Firebird Conf: конференция для разработчиков и администраторов СУБД Firebird","descriptionHtml":"\u003Cp\u003E6 июня приглашаем разработчиков и администраторов СУБД Firebird на конференцию «Firebird Conf 2024».&nbsp;\u003C\u002Fp\u003E\u003Cp\u003EЛокация: Radisson Blu Olympiyskiy Hotel Moscow.\u003C\u002Fp\u003E\u003Cp\u003EВас ждёт полное погружение в техническую часть, теория и практика для IT-сообщества.\u003C\u002Fp\u003E\u003Cp\u003EВ этом году конференция будет посвящена:&nbsp;\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli\u003E\u003Cp\u003Eновым возможностям Firebird 5;&nbsp;\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003Eоптимизации производительности больших БД;&nbsp;\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003Cli\u003E\u003Cp\u003Eполезным особенностям Firebird для прикладных разработчиков.\u003C\u002Fp\u003E\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp\u003EFirebird Conf 2024 — отличная возможность приятно провести время и неформально пообщаться с истинными единомышленниками. Знакомимся и обсуждаем интересные темы, связанные с разработкой и поддержкой баз данных.\u003C\u002Fp\u003E\u003Cp\u003EОрганизатор конференции РЕД СОФТ — российский разработчик IT-решений и услуг. Осуществляет комплексные проекты в области хранения и управления данными на основе собственного технологического стека.\u003C\u002Fp\u003E","imageUrl":"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fupload_files\u002Fd78\u002F8ad\u002F5cb\u002Fd788ad5cb236e7eebb4832ccc5310645.png","eventUrl":"https:\u002F\u002Fu.habr.com\u002Fcldr_firebird","startDay":"2024-06-06","startTime":"09:00","finishDay":"2024-06-06","finishTime":"20:00","isOnline":false,"geo":["Москва"]}]},"wysiwyg":{"WYSIWYGRulesRefs":null}};(function(){var s;(s=document.currentScript||document.scripts[document.scripts.length-1]).parentNode.removeChild(s);}());</script>
<script src="https://assets.habr.com/habr-web/js/chunk-vendors.02fc1736.js" defer></script><script src="https://assets.habr.com/habr-web/js/7298.c8f1d73c.js" defer></script><script src="https://assets.habr.com/habr-web/js/app.6cc13d0e.js" defer></script>



    <script async src="https://www.googletagmanager.com/gtag/js?id=G-S28W1WC23F"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
    </script>
  
  <script type="text/javascript" >
    (function(m,e,t,r,i,k,a){m[i]=m[i]||function(){(m[i].a=m[i].a||[]).push(arguments)};
    m[i].l=1*new Date();k=e.createElement(t),a=e.getElementsByTagName(t)[0],k.async=1,k.src=r,a.parentNode.insertBefore(k,a)})
    (window, document, "script", "https://mc.yandex.ru/metrika/tag.js", "ym");

    ym(24049213, "init", {
      defer:true,
      trackLinks:true,
      accurateTrackBounce:true,
      webvisor:false,
    });
  </script>
  <noscript>
    <div>
      <img src="https://mc.yandex.ru/watch/24049213" style="position:absolute; left:-9999px;" alt="" />
    </div>
  </noscript>
  
    <script type="text/javascript">
      window.addEventListener('load', function () {
        setTimeout(() => {
          const img = new Image();
          img.src = 'https://vk.com/rtrg?p=VK-RTRG-421343-57vKE';
        }, 0);
      });
    </script>
  
<script src="/js/ads.js" onload="window['zhY4i4nJ9K'] = true"></script>
</body>
</html>
